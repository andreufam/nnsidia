{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rede Neural Feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcocristo/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta aula, vamos usar a coleção MNIST. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('data/MNIST_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print mnist.train.num_examples\n",
    "print mnist.test.num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta é uma coleção com 55 mil casos de treino e 10 mil casos de teste. Cada instância é a imagem de 28x28 pixels de um número de 0 a 9 (em escala de cinza). Nosso objetivo é reconhecer o número. O atual estado-da-arte para este problema tem acurácia de 99.79 (Novembro de 2016, http://paper.researchbib.com/view/paper/104069). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD7CAYAAABKWyniAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRnQSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11409a750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "idx = random.randint(0, mnist.train.num_examples)\n",
    "plt.matshow(mnist.train.images[idx].reshape((28,28)), cmap = 'gray')\n",
    "print mnist.train.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Activation(object):\n",
    "    \"\"\"Funcao de ativacao\"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def init(self, n_inputs, n_outputs):\n",
    "        return tf.random_uniform([n_inputs, n_outputs], -1.0, 1.0)\n",
    "        \n",
    "    def fire(self, ypred):\n",
    "        if self.name == 'sigmoid':\n",
    "            return tf.nn.sigmoid(ypred) \n",
    "        else:\n",
    "            return ypred\n",
    "            \n",
    "class Layer(object):\n",
    "    \"\"\"Camada de rede neural sequencial\"\"\"\n",
    "    def __init__(self, units, activation = None, name = None):\n",
    "        self.units = units\n",
    "        self.name = name \n",
    "        self.activation = activation if activation != None else Activation('')\n",
    "        \n",
    "    def output(self, X):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        with tf.name_scope(self.name):\n",
    "            self.W = tf.Variable(self.activation.init(n_inputs, self.units), name = 'W')\n",
    "            self.b = tf.Variable(tf.zeros([self.units]), name = 'b')\n",
    "            ypred = self.activation.fire(tf.matmul(X, self.W) + self.b)\n",
    "        return ypred\n",
    "\n",
    "class LossFunction(object):\n",
    "    def __init__(self, name = 'sigmoid'):\n",
    "        self.name = name\n",
    "\n",
    "    def get(self, yreal, ypred):\n",
    "        if self.name == 'sigmoid':\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels = yreal, logits = ypred) \n",
    "        return tf.reduce_mean(loss, name = 'lossf')\n",
    "    \n",
    "class Optimizer(object):\n",
    "    def __init__(self, name = 'sgd'):\n",
    "        self.name = name\n",
    "        self.lrate = 0.1\n",
    "\n",
    "    def get(self, lossf):\n",
    "        if self.name == 'sgd':\n",
    "            opt = tf.train.GradientDescentOptimizer(learning_rate = self.lrate) \n",
    "        return opt.minimize(lossf)\n",
    "    \n",
    "class FeedforwardNeuralNet(object):\n",
    "    \"\"\"Rede neural sequencial\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.learning_rate = 0.1\n",
    "        self.layers = []\n",
    "        \n",
    "    def add(self, units, activation = None, name = None):\n",
    "        \"\"\"Adiciona camadas para rede neural\"\"\"\n",
    "        self.layers += [Layer(units, activation, name)]\n",
    "    \n",
    "    def compile(self, loss = 'sigmoid', optimizer = 'sgd'):\n",
    "        \"\"\"Cria grafo da rede neural\"\"\"\n",
    "        self.X = tf.placeholder(tf.float32, \n",
    "                           shape = (None, self.input_dim), \n",
    "                           name = 'X')\n",
    "        self.y = tf.placeholder(tf.int64, shape = (None), name = 'y')\n",
    "        \n",
    "        # cria layers\n",
    "        with tf.name_scope('layers'):\n",
    "            layer_in = self.X\n",
    "            for layer in self.layers:\n",
    "                layer_out = layer.output(layer_in)\n",
    "                layer_in = layer_out\n",
    "                    \n",
    "        # loss function\n",
    "        with tf.name_scope('loss'):\n",
    "            self.lossf = LossFunction(loss).get(self.y, layer_out)\n",
    "    \n",
    "        # optimizer\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_op = Optimizer(optimizer).get(self.lossf)\n",
    "            \n",
    "        # evalution metrics\n",
    "        with tf.name_scope('eval'):\n",
    "            correct = tf.nn.in_top_k(layer_out, self.y, 1)\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "            \n",
    "        self.init_op = tf.global_variables_initializer()\n",
    "        self.saver = tf.train.Saver()\n",
    "    \n",
    "    def fit(self, train_data, n_epochs, batch_size):\n",
    "        \"\"\"Executa treino da rede neural\"\"\"\n",
    "        num_batches = train_data.num_examples // batch_size\n",
    "        with tf.Session() as s:\n",
    "            s.run(self.init_op)\n",
    "            for e in range(n_epochs):\n",
    "                tloss = 0.\n",
    "                for i in range(num_batches):\n",
    "                    X_b, y_b = train_data.next_batch(batch_size)\n",
    "                    _, loss_e = s.run([self.train_op, self.lossf], \n",
    "                                      feed_dict = {self.X: X_b, self.y: y_b})\n",
    "                    tloss += loss_e\n",
    "                acc_train = s.run(self.acc, \n",
    "                                  feed_dict = {self.X: X_b, self.y: y_b})\n",
    "                print '%2d loss: %.8f acc: %.2f' % (e, tloss/num_batches, acc_train)\n",
    "            self.saver.save(s, '/tmp/model.ckpt')\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Avalia rede neural em colecao de teste\"\"\"\n",
    "        with tf.Session() as s:\n",
    "            self.saver.restore(s, '/tmp/model.ckpt')\n",
    "            acc_test = s.run(self.acc, \n",
    "                             feed_dict = {self.X: X_test, self.y: y_test})\n",
    "        return acc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando nossa rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = FeedforwardNeuralNet(input_dim = 28*28)\n",
    "\n",
    "model.add(units=300, activation = Activation('sigmoid'), name = 'h1')\n",
    "model.add(units=100, activation = Activation('sigmoid'), name = 'h2')\n",
    "model.add(units=10, name = 'out')\n",
    "\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 loss: 0.75985401 acc: 0.74\n",
      " 1 loss: 0.39017252 acc: 0.98\n",
      " 2 loss: 0.31436071 acc: 0.90\n",
      " 3 loss: 0.27114505 acc: 0.96\n",
      " 4 loss: 0.24173559 acc: 0.92\n",
      " 5 loss: 0.21857499 acc: 0.96\n",
      " 6 loss: 0.20063361 acc: 0.94\n",
      " 7 loss: 0.18548460 acc: 1.00\n",
      " 8 loss: 0.17280212 acc: 0.98\n",
      " 9 loss: 0.16134568 acc: 0.98\n"
     ]
    }
   ],
   "source": [
    "model.fit(mnist.train, n_epochs = 10, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.94160002"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiper parâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Número de camadas: dependendo da complexidade do problema, mais é melhor. Em geral, se ganha mais com mais camadas que com mais neurônios. Além disso, elas permitem a transferência de aprendizado, já que você não precisa treinar a rede toda. Pode sempre aproveitar camadas anteriores pré-treinadas. Uma estratégia comum é usar muitas camadas e checar _overfitting_. Se ele ocorre, começamos a fazer regularização."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Número de neurônios por camada_: quanto mais melhor. Contudo, dado a natureza hierárquica do aprendizado, é comum uma estrutura geral de funil, pois é mais provável que usemos mais atributos de baixo nível que de alto nível. Uma estratégia comum é usar muitos neurônios e verificar _overfitting_. Se ele ocorre, então regularizamos para diminui-lo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Funções de ativação_: vamos usar quase sempre ReLU e suas variantes. Em classificação, a última camada é normalmente _softmax_. Em regressão, nenhuma ativação é usada na última camada. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando FNNs profundas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing and Exploding Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métodos dependem de gradientes. Contudo, gradientes podem desaparecer ou explodir, inviabilizando o aprendizado. Uma das razões para saturação são o uso combinado de funções de ativação que saturam (como sigmoids e variantes) e métodos de normalização de média 0, variância 1. \n",
    "\n",
    "O efeito dessa combinação é que a variância da saída de cada camada é bem mais alta que da entrada. Logo, após várias camadas, esta variância leva a ocorrência de valores críticos.\n",
    "\n",
    "Uma solução para este problema é o uso de diferentes funções de ativação (ReLU e família) e diferentes técnicas de inicialização (Xavier e He)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inicialização de Xavier (ou Glorot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variância da saída deveria ser igual da entrada; o mesmo vale para variância dos gradientes antes e depois de passar por uma camada na volta. Embora não seja possível garantir isso, um bom compromisso é alcançado se os pesos forem inicializados assim:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* normalização média 0, variância $\\sigma$: $\\sigma = \\alpha \\sqrt{\\frac{2}{n_{entradas}+n_{saidas}}}$\n",
    "* normalização uniforme entre $-r$ e $+r$: $r = \\alpha \\sqrt{\\frac{6}{n_{entradas}+n_{saidas}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onde $\\alpha$ é 1 para ativação sigmoid, 4 para tangente hiperbólica e $\\sqrt{2}$ para ReLU. Quando o número de entradas e saídas é basicamente o mesmo, vc pode usar:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* normalização média 0, variância $\\sigma$: $\\sigma = \\alpha \\frac{1}{\\sqrt{n_{entradas}}}$\n",
    "* normalização uniforme entre $-r$ e $+r$: $r = \\alpha \\frac{\\sqrt{3}}{\\sqrt{n_{entradas}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Activation(object):\n",
    "    \"\"\"Funcao de ativacao\"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "                \n",
    "    def init(self, n_inputs, n_outputs):\n",
    "        \"\"\"Xavier initialization\"\"\"\n",
    "        if self.name == 'sigmoid':\n",
    "            stddev = np.sqrt(2. / (n_inputs + n_outputs))\n",
    "        else:\n",
    "            stddev = 2. / np.sqrt(n_inputs + n_outputs)\n",
    "        return tf.truncated_normal((n_inputs, n_outputs), stddev = stddev)\n",
    "        \n",
    "    def fire(self, ypred):\n",
    "        if self.name == 'sigmoid':\n",
    "            return tf.nn.sigmoid(ypred) \n",
    "        elif self.name == 'relu':\n",
    "            return tf.nn.relu(ypred)\n",
    "        else:\n",
    "            return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = FeedforwardNeuralNet(input_dim = 28*28)\n",
    "\n",
    "model.add(units=300, activation = Activation('sigmoid'), name = 'h1')\n",
    "model.add(units=100, activation = Activation('sigmoid'), name = 'h2')\n",
    "model.add(units=10, name = 'out')\n",
    "\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 loss: 1.04424335 acc: 0.92\n",
      " 1 loss: 0.39326734 acc: 0.92\n",
      " 2 loss: 0.32933853 acc: 0.90\n",
      " 3 loss: 0.29854138 acc: 0.86\n",
      " 4 loss: 0.27687091 acc: 0.98\n",
      " 5 loss: 0.25798153 acc: 0.92\n",
      " 6 loss: 0.24208367 acc: 0.98\n",
      " 7 loss: 0.22774169 acc: 1.00\n",
      " 8 loss: 0.21356254 acc: 0.96\n",
      " 9 loss: 0.20139625 acc: 0.94\n"
     ]
    }
   ],
   "source": [
    "model.fit(mnist.train, n_epochs = 10, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.94319999"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de ativação não saturáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5, 5, -0.2, 1.2]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAENCAYAAADqsBXqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11583b6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "sz = sigmoid(z)\n",
    "dsz = np.gradient(sz, z)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k--')\n",
    "plt.plot([-5, 5], [1, 1], 'r--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k--')\n",
    "plt.plot(z, sz, \"b-\", linewidth=2)\n",
    "plt.plot(z, dsz, \"g-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturacao', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturacao', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate(r\"$\\sigma'$\", xytext=(2, 0.2), xy=(0, 0.5), fontsize=20, ha=\"center\")\n",
    "plt.title(r\"Funcao de ativacao Sigmoid $\\sigma$\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O grande problema de sigmoids (incluindo a tangente hiperbólica) é a existência de regiões de saturação. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para lidar com este problema, foram introduzidas as Rectifier Linear Units -- ReLUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAAELCAYAAAAFjkesAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niESQwllEJIIUziIiEfT//3M7NV4lL+8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11582d950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rz = relu(z)\n",
    "drz = np.gradient(rz, z)\n",
    "\n",
    "plt.plot(z, rz, \"b-\", linewidth=2)\n",
    "plt.plot(z, drz, \"g-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k--')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k--')\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.title(\"Funcao de ativacao ReLU\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = FeedforwardNeuralNet(input_dim = 28*28)\n",
    "\n",
    "model.add(units=300, activation = Activation('relu'), name = 'h1')\n",
    "model.add(units=100, activation = Activation('relu'), name = 'h2')\n",
    "model.add(units=10, name = 'out')\n",
    "\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 loss: 0.27734450 acc: 0.96\n",
      " 1 loss: 0.12024473 acc: 1.00\n",
      " 2 loss: 0.08184821 acc: 1.00\n",
      " 3 loss: 0.06092479 acc: 1.00\n",
      " 4 loss: 0.04713256 acc: 1.00\n",
      " 5 loss: 0.03673371 acc: 1.00\n",
      " 6 loss: 0.02834832 acc: 1.00\n",
      " 7 loss: 0.02083865 acc: 1.00\n",
      " 8 loss: 0.01569651 acc: 1.00\n",
      " 9 loss: 0.01182602 acc: 1.00\n"
     ]
    }
   ],
   "source": [
    "model.fit(mnist.train, n_epochs = 10, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98079997"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um problema com ReLUs é que elas podem ainda saturar em 0. Assim, novas variantes foram criadas como Leaky ReLU, ELU (Exponential Linear Unit) e SELU (Self-normlizing Exponential Linear Unit).\n",
    "\n",
    "A ideia da leakyReLU e eLU é que não haja saturação em 0. A leakyReLU usa uma estratégia linear e a eLU, uma estratégia exponencial. Como a eLU sempre é melhor que a leakyReLU, vamos mostrar o seu efeito a seguir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAENCAYAAADAAORFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XecVNXdx/HPj95RRFABxUYsWAOIorKiRsQQg9iJXR41\nalA0qGhcfGKPij4aNSqSACFGEUGsKLA2RFFBEUFAekfpbduc548zwLDsbGFn5s6d+b5fr/vaKXfm\n/vay++XsmXPPMeccIiISLtWCLkBERCpP4S0iEkIKbxGREFJ4i4iEkMJbRCSEFN4iIiGk8BYRCSGF\nt4hICCm8JWHMbIyZvRzQsQeb2ZtBHDuTmFkjM1tuZgcFXQuAmb1uZn2CriMdKbyTKBooETMrjn7d\ndvvooGsLKzObYGb/V8pTfwL+kOp6Ei3Oz0zEzCaW2KfU/6jinR8zu8LMNlSghH7Ah865ubv/XSTU\nA0B/M6sbdCHpRuGdfB8A+8Rs+wLfB1pRBnLObXDOrQ+6jgQp+TOzD9AtAe9b5lwYZlYT6A2k/K8n\nM2tgZq+ZWcvYx51z3wCrgAtTXVO6U3gnX75zbpVzbmXMFoHSW0mxraro8383swfMbJWZrTCzv5U8\ngJndZmazzGyrmS00swdinjvLzD42s9Vm9ouZvWdmh5V4fS0zezL65/IWM/vczDqV9U2ZWV0z+6eZ\nbTCzZWZ2Vyn79DOzOWa22cy+NbNe5Z2ssuo1s8FAZ+DGmNbp/jHnbYyZ9Y5+H1bifYeb2ahKnpeU\nn9eo0n5m1lbgdVV1JlAHmJCCY21nZtcAtwHnUXomjQYuSWVNYaDwTn+XAoXAicCNwC1mdtG2J83s\nIeBu/J+Xh+N/ARbGvL4+MBBohw++tcAYM6sRs8/fgAuAK4FjgWnAe2bWvIy6HgdOB3pEvx4HnBpT\n1wPAVcAN0boeAp43s7PL+X7LqrcP8DkwGGiO/ytmUcxrHfAq0BgfRNtqqQ/8DhhaweMEeV6DdAow\nxaV4tjrn3CDn3H2AxdnlS6CTmSmvYjnntCVpw4dMIbAhZns75vkJwP+V8po3Y57/rMTzY4EXorfr\nA1uA3pWoqT5QBJwUvV8PyAd6xexTDZgD/G8Z77EVuLjEY2vwf3LXAzYDnUq8biDwViXPYcl6dzln\npZy314F/xTz3h2httSpynKDOaxk/M+uBh0r7Xkt5fbzzcwWwvpzvYQQwpJTHbwJuAa4DngNqRx9v\nXWK/PYEHY7aHolvs/f8Fqsc5fgTYv5THjwOKgVZV/Z3MpC22lSDJ8RG+H3Fbq2JLJV//XYn7S4Fm\n0dtHALWA8fFeHB01cD/QAdgbHyAG7A9MBA4GakRvA+Cci5jZ59H3L83BQE1gUsxrNpnZtJi66uBb\nmbGvqwHMi1drBeutiGHAP82sjnNuK/6vl9edcwUVPE4hwZzXbUr+zIBv2SdbI2Bl7ANm9gyw0Dn3\naPT+I8Bt0Q9QV8Xu65xbA/RPQl3r8eeiMTv/pZXVFN7Jt9k5Fy+wIuz6p2LNEvcLS9x3VK676238\nn/v/AyzBtw5n4MOpPLv75/O2+n7Lrr9sJb+fkqpSb+x7FAPnmtl44AxiulEqeJx4f8Inos7yzmtZ\nPzPlWY8PuZL2ANaV89p1QINtd8zs1/hun9gPEWcD5wBbnHMDd7PGymqEP2ep+A8sNBTewVqF77eN\ndQzltE5jzAAK8H3OP5V80syaAL8CrnfOfRR97Hh2/nf/CR+onbYdN9q3eCLw7zjH/QkfVh2B+dHX\n1Afa4rsFfsB3GbTedtyKqGC9BUD1st7HOVdgZq/hu0v2BpbF1lGB48yI1p/q85oIPwKlfa7w6+hz\nZZmDr2+bHCDPORf7H24hvhvj1pIvjp6X28t4f8P/3AxwzhWXU0usA/DdcEsr8ZqMp/AO1nhgoJl1\nx/9iXQe0ooLh7ZzbaGZPAQ+ZWQHwMbAX8Gvn3PP4ft6fgd5mthjfgnqUmNavc26zmT0HPGJmv0SP\n3RffNfNsnONuMrNB0df8DCwD/kK0xR2t6zHgsWhgfYxv0XUEip1zL8X5lsqtF/+fRQczOwDY6Jz7\nJc57DQPGAQcC/6nMcYI6rzFql/KhZrFz7ueY+43M7JgS+6zF90nfaH4U00v4zybOAS4Cupdz3E+A\nP5qZOd/ZvAzYVGKfQ4EZzrn5ZlY9NoSdc6tJTrdJB2Cii47SkqigO90zeaOMD5aiz9cAnsb3M64E\ncvEf+I2OPl/mB5oxj/XDt5q2AguAv8Y8l4PvN98c/Xom/k/ry2P2qQU8gf9l3YLvpz2xnO+tHvDP\n6Hstx4/MeBN4OWafG/Fj2rcAK4D3gdPLed8y68WHx2f4UCkm+gFXnPMyD9/Sa7sbx7GAzuvg6PdV\ncltYgX1ejT7/a+Dd6HHX4EfodK/Az2vN6L/T6THn4H7gWqAXfvTQAfguo1uAfRL0e3Ip/j+0YmA4\n8McSz08Drgj69zndNoueHBERzOyvwEHOuXLH5KdCtN/9HeBA59zmoOtJJwpvEdnOzBoDM/HDPAO/\nRN7MXsf3uz8ddC3pRuEtIhJCumJJRCSEFN4iIiGUsqGCZqb+GRGR3eCc2+WisZS2vIMeWpObmxt4\nDemy6Vzs2Dp37hx4DemypePPxfDhDnBUr+6YMCH7zkU86jaRrPfRRxW+CFRS7Jtv4Jpr/O0nn4Sc\nnEDLSStV7jYxs9r4K9BqRbfRzrlkXGUlIllk5Ur4/e9hyxa4+mq48cagK0ovVQ5v51y+mZ3m/OXA\n1YHPzKyTc+6zBNSXUDn6b3s7nQspTbr8XBQWwgUXwKJF0LEjPPssWHlThSVYupyLeBI6ztvM6gF5\nwJXOuR9KPOcSeSyRRDGzMvsWJfVuvNEH9n77wVdfwb4lp2/LItGfz+R8YGlm1cxsCn6Oi7ySwS0i\nUlEvvuiDu1YtGDkyu4O7LAkJb+dcxDl3HH52tVPNrHMi3lckFXJzc4MuQaI++2xH3/Y//gEnnBBs\nPeksoeO8nXPrzext/Lp+u3yEP2DAgO23c3Jy0r5PSbJD7M+lBGfxYujZ0/d39+kDV14ZdEXByMvL\nIy8vr9z9qtznbWZNgULn3Dozq4uf9vM+59y4Evupz1tESrVlC5x6qu/f7tIF3n8fami1ASB+n3ci\nTs++wL/ML1ZYDRhaMrhFROJxDv7nf3xwt24N//2vgrsiUjaroFreIlKaJ56A226DevXg88/h6KOD\nrii9xGt5K7xFJDAffABdu0IkAq+9BuefH3RF6SepQwVFwkwfWAbjp5/goot8cN9zj4K7stTylqyn\ni3RSb8MGOPFEmD4duneHUaOgmpqSpVLLW0TSQiQCV1zhg/uww2DYMAX37tApE5GUuv9+eOMNaNwY\nRo+GRo2Criic1G0iWU/dJqkzahT06OEnmXrnHf9hpZRN3SYiEqjp0+Gyy/zthx5ScFeVwluynuY2\nSb7Vq+Hcc2HjRrj4YujXL+iKwk/dJiKSVEVFcM45MHYsHHccfPqpvyBHKkbdJiISiLvu8sHdtKn/\noFLBnRgKbxFJmn//Gx57zM9VMmIEHHBA0BVlDoW3iCTF11/Dtdf62089BZ01y39Cqc9bRBJuxQpo\n187P0X3ttfDCC6lfgzJTqM9bJA7NbZJYBQV+UYXFi+Gkk+CZZxTcyaCWt2Q9XaSTWNdf75cwa9HC\nz9G9zz5BVxRuanmLSNI9/7wP7tq1/cgSBXfyKLxFJCE++QRuvtnffvFFaN8+2HoyncJbRKps0SI/\nH3dREdx6647L4CV51OctWU993lWzeTOccgp88w2ccQa8+67WoEwk9XmLxKG5TXafc9C7tw/ugw7S\n4sGppJa3iOy2xx6DP/8Z6teHSZOgbdugK8o8WoBYRBLq/fehWze/Ms7rr8N55wVdUWZSt4mIJMyc\nOX5q10gE7r1XwR0EtbxFpFLWr4eOHWHGDD9H98iRWoMymdTyFpEqi0T8MMAZM+CII2DIEAV3UHTa\nJetpbpOKu+8+ePNN2GMPLR4cNHWbSNbTOO+KGTnSTzhVrZpfPPiss4KuKDuo20REdtu0aXD55f72\nI48ouNOBWt6S9dTyLtvq1X6ekrlzoVcvGDpUU7ymksZ5i8Sh8I6vqAjOPhs+/BCOP94vHly3btBV\nZRd1m4hIpfXr54O7WTMYNUrBnU4U3pL1NLdJ6YYMgYEDdywe3KpV0BVJLHWbiMguvvwSTj0V8vP9\nAgvXXRd0Rdkrad0mZtbSzMab2XQzm2Zmf6rqe4pIcJYv95e75+f70FZwp6cqt7zNbB9gH+fcVDNr\nAHwNnOucm1liP7W8RdJcfj506QITJ8LJJ8O4cVCrVtBVZbektbydc8udc1OjtzcCM4AWVX1fEUkt\n5+Cmm3xwt2zp+7kV3OkroR9Ymllr4Fjgi0S+r4gk33PPwUsvQZ06fmRJ8+ZBVyRlSdiaF9EukxFA\nn2gLfBexc0jk5OSQk5OTqMOL7LYBAwZk/fwmH30Effr42y+9BL/+dbD1ZLO8vDzy8vLK3S8ho03M\nrAbwFvCuc+6pOPuoz1vSUrZfpLNgAbRrBz//DLffDn/7W9AVSaykXmFpZkOAn51zfcvYR+EtaSmb\nw3vzZv/B5JQp8Jvf+AmnqlcPuiqJlcyhgp2AXkAXM5tiZt+YWdeqvq+IJJdzcM01PrgPPhheeUXB\nHSZV7vN2zn0G6J9cJGQefdQHdoMGfm7uPfcMuiKpDF1hKVkvG7tN3nkHfvtb3/oeNcovZybpSRNT\nicSRbXObzJoFl17qg/u++xTcYaWWt0gWWb8eTjgBZs6EHj38hThagzK9qeUtkuUiEb+YwsyZcOSR\n8K9/KbjDTP90Ilni3nvhrbf8B5OjR0PDhkFXJFWhbhORLDBiBFxwgW9pv/8+nHFG0BVJRanbRCRL\nffcdXHGFv/23vym4M4XCW7JeJs9r8vPPfjTJ5s1w2WVw661BVySJom4TyXqZOs67qAjOOgvGj/dz\nl3z8sdagDCN1m4hkmdtv98HdvDm88YaCO9MovEUy0ODB8NRTULMmvP66X1xBMovCWyTDfPEFXH+9\nv/33v0OnTsHWI8mh8BbJIEuX+isnCwrghhugd++gK5JkUXhL1suUuU3y86FnT1i2DE49FZ58MuiK\nJJk02kQkA2ybm3vwYGjVCr76Cpo1C7oqSQSNNhHJYM8844O7bl0/xauCO/Op5S0SchMmwJlnQnEx\nDB8Ol1wSdEWSSGp5i2Sg+fP9nCXFxdCvn4I7m6jlLRJSmzbBSSf5uUu6dvUzBmoNysyjlrdIHGGc\n28Q5uOoqH9yHHgr/+Y+CO9uo5S1ZL4xzmzz0EPTv7+fk/uILOPzwoCuSZInX8lZ4S9YLW3i//TZ0\n7+5vjx6947ZkpnjhXSOIYkRk98ycuWPx4L/+VcGdzdTylqwXlpb32rV+8eBZs/yVlK+9BrZLe0wy\njT6wFAmx4mK/ePCsWXDUUfDPfyq4s53CW7JeGOY2+ctf4J13oEkT38/doEHQFUnQ1G0ikub++1+4\n+GI/FPD99+H004OuSFJJ3SYiITR1qh/PDfD44wpu2UEtb5E0tWoVtG8PCxb41d8HD1Y/dzbSOG+R\nECks9JNNffQRdOjgv9apE3RVEgR1m4iESN++PrD32ccvHqzglpIU3pL10m1uk0GD/PzctWrByJGw\n335BVyTpKCHdJmY2CPgtsMI5d3ScfdRtImkpnS7S+fxz6NzZd5sMGgRXXx10RRK0ZHebDAbOStB7\niWSlJUvgvPN8cN90k4JbypaQ8HbOfQqsScR7iWSjrVv9qu/Ll0NODjzxRNAVSbrTxFQiwJBvh/DL\n5l8oKC5I+bEdfp6Sb2rBHr+Fk2+CJ75IeRkSMgpvyVoFxQU8/OnDAFwx6opgi2kCnAlrgfsnBVuK\nhENKwzv2U/2cnBxycnJSeXiR7YoiRVw04iJGzRwFnaHbod04ZM9DqFMjtWPyFiyAV18DF4Huv4PD\nD0vp4SUNLfx2IQu/Xbj9/kQmlrpfwi7SMbPWwBjn3FFxntdoE0kLzjl6j+nNoCmD2KPOHoy8cCSn\nHXhayuuYO9dfQbl6Ndx1Fzz4YMpLkBBI6mgTMxsOTATamNlCM7sqEe8rkgx3j7+bQVMGUadGHd66\n5K1AgnvjRvj9731wd+vmF1YQqQxdHi9ZZeDnA+k7ti/VrTqjLx7NOW3OSXkNzsEFF8Drr8OvfuXX\noGzcOOVlSEjo8njJesO+G0bfsX0BePnclwMJboAHHvDB3aiRn5tbwS27Q+EtWWHMj2O4arTvzXv8\nN49z+TGXB1LHm2/6hRXMYPhw3/IW2R0Kb8l4o2eOpuerPSmKFHFHpzvoe2LfnZ5P1dwmM2bAH/7g\nbz/wAJwTTMNfMoT6vCWjjZwxkotGXERRpIi+Hfvy2G8ew0pMip2KuU3WrPGLB8+eDRdeCK+8orm5\npWLU5y1Z59Xpr3LhaxdSFCmi30n9Sg3uVCguhksv9cF9zDHw8ssKbqk6hbdkpGe+fIaLR1xMsSum\n/8n9efiMhwMJboD+/eG992CvvWDUKKhfP5AyJMOo20QyinOO/uP68/Bn/rL3+0+7n/6n9C8zuJPZ\nbfKf//hWd/Xq8OGHftIpkcqI122iuU0kY+QX5dN7TG+GfjeU6ladl373Elcee2Vg9XzzzY5pXQcO\nVHBLYim8JSMs27CM8187n4mLJlK/Zn1GXDiCrod0rdBrc3NzE17PypX+CsqtW32A33RTwg8hWU7d\nJhJ6Xyz+gvNePY+lG5bSslFLRl88muP3PT6wegoK4Iwz4JNPoGNHyMuD2rUDK0dCTt0mknGcc7z4\nzYvc/O7NFBQXcMr+pzDiwhE0q98s0LpuucUH9377+TUoFdySDApvCaW1W9fSe0xvRvwwAoA/tvsj\nA7sOpFb1WoHW9cIL8NxzOxYP3nffQMuRDKbwltCZuGgil75+KQvWLaBhrYY8d85z9Dq6V9Bl8dln\nO/q2X3jBX5QjkiwKbwmNLYVb+MuEvzBw0kAiLkK7/drxSs9XOLjJwUGXxqJF0LOnXzy4Tx+4IuCF\neSTz6SIdCYVPFnzCMc8fw+OfPw7AHZ3u4LOrP0tIcFd1bpMtW/ziwStWQJcu8NhjVS5JpFwabSJp\nbdWmVfQf159BUwbhcBy595EMPncw7Vu0T9gxqnKRjnNw+eUwbBgceCBMnuyvpBRJFI02kVApjhTz\n/FfPc8+Ee1i7dS01q9Xkjk53cM+p91C7RvoM3xg40Ad3vXr+0ncFt6SKWt6SVpxzvDP7He4cdyff\nr/wegDMPOpOnz36aXzVNzuTXu9vyHjsWzj4bIhEYMcL3eYskmlrekvY+X/Q5d467k48XfAzAAY0P\n4ImznqDHYT0Cm1Qqnjlz4OKLfXDfc4+CW1JPLW8JlHOOCfMn8OAnDzJu3jgAmtRtwj2n3MMN7W+g\nTo06Sa+hsi3vDRv8lZM//ADdu/vukmr66F+SRC1vSSsRF2HMj2N48NMH+XLJlwA0rNWQmzvcTL9O\n/WhcJ3ULO1ZmbpNIxH9A+cMPcPjhvr9bwS1BUMtbUmrd1nUM/W4oz05+lhk/zwCgab2m3HLCLdzY\n4Ub2qLNHwBWWbcAAuO8+2GMP+PJLOPTQoCuSTBev5a3wlpSYsmwKz331HMOnDWdT4SYAWjZqyZ9P\n+jPXHn8t9WrWC7jC8r3xBpx3nm9pv/02dK3YpIUiVaJuE0m55RuX8+r0Vxn23TAmL528/fHTWp/G\nDe1u4NzDzg18LpKK+v57uOwyf/vhhxXcEjy1vCWh1m1dxxsz32D4tOGMmzeOiIsAsEedPbjymCu5\nrt11HNb0sICrrJzVq6F9e5g7Fy65BP79b61BKamjbhNJmkXrFjFm1hje/PFNJsyfQEFxAQA1q9Xk\n7EPP5tK2l9L9V91D0TVSUlERdOsGH3wAxx/vp3qtF75vQ0JM3SaSMPlF+UxaPIkP537IW7PfYury\nqdufM4zOB3Sm11G96HlET5rUbRJgpRUzYMCAuPOb3HGHD+699/Z93gpuSRdqeUu5CosLmbp8KuPn\njWf8/PF8suATthRt2f58/Zr1OeuQs+jepjvdDu0W+GIIlRVvnPfQoX5YYI0aMH48nHJKAMVJ1lPL\nWypsyfolTFo8yW9LJvHV0q/YWrR1p33aNmtLl9ZdOPvQs8lpnZOSi2lSafJk6N3b3376aQW3pB+F\ndxaLuAhz18zluxXfbd8mL53M4vWLd9n30CaH0vmAznQ5sAtdDuxC8wbNA6g4NZYv91O85ufDddfB\n9dcHXZHIrtRtkgUKiguYt2Yes36ZxezVs5mxagbfrfyO71d+z+bCzbvs37h2Y05oeQIdW3SkY8uO\ndGjRgb3qZe50ebHdJvn5fk7uiRPh5JNh3Di/pJlIUNRtksEiLsKKjStYuG4hi9YvYuG6hcxfO5/Z\nq2cz+5fZzF87n2JXXOprWzRswdHNj+bo5kdzVLOjOG7f4zis6WFUs+y75ts5uPlmH9wtW/qZAhXc\nkq4SEt5m1hV4Er8yzyDn3COJeN9s55xj9ZbVrNi0ghUbV7B84/Ltt5duXMrCdQt9YK9bRGGkMO77\nGMaBexzIoXsdSpsmbWizVxsf1s2PCsVokGTbNrfJ88/Diy9CnTp+sqnmmdszJBmgyt0mZlYNmAWc\nDiwFJgMXO+dmltgva7tNiiJFbCzYyIb8DazZuobVW1azestq1myJuR3z+Ootq1m5aSUrNq2gKFJU\noWM0rdeU/Rvv77dG/ushTQ6hzV5tOGjPg9JqAYN09PHHcPrpflz3sGHQK/j1jEWA5HabdABmO+cW\nRA/0CnAuMLPMV6UB5xwFxQVsLdrKlqItbC3aGnfbUrjz85sLN7OhYAMbCzb6YI7e3pC/Yaf7Gws2\n7jJSozIa1W7EPg32oXn95jRv0Jzm9ZuzT4N92LfBvtvDulXjVqG8ACZdLFgA55/vg/v22xXcEg6J\nCO8WwKKY+4vxgb6LJyc9SXGkmIiLUOyKK327sLiQIldEYXEhhZHCXb4WRSr33LYrAZOtmlWjQa0G\nNKzVkD3r7smedfakSd0mNKnbZPvtPevu/Fiz+s1oVr8ZdWvWTUmN2WrzZj+yZNUq+M1v/LwlImGQ\n0g8sb33/1lQerkJqVa9FnRp1St3q1qhb5nMNazekYa2GPphrN9we0A1qNdjpsbo16qbdSjDiP6C8\n5hqYMgUOOQReeQWqVw+6KpEKcs5VaQM6Au/F3L8TuKOU/Vxp2ymXn+LuHX+vuy/vPnf/R/e7Bz9+\n0D3y6SPuzKvPLHX/c6871w37dpj77/f/dSN/GOnG/DjGvTf7PXfZny4rdf+b/nyTm7t6rlu0bpFb\nvmG5+2XzL2791vXu7r/cXer+ubm5rjS5ubnaP8P2v/9+58C5Bg2cu+GG4OvR/to/NzfXTZgwweXm\n5m7fAOdKyd5EfGBZHfgR/4HlMuBL4BLn3IwS+7mqHkskUYYN81O8msGFFw7glVcGBF2SSKmSOqtg\ndKjgU+wYKrhLz6HCW9LFhAlw1llQWAhPPQV9+uze6vEiqaApYUWA6dOhUydYtw5uvRWeeKLyCxCL\npJLCW7LevHlw6qmweLFfzuy11/ySZgpvSWcKb8lqixf74J43z89ZMnYs1I2OwlR4SzqLF97ZN4GF\nZJ3ly/3Vk/PmQYcOfvHguho+LyGn8JaMtmIFnHEGzJoFxx4L770HjRrtvM+2uU1EwkTdJpKxFizw\nwT1nDhxxBOTl+eXMRMJE3SaSVWbO9H3bc+bAccf54YEKbskkCm/JOF9/vWNUyckn++BuFq5lNUXK\npfCWjPLGGz64V62Crl3h/fehceOgqxJJPIW3ZATn4NFHoWdPP1PglVfC6NFQTzPlSoZSeEvobdni\nZwe84w4f4g89BC+/XPElzAYMGJDU+kSSQaNNJNR++skvpDB1qh+7PXSob31Xhi7SkXSm0SaScd54\nA44/3gf3wQf7hYMrG9wiYaXwltDZtAn++Ec/P8n69f7r11/7i3BEskVKV9IRqapJk+Dyy2H2bKhZ\n0y9bduutfl5ukWyilreEwubNcNddfjrX2bPhqKNg8mTo21fBLdlJ4S1pb+xYH9YPP+xHk9x+O3z5\nJRxzTGLeX3ObSBhptImkrUWLoF8/vzAw+AD/xz/gxBODrUsklTTaREJj40a4915o08YHd926vtX9\n9dcKbpFt9IGlpI2CAn9xzV//CkuX+scuvBAeeQRatw60NJG0o/CWwBUVwZAhPrTnz/ePtWsHAwf6\niaVEZFcKbwnMhg2+pT1woJ97G+Dww+G++/zFNtXUqScSl349JOWWLIE774RWreCWW3xwt2kDw4bB\ntGlwwQWpDW7NbSJhpNEmkhKRiF/JZtAgv2p7YaF//OST/dC/7t2Da2lrbhNJZ/FGm6jbRJJqyRL4\n1798aM+d6x+rVs23rm+7DU44Idj6RMJK4S0Jt2YNjBoFr77qL7CJRPzjrVrBVVf5TaNHRKpG4S0J\nsXo1vPmmD+wPPvAjSABq1IAePeDaa+HMM6F69WDrFMkUCm/ZLc75qVjffRfeeQc+/3xHC7taNTj9\ndD9Gu0cPLfwrkgwKb6kQ52DePPjoI7+NHQvLlu14vkYNOO0035fdo0e4FvzV3CYSRhptIqUqLPTD\n9iZPhk8/9SNFFi/eeZ8WLaBbNzj7bN/SbtQokFJFMppGm0hckQjMmuWDets2ZQrk5++8X5MmfmX2\nnBzo0gXattV0rCJBUXhnEedg4UKYPh2+/95/nT4dfvjBL+Jb0iGHQPv20LGjD+y2bXXVo0i6UHhn\nmKIiH9Bz5/rFebdtc+fCnDl+xr7StGzp5xNp395v7drBnnumtnYRqTiFd4gUFcGKFf7Cl9K2BQv8\ntm2YXmn23tu3oI880n9t2xaOOEJBLRI2VQpvMzsfGAAcDrR3zn2TiKKygXO+q2LtWvjlF1i1Cn7+\nedev226vWuWDe9twvLK0aAEHHeRXVD/44J1vN22a/O8tbAYMGKD5TSR0qjTaxMx+BUSAfwC3lxXe\nYR9tUljow3bz5vK39ev9tm5d/G39+rJbyPE0a+bDubStVSs48EC/eIFUnOY2kXSWlNEmzrkfo29e\noTEHS5fbbBeRAAAFSElEQVRCcfHOW1HRro/Fe7y8fYuK/IT+Vdny8/3XkkG9O0Fbnjp1/PC6pk19\nd0bTpjvfLvm1eXOoVSvxdYhI+KS0z7tFi1QeLbGqVYP69aFevbK3unWhYUNo3HjnrVGjXe/Xrh30\ndyUioeWcK3MDPgC+i9mmRb92j9lnAnB8Oe/jcmO2Cb7b1z29V6478kjnjj7aueOOc65dO+c6dnRu\nUKtc56L7xG7D2+S6Hj2cO/985y66yLlevZy7/HLnRh1b+v4fnZbrHn3UuSefdO7ZZ5176SXnhgxx\nblrP0vdf0jvXffutc7NnO7dkiXNr1jiXn+9c5N7S93e5ua5Uudo/LPuz7fE0qUf7Z/f+EyZMcLm5\nuds3H9O7ZmpCrrA0swnAbS6D+7wlc6nPW9JZKlaP17V2Ekqa20TCqKqjTX4PPA00BdYCU51zZ8fZ\nVy1vEZFKitfy1sRUIiJpLBXdJiIikiIKbxGREFJ4i4iEkMJbsp7mNZEw0geWkvU0zlvSmT6wFBHJ\nIApvEZEQUniLiISQwltEJIQU3pL1NLeJhJFGm4iIpDGNNhERySAKbxGREFJ4i4iEkMJbRCSEFN6S\n9TS3iYSRRptI1tPcJpLONNpERCSDKLxFREJI4S0iEkIKbxGREFJ4S9bT3CYSRhptIiKSxjTaREQk\ngyi8RURCSOEtIhJCCm8RkRBSeEvW09wmEkYabSJZT3ObSDrTaBMRkQyi8BYRCSGFt4hICFUpvM3s\nUTObYWZTzex1M2uUqMJERCS+qra8xwJHOueOBWYDd1W9JJHU0twmEkYJG21iZr8HejrnLovzvEab\niIhUUipGm1wNvJvA9xMRkThqlLeDmX0ANI99CHDA3c65MdF97gYKnXPDk1KliIjspNzwds6dWdbz\nZnYl0A3oUt57xV7JlpOTQ05OTnkvERHJKnl5eeTl5ZW7X5X6vM2sK/A4cKpz7pdy9lWft4hIJSWr\nz/tpoAHwgZl9Y2bPVvH9RFJOc5tIGGluE8l6mttE0pnmNoEK9SNlC50LKY1+LnZI93Oh8M5SOhdS\nGv1c7JDu5yKrwltEJFMovEVEQiilH1im5EAiIhmmtA8sUxbeIiKSOOo2EREJIYW3iEgIZW14m9lt\nZhYxsyZB1xIULabhp3gws5lmNsvM7gi6nqCYWUszG29m081smpn9KeiagmZm1aJXjr8ZdC2lycrw\nNrOWwJnAgqBrCVhWL6ZhZtWAZ4CzgCOBS8zssGCrCkwR0Nc5dyRwInBjFp+LbfoAPwRdRDxZGd7A\nQODPQRcRNOfch865SPTuJKBlkPUEoAMw2zm3wDlXCLwCnBtwTYFwzi13zk2N3t4IzABaBFtVcKIN\nvG7AS0HXEk/WhbeZ/Q5Y5JybFnQtaSYbF9NoASyKub+YLA6sbcysNXAs8EWwlQRqWwMvbYfjlTuf\ndxiVsYDEPUB/fJdJ7HMZS4tpSGWYWQNgBNAn2gLPOmZ2DrDCOTfVzHJI04zIyPCOt4CEmbUFWgPf\nmpnhuwm+NrMOzrmVKSwxZRK5mEYGWgLsH3O/ZfSxrGRmNfDBPdQ5NzroegLUCfidmXUD6gINzWyI\nc+7ygOvaSVZfpGNm84DjnXNrgq4lCJVZTCMTmVl14EfgdGAZ8CVwiXNuRqCFBcTMhgA/O+f6Bl1L\nujCzzsBtzrnfBV1LSVnX512CI03/JEqRrF5MwzlXDNyEH3UzHXgli4O7E9AL6GJmU6I/D12Drkvi\ny+qWt4hIWGV7y1tEJJQU3iIiIaTwFhEJIYW3iEgIKbxFREJI4S0iEkIKbxGREFJ4i4iE0P8D7vtm\nTlxmIKsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1158b1b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ez = elu(z)\n",
    "dez = np.gradient(ez, z)\n",
    "\n",
    "plt.plot(z, ez, \"b-\", linewidth=2)\n",
    "plt.plot(z, dez, \"g-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k--')\n",
    "plt.plot([-5, 5], [-1, -1], 'r--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k--')\n",
    "plt.title(r\"Funcao de ativacao ELU ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fire(self, ypred):\n",
    "    if self.name == 'sigmoid':\n",
    "        return tf.nn.sigmoid(ypred) \n",
    "    elif self.name == 'relu':\n",
    "        return tf.nn.relu(ypred)\n",
    "    elif self.name == 'elu':\n",
    "        return tf.nn.elu(ypred)\n",
    "    else:\n",
    "        return ypred\n",
    "        \n",
    "Activation.fire = fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = FeedforwardNeuralNet(input_dim = 28*28)\n",
    "\n",
    "model.add(units=300, activation = Activation('elu'), name = 'h1')\n",
    "model.add(units=100, activation = Activation('elu'), name = 'h2')\n",
    "model.add(units=10, name = 'out')\n",
    "\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 loss: 0.29014348 acc: 0.98\n",
      " 1 loss: 0.14778161 acc: 1.00\n",
      " 2 loss: 0.10693193 acc: 0.98\n",
      " 3 loss: 0.08373848 acc: 0.98\n",
      " 4 loss: 0.06892067 acc: 1.00\n",
      " 5 loss: 0.05790422 acc: 1.00\n",
      " 6 loss: 0.04876475 acc: 1.00\n",
      " 7 loss: 0.04095161 acc: 1.00\n",
      " 8 loss: 0.03494837 acc: 1.00\n",
      " 9 loss: 0.02995328 acc: 1.00\n"
     ]
    }
   ],
   "source": [
    "model.fit(mnist.train, n_epochs = 10, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97509998"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outro problema é que, na prática, mesmo que a entrada seja normalizada em média 0, variância 1, as camadas seguintes vão perdendo essa normalização (esse problema é conhecido como _internal covariate shift_, ver https://github.com/aleju/papers/blob/master/neural-nets/Batch_Normalization.md).\n",
    "\n",
    "Uma forma de lidar com esse problema é aprender, durante o treino, a média e a variância de cada camada. Assim, é possível re-normalizar a entrada das camadas seguintes. Esses dois novos parâmetros pode ser estimados nos batches, através de uma estratégia que que ficou conhecida como _batch normalization_ -- BN (ver https://github.com/aleju/papers/blob/master/neural-nets/Batch_Normalization.md). O uso de BN virtualmente acaba com os problemas de perda e estouro de gradientes.\n",
    "\n",
    "Mais recentemente (junho de 2017), uma alternativa à BN, a função SELU, foi proposta em um artigo (https://arxiv.org/pdf/1706.02515.pdf) de Günter Klambauer, Thomas Unterthiner e Andreas Mayr. A ideia é que a própria função de ativação já garanta saídas normalizadas, sem necessidade de parâmetros adicionais. Embora muito recente, esta tem se mostrado a melhor solução para o problema em redes profundas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# valores de SELU para media 0, desvio 1\n",
    "# ver paper para detalhes\n",
    "def selu(z,\n",
    "         scale = 1.0507009873554804934193349852946,\n",
    "         alpha = 1.6732632423543772848170429916717):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAENCAYAAADAAORFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKXmLiCSg/wfZPGzPit+VGwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1144c0450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sz = selu(z)\n",
    "dsz = np.gradient(sz, z)\n",
    "\n",
    "plt.plot(z, sz, \"b-\", linewidth=2)\n",
    "plt.plot(z, dsz, \"g-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k--')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'r--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k--')\n",
    "plt.title(r\"Funcao de ativacao SELU ($\\alpha \\approx 1.67$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SeLU tem a propriedade de preservar a variância em torno de 1 (média 0), para redes com muitas camadas (centenas delas), evitando assim o problema de desaparecimento e explosão de gradientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: -0.26 < media < 0.27, 0.74 < stddev < 1.27\n",
      "Layer 20: -0.17 < media < 0.18, 0.74 < stddev < 1.24\n",
      "Layer 40: -0.38 < media < 0.39, 0.74 < stddev < 1.25\n",
      "Layer 60: -0.26 < media < 0.43, 0.74 < stddev < 1.35\n",
      "Layer 80: -0.18 < media < 0.16, 0.72 < stddev < 1.19\n",
      "Layer 100: -0.26 < media < 0.32, 0.74 < stddev < 1.26\n",
      "Layer 120: -0.19 < media < 0.20, 0.78 < stddev < 1.36\n",
      "Layer 140: -0.24 < media < 0.34, 0.74 < stddev < 1.14\n",
      "Layer 160: -0.27 < media < 0.21, 0.72 < stddev < 1.22\n",
      "Layer 180: -0.35 < media < 0.35, 0.77 < stddev < 1.41\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500, 100))\n",
    "for layer in range(200):\n",
    "    W = np.random.normal(size=(100, 100), scale = np.sqrt(1./100))\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=1)\n",
    "    stds = np.std(Z, axis=1)\n",
    "    if layer % 20 == 0:\n",
    "        print(\"Layer {}: {:.2f} < media < {:.2f}, {:.2f} < stddev < {:.2f}\".format(\n",
    "            layer, means.min(), means.max(), stds.min(), stds.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Activation(object):\n",
    "    \"\"\"Funcao de ativacao\"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "                \n",
    "    def init(self, n_inputs, n_outputs):        \n",
    "        if self.name == 'selu':\n",
    "            stddev = np.sqrt(1./n_inputs)\n",
    "        elif self.name == 'sigmoid':\n",
    "            stddev = np.sqrt(2. / (n_inputs + n_outputs))\n",
    "        else:\n",
    "            stddev = 2. / np.sqrt(n_inputs + n_outputs)\n",
    "        return tf.truncated_normal((n_inputs, n_outputs), stddev = stddev)\n",
    "        \n",
    "    def fire(self, ypred):\n",
    "        def selu(z, scale=1.0507009873554804934193349852946,\n",
    "                 alpha=1.6732632423543772848170429916717):\n",
    "            return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))        \n",
    "        \n",
    "        if self.name == 'sigmoid':\n",
    "            return tf.nn.sigmoid(ypred) \n",
    "        elif self.name == 'relu':\n",
    "            return tf.nn.relu(ypred)\n",
    "        elif self.name == 'elu':\n",
    "            return tf.nn.elu(ypred)\n",
    "        elif self.name == 'selu':\n",
    "            return selu(ypred)\n",
    "        else:\n",
    "            return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SNN(FeedforwardNeuralNet):\n",
    "    \"\"\"Rede neural sequencial auto-normalizavel\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        FeedforwardNeuralNet.__init__(self, input_dim)\n",
    "        \n",
    "    def fit(self, train_data, n_epochs, batch_size):\n",
    "        \"\"\"Executa treino da rede neural\"\"\"\n",
    "        self.means = train_data.images.mean(axis=0, keepdims=True)\n",
    "        self.stds = train_data.images.std(axis=0, keepdims=True) + 1e-10\n",
    "\n",
    "        num_batches = train_data.num_examples // batch_size\n",
    "        with tf.Session() as s:\n",
    "            s.run(self.init_op)\n",
    "            for e in range(n_epochs):\n",
    "                tloss = 0.\n",
    "                for i in range(num_batches):\n",
    "                    X_b, y_b = train_data.next_batch(batch_size)\n",
    "                    # scale data!\n",
    "                    X_b_scaled = (X_b - self.means) / self.stds\n",
    "                    _, loss_e = s.run([self.train_op, self.lossf], \n",
    "                                      feed_dict = {self.X: X_b_scaled, self.y: y_b})\n",
    "                    tloss += loss_e\n",
    "                acc_train = s.run(self.acc, \n",
    "                                  feed_dict = {self.X: X_b_scaled, self.y: y_b})\n",
    "                print '%2d loss: %.8f acc: %.2f' % (e, tloss/num_batches, acc_train)\n",
    "            self.saver.save(s, '/tmp/model.ckpt')\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Avalia rede neural em colecao de teste\"\"\"\n",
    "        with tf.Session() as s:\n",
    "            self.saver.restore(s, '/tmp/model.ckpt')\n",
    "            X_test_scaled = (X_test - self.means) / self.stds\n",
    "            acc_test = s.run(self.acc, \n",
    "                             feed_dict = {self.X: X_test_scaled, self.y: y_test})\n",
    "        return acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = SNN(input_dim = 28*28)\n",
    "\n",
    "model.add(units=300, activation = Activation('selu'), name = 'h1')\n",
    "model.add(units=100, activation = Activation('selu'), name = 'h2')\n",
    "model.add(units=10, name = 'out')\n",
    "\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 loss: 0.28395527 acc: 0.98\n",
      " 1 loss: 0.13064164 acc: 1.00\n",
      " 2 loss: 0.08459703 acc: 1.00\n",
      " 3 loss: 0.06127598 acc: 1.00\n",
      " 4 loss: 0.04690037 acc: 1.00\n",
      " 5 loss: 0.03162947 acc: 1.00\n",
      " 6 loss: 0.02420431 acc: 1.00\n",
      " 7 loss: 0.01609532 acc: 1.00\n",
      " 8 loss: 0.01170439 acc: 1.00\n",
      " 9 loss: 0.00869325 acc: 1.00\n"
     ]
    }
   ],
   "source": [
    "model.fit(mnist.train, n_epochs = 10, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97430003"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes da introdução das SELUs, BN era a técnica preferível para lidar com o problema de deriva de co-variância. Atualmente, há uma tendência para preferir a solução com SELUs, pois:\n",
    "\n",
    "* A propriedade de auto-normalização de SELUs é baseada em um arcabouço matemático sólido;\n",
    "* Elas são bem mais simples que BN, uma vez que a função de ativação já produz saídas normalizadas;\n",
    "* Há razoável comprovação empiríca (na medida em que é possível dado o pouco tempo de existência das SELUs) de que elas atingem maior acuidade em menos tempo, quando aplicadas a redes com várias camadas, em comparação a ReLUs, ELUs e BN. \n",
    "\n",
    "_Note que BNs ainda podem ser úteis onde SELUs não puderem ser usadas, como qualquer rede em que uma função muito particular de perda tiver que ser usada que não case bem com SELUs (embora este seja o caso de LSTMs, há atualmente um esforço grande em adaptar SELUs para LSTMs). Além disso, BN também opera como regularizador, o que pode resultar em melhores desempenhos em arquiteturas como as CNNs. A seguir, temos uma explicação um pouco melhor sobre BNs. Vamos ver exemplos de uso mais à frente, com Keras._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch normalisation\n",
    "\n",
    "Este método foi descrito por [Ioffe and Szegedy](https://arxiv.org/abs/1502.03167) para acelerar o treinamento de redes neurais. Ele é baseada num simples modo de falha que atrapalha o treinamento de redes neurais: _à medida que o sinal se propaga pela rede, mesmo se normalizado na entrada, ele pode acabar completamente enviesado em alguma camada oculta, tanto em termos de variância quanto média_ (efeito conhecido como *deriva interna da covariância*). Isto resulta em grandes discrepâncias entre as atualizações de gradientes ao longo de diferentes camadas. Como resultado, somos mais conservadores com a taxa de aprendizado e aplicamos regularizadores mais fortes, o que desacelera o treino.\n",
    "\n",
    "Batch normalisation propõe a normalização das ativações de uma camada para média zero e variância 1, através do batch de dados passando pela rede (ou seja, no treino, nós normalizamos pelos `batch_size` exemplos e, no teste, normalizamos considerando estatísticas derivadas do _treino todo_---uma vez que não conhecemos os dados de teste com antecedência). Nós calculamos a média e a variância para um batch particular de ativações $\\mathcal{B} = \\{x_1, \\dots, x_m\\}$ assim:\n",
    "\n",
    "$$\\begin{align*}\\mu_{\\mathcal{B}} &= \\frac{1}{m}\\sum_{i=1}^{m}x_i \\\\ \\sigma_\\mathcal{B}^2 &= \\frac{1}{m}\\sum_{i=1}^{m}\\left(x_i - \\mu_\\mathcal{B}\\right)^2\\end{align*}$$\n",
    "\n",
    "Nós então usamos estas estatísticas para transformar as ativações de forma que elas tenham média zero e variância um:\n",
    "\n",
    "$$\\hat{x}_i = \\frac{x_i - \\mu_\\mathcal{B}}{\\sqrt{\\sigma_\\mathcal{B}^2 + \\varepsilon}}$$\n",
    "\n",
    "onde $\\varepsilon > 0$ é um pequeno valor que nos protege de divisão por zero (no caso do desvio padrão do batch ser muito pequeno ou mesmo zero). Finalmente, para obter a ativação final $y$, precisamos estar certos que nenhuma propriedade de generalizaçao foi perdida ao executar a normalização---e desde que as operações feitas foram um deslocamento (média) e um escalonamento (desvio), nós permitimos um deslocamento e um escalonamento arbitrários dos valores normalizados para obter o valor final (isso permite a rede, por exemplo, voltar aos valores originais se ela os considerar mais úteis):\n",
    "\n",
    "$$y_i = \\gamma\\hat{x}_i + \\beta$$\n",
    "\n",
    "onde $\\beta$ e $\\gamma$ são parâmetros *treináveis* da operação de batch normalization (otimizáveis via gradiente descendente nos dados de treino). Esta generalização significa que batch normalisation pode se aplicada diretamente às _entradas_ da rede neural (dado que a presença desses parâmetros permite à rede assumir uma estatística de entrada diferente da que nós selecionamos através do processamento manual dos dados).\n",
    "\n",
    "Este método, quando aplicado às camadas de uma rede convolutiva quase sempre levam a maior velocidade do aprendizado. Eles também agem como ótimos regularizadores, nos permitindo um cuidado maior na escolha da taxa de aprendizado, na importância do $L_2$ e uso do dropout (tornando-o muitas vezes completamente desnecessário). Esta regularização ocorre como consequência do fato de que a saída de um único exemplo *não é mais determinística* (já que ela depende do batch inteiro a qual ela pertence), ajudando a rede a generalizar melhor.\n",
    "\n",
    "Note que no artigo original, os autores usam batch normalisation *antes* de aplicar a função de ativação do neurônio (nas combinações lineares computadas dos dados de entrada). Contudo, [em resultados recentes](http://arxiv.org/abs/1511.06422) observou-se que poderia ser mais benéfico (e, no mínimo, tao bom quanto) aplicá-la *depois*.\n",
    "\n",
    "Em Keras, batch normalisation corresponde a uma camada: `BatchNormalization`, para a qual podemos fornecer alguns parâmetros. O mais importante deles é o `axis` (sobre que eixo dos dados as estatísticas deveriam ser computadas). Em particular, quando trabalhando com camadas de convolução, queremos normalizar através dos canais individuais, logo `axis = 1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otimizadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os otimizadores a serem usados com redes neurais são os mesmos que vimos antes, uma vez que o gradiente descendente é um caso particular da propagação retrógrada. \n",
    "\n",
    "Dos vários métodos que vimos antes, a opção em geral será pelo Adam, que combina momento com taxas de aprendizado pora cada dimensão (adaptativo). Eventualmente, quando se quiser um modelo muito esparso (e rápido no teste), deve-se optar pelo FTRLOpimizer (junto com uma regularização $L_1$ -- a ser visto mais tarde)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self, name = 'sgd'):\n",
    "        self.name = name\n",
    "        self.lrate = 0.1\n",
    "\n",
    "    def get(self, lossf):\n",
    "        if self.name == 'sgd':\n",
    "            opt = tf.train.GradientDescentOptimizer(learning_rate = self.lrate) \n",
    "        elif self.name == 'adam':\n",
    "            opt = tf.train.AdamOptimizer() \n",
    "        return opt.minimize(lossf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = FeedforwardNeuralNet(input_dim = 28*28)\n",
    "\n",
    "model.add(units=300, activation = Activation('relu'), name = 'h1')\n",
    "model.add(units=100, activation = Activation('relu'), name = 'h2')\n",
    "model.add(units=10, name = 'out')\n",
    "\n",
    "model.compile(optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 loss: 0.21872286 acc: 0.94\n",
      " 1 loss: 0.08769465 acc: 0.98\n",
      " 2 loss: 0.05881141 acc: 0.96\n",
      " 3 loss: 0.04361934 acc: 0.98\n",
      " 4 loss: 0.03224598 acc: 1.00\n",
      " 5 loss: 0.02709712 acc: 1.00\n",
      " 6 loss: 0.02343668 acc: 0.98\n",
      " 7 loss: 0.01949345 acc: 1.00\n",
      " 8 loss: 0.01588980 acc: 1.00\n",
      " 9 loss: 0.01684354 acc: 1.00\n"
     ]
    }
   ],
   "source": [
    "model.fit(mnist.train, n_epochs = 10, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97790003"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de BNs e SNNs, era comum o uso de corte de gradientes para evitar estouro de gradientes. Atualmente, esquemas de normalização são preferidos. Contudo, é importante entender esta técnica, especialmente para o caso onde a normalização não puder ser trivialmente aplicada, como LSTMs.\n",
    "\n",
    "A ideia de clipping é cortar todos os gradientes que estão abaixo ou acima de valores críticos, de forma que eles assumam os valores críticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self, name = 'sgd'):\n",
    "        self.name = name\n",
    "        self.lrate = 0.1\n",
    "        self.threshold = 1\n",
    "\n",
    "    def get(self, lossf):\n",
    "        if self.name == 'sgd':\n",
    "            opt = tf.train.GradientDescentOptimizer(learning_rate = self.lrate) \n",
    "            return opt.minimize(lossf)\n",
    "        \n",
    "        elif self.name == 'clipped_sgd':\n",
    "            opt = tf.train.GradientDescentOptimizer(learning_rate = self.lrate) \n",
    "            grads_vars = opt.compute_gradients(lossf)\n",
    "            clipped_vals = [(tf.clip_by_value(grad, -self.threshold, self.threshold), var) \n",
    "                            for grad, var in grads_vars]\n",
    "            return opt.apply_gradients(clipped_vals)\n",
    "        \n",
    "        elif self.name == 'adam':\n",
    "            opt = tf.train.AdamOptimizer() \n",
    "            return opt.minimize(lossf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = FeedforwardNeuralNet(input_dim = 28*28)\n",
    "\n",
    "model.add(units=300, activation = Activation('relu'), name = 'h1')\n",
    "model.add(units=100, activation = Activation('relu'), name = 'h2')\n",
    "model.add(units=10, name = 'out')\n",
    "\n",
    "model.compile(optimizer = 'clipped_sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 loss: 0.28610926 acc: 1.00\n",
      " 1 loss: 0.12422661 acc: 1.00\n",
      " 2 loss: 0.08472956 acc: 1.00\n",
      " 3 loss: 0.06252001 acc: 0.98\n",
      " 4 loss: 0.04852289 acc: 1.00\n",
      " 5 loss: 0.03672612 acc: 1.00\n",
      " 6 loss: 0.02852646 acc: 1.00\n",
      " 7 loss: 0.02216926 acc: 1.00\n",
      " 8 loss: 0.01721169 acc: 1.00\n",
      " 9 loss: 0.01256100 acc: 1.00\n"
     ]
    }
   ],
   "source": [
    "model.fit(mnist.train, n_epochs = 10, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97780001"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Política de evolução de taxa de aprendizado (_learning rate schedulling_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ideia aqui é modificar a taxa de aprendizado de acordo com uma política de evolução em lugar de usar uma taxa fixa. Naturalmente, esta é uma possibilidade apenas para os métodos não adaptativos, uma vez que estes modificam a taxa de aprendizado de acordo com o processo de otimização.\n",
    "\n",
    "Para os métodos em que tais políticas fazem sentido, as principais estratégias são:\n",
    "\n",
    "* _Modificação em intervalos pre-determinados_: por exemplo, inicie com 0.1 e, então, após 50 épocas, mude para 0.001. Complicado aqui é definir que valores usar e quando mudar.\n",
    "\n",
    "* _Modificação de acordo com desempenho_: a cada N passos, avalie o erro no conjunto de validação e, se este parou de cair, reduza a taxa por um fator $\\lambda$.\n",
    "\n",
    "* _Escala exponencial_: diminua a taxa de aprendizado por um fator de 10 a cada $r$ passos. Assm, dado o passo $t$, um valor inicial $\\eta_0$, $\\eta(t) = \\eta_0 10^{\\frac{-t}{r}}$\n",
    "\n",
    "* _Escala de potência_: equivalente ao anterior, mas bem mais lento. Nesse caso, $\\eta(t) = \\eta_0 (1 + \\frac{t}{r})^{-c}$, onde $c$ é tipicamente definido como 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self, name = 'sgd'):\n",
    "        self.name = name\n",
    "        self.lrate = 0.1\n",
    "        # clipping\n",
    "        self.threshold = 1\n",
    "        # scheduling\n",
    "        self.initial_lrate = 0.1 # eta0\n",
    "        self.decay_steps = 10000 # r\n",
    "        self.decay_rate = 1./10 \n",
    "        self.global_step = tf.Variable(0, trainable = False) ##\n",
    "\n",
    "    def get(self, lossf):\n",
    "        if self.name == 'sgd':\n",
    "            opt = tf.train.GradientDescentOptimizer(learning_rate = self.lrate) \n",
    "            return opt.minimize(lossf)\n",
    "        \n",
    "        elif self.name == 'clipped_sgd':\n",
    "            opt = tf.train.GradientDescentOptimizer(learning_rate = self.lrate) \n",
    "            grads_vars = opt.compute_gradients(lossf)\n",
    "            clipped_vals = [(tf.clip_by_value(grad, -self.threshold, self.threshold), var) \n",
    "                            for grad, var in grads_vars]\n",
    "            return opt.apply_gradients(clipped_vals)\n",
    "        \n",
    "        elif self.name == 'exp_decay_momentum':\n",
    "            lrate = tf.train.exponential_decay(self.initial_lrate,\n",
    "                                              self.global_step,\n",
    "                                              self.decay_steps,\n",
    "                                              self.decay_rate)\n",
    "            opt = tf.train.MomentumOptimizer(learning_rate = lrate, momentum = 0.9)  ##\n",
    "            return opt.minimize(lossf, global_step = self.global_step) ##\n",
    "        \n",
    "        elif self.name == 'adam':\n",
    "            opt = tf.train.AdamOptimizer() \n",
    "            return opt.minimize(lossf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = FeedforwardNeuralNet(input_dim = 28*28)\n",
    "\n",
    "model.add(units=300, activation = Activation('relu'), name = 'h1')\n",
    "model.add(units=100, activation = Activation('relu'), name = 'h2')\n",
    "model.add(units=10, name = 'out')\n",
    "\n",
    "model.compile(optimizer = 'exp_decay_momentum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 loss: 0.24408096 acc: 0.98\n",
      " 1 loss: 0.09896268 acc: 0.98\n",
      " 2 loss: 0.05616739 acc: 1.00\n",
      " 3 loss: 0.03323490 acc: 1.00\n",
      " 4 loss: 0.01725611 acc: 1.00\n",
      " 5 loss: 0.01007644 acc: 1.00\n",
      " 6 loss: 0.00595098 acc: 1.00\n",
      " 7 loss: 0.00390795 acc: 1.00\n",
      " 8 loss: 0.00296838 acc: 1.00\n",
      " 9 loss: 0.00247777 acc: 1.00\n"
     ]
    }
   ],
   "source": [
    "model.fit(mnist.train, n_epochs = 10, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98199999"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combatendo _overfitting_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada a complexidade de modelos profundos, é comum que os modelos se super-especializem no treino. Para verificar se isso está acontecendo, é recomendado durante o treino avaliar o modelo tanto na coleção de treino quanto em uma coleção de validação (separada da coleção de treino e diferente da de teste). Se as taxas de erro diferem, com o erro do treino menor que na validação, está acontecendo _overfitting_.\n",
    "\n",
    "Para lidar com esse problema, as seguintes técnicas podem ser aplicadas:\n",
    "\n",
    "* _early stopping_: parar o processo de treino quando nenhum novo modelo supera o melhor observado após N épocas;\n",
    "\n",
    "* uso de regularizadores (como $L_1$ e $L_2$): reguladorizadores penalizam certos conjuntos de pesos, obrigando o modelo a encontrar soluções \"melhores\". Por exemplo, regularizadores $L_1$ tendem a gerar um conjunto esparso de pesos enquanto $L_2$ penaliza outliers.\n",
    "\n",
    "* _dropout_: neurônios são retirados aleatoriamente do modelo (também atributos de entrada), de forma que nenhum neurônio pode se co-adaptar a outro em particular. De certa forma, isso simula a natureza estocástica de redes neurais biológicas.\n",
    "\n",
    "* introdução de ruído/mais casos no treino: aumento dos casos de treino com casos ruidosos (por exemplo, uma versão embaçada de uma imagem), ou variantes prováveis (ex: a imagem de ponta a cabeça, um fragmento de imagem, a imagem rotacionada, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularizador $L_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função de perda $\\ell(\\hat{y}({\\bf w}), {\\bf y})$ pode ser minimizada com _qualquer_ conjunto de pesos ${\\bf w}$. Contudo, certos conjuntos de pesos podem envolver pesos muito mais importantes que outros (_outliers_), configurações onde todos os pesos são relevantes (não esparsos), etc. Uma função regularizadora $R$ pode ser usada para penalizar esses conjuntos, melhorando a possibiliade de generalização do modelo seja por simplificá-lo ou remover _outliers_, por exemplo. \n",
    "\n",
    "Incorporando $R$, temos a nova função de perda:\n",
    "\n",
    "$$L(\\hat{y}({\\bf w}), {\\bf y}) = \\ell(\\hat{y}({\\bf w}), {\\bf y}) + \\lambda R({\\bf w})$$\n",
    "\n",
    "onde $\\lambda$ é o fator que indica a importância do regularizador para o custo final. Dois regularizadores comuns são:\n",
    "\n",
    "* $L_1 = R({\\bf w}) = \\sum_i{|{\\bf w}_i|}$ -- prefere conjuntos de pesos esparsos. Este conjunto tem certa motivação biológica, uma vez que redes neurais biológicas são esparsas. Do ponto de vista estatístico, pesos esparsos levam a um modelo mais simples e, portanto, mais fácil de generalizar.\n",
    "\n",
    "* $L_2 = R({\\bf w}) = \\sum_i{{\\bf w}_i^2}$ -- prefere conjuntos de pesos densos, sem _outliers_. A ideia desta penalização é que confiar em conjuntos de pesos esparsos, baseado em amostras (provavelmente pequenas, como as vindas de pequenos lotes de instâncias, normalmente usadas em algoritmos da família do gradiente descendente), não é seguro, uma vez que os pesos relevantes podem ser _outliers_.\n",
    "\n",
    "A seguir, mudamos nossa implementação para incorporar um regularizador $L_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossFunction(object):\n",
    "    def __init__(self, name = 'sigmoid', reg_lambda = 0.001):\n",
    "        self.name = name\n",
    "        self.reg_lambda = reg_lambda # regularizer importance factor\n",
    "\n",
    "    # get needs to access layers' weights\n",
    "    def get(self, yreal, ypred, layers = None):\n",
    "        reg_losses = 0. # reg loss component\n",
    "        if self.name == 'sigmoid' or self.name == 'sigmoid_l1':\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels = yreal, logits = ypred) \n",
    "            # L1 regularization\n",
    "            if layers != None and self.reg_lambda > 0:\n",
    "                for layer in layers:\n",
    "                    reg_losses += tf.reduce_sum(tf.abs(layer.W))\n",
    "                reg_losses *= self.reg_lambda\n",
    "        return tf.reduce_mean(loss, name = 'lossf') \\\n",
    "                + reg_losses # reg loss component\n",
    "    \n",
    "def compile(self, loss = 'sigmoid', optimizer = 'sgd'):\n",
    "    \"\"\"Cria grafo da rede neural\"\"\"\n",
    "    self.X = tf.placeholder(tf.float32, \n",
    "                       shape = (None, self.input_dim), \n",
    "                       name = 'X')\n",
    "    self.y = tf.placeholder(tf.int64, shape = (None), name = 'y')\n",
    "\n",
    "    # cria layers\n",
    "    with tf.name_scope('layers'):\n",
    "        layer_in = self.X\n",
    "        for layer in self.layers:\n",
    "            layer_out = layer.output(layer_in)\n",
    "            layer_in = layer_out\n",
    "\n",
    "    # loss function\n",
    "    with tf.name_scope('loss'):\n",
    "        self.lossf = LossFunction(loss).get(self.y, layer_out, self.layers)\n",
    "\n",
    "    # optimizer\n",
    "    with tf.name_scope('train'):\n",
    "        self.train_op = Optimizer(optimizer).get(self.lossf)\n",
    "\n",
    "    # evalution metrics\n",
    "    with tf.name_scope('eval'):\n",
    "        correct = tf.nn.in_top_k(layer_out, self.y, 1)\n",
    "        self.acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    self.init_op = tf.global_variables_initializer()\n",
    "    self.saver = tf.train.Saver()\n",
    "    \n",
    "FeedforwardNeuralNet.compile = compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = FeedforwardNeuralNet(input_dim = 28*28)\n",
    "\n",
    "model.add(units=300, activation = Activation('relu'), name = 'h1')\n",
    "model.add(units=100, activation = Activation('relu'), name = 'h2')\n",
    "model.add(units=10, name = 'out')\n",
    "\n",
    "model.compile(optimizer = 'exp_decay_momentum', loss = 'sigmoid_l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 loss: 1.58532247 acc: 0.94\n",
      " 1 loss: 0.75589910 acc: 0.96\n",
      " 2 loss: 0.66857005 acc: 0.90\n",
      " 3 loss: 0.61096120 acc: 0.96\n",
      " 4 loss: 0.56374224 acc: 0.94\n",
      " 5 loss: 0.53321255 acc: 0.90\n",
      " 6 loss: 0.49640640 acc: 0.98\n",
      " 7 loss: 0.47345848 acc: 0.96\n",
      " 8 loss: 0.45177614 acc: 0.98\n",
      " 9 loss: 0.43086177 acc: 0.96\n"
     ]
    }
   ],
   "source": [
    "model.fit(mnist.train, n_epochs = 10, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95359999"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em redes neurais biológicas, neurônios têm natureza estocástica. Ou seja, em condições apropriadas para disparar um sinal, eles disparam apenas 50% das vezes, devido a uma série de condições bio-químicas. \n",
    "\n",
    "Uma forma de implementar isso numa rede neural artificial é meramente por eliminar, aleatoriamente, metade dos neurônios da rede. Esta eliminação pode atingir mesmo a entrada. Este processo de eliminação de neurônios durante o treino é chamado de dropout.\n",
    "\n",
    "Há algumas motivações para usar dropout:\n",
    "\n",
    "* Neurônios não podem se co-adaptar, tornando-se mais robustos;\n",
    "* Dropout pode ser visto como um tipo de ensemble, onde $2^H$ ($H$ é o número de neurônios no modelo) redes neurais são treinadas e, juntas, colaboram para a solução final; \n",
    "* Quando o dropout atinge a entrada da rede, ela também opera como um processo de aleatorização similar ao observado em random forests;\n",
    "* Do ponto de vista prático, o uso de dropout implica em aumentos de 1 a 2% no desempenho do sistema (cerca de 40% de redução em taxas de erro). Isso, contudo, _ao custo de aprendizado mais lento e complexo_.\n",
    "\n",
    "Abaixo, temos nosso código modificado para suportar dropout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compile(self, loss = 'sigmoid', optimizer = 'sgd', \n",
    "            dropout_rate = 0.0): # dropout rate = 0 -> sem dropout\n",
    "    \"\"\"Cria grafo da rede neural\"\"\"\n",
    "    self.X = tf.placeholder(tf.float32, \n",
    "                       shape = (None, self.input_dim), \n",
    "                       name = 'X')\n",
    "    self.y = tf.placeholder(tf.int64, shape = (None), name = 'y')\n",
    "    # in_training indicates if graph is evaluated during training\n",
    "    # During training, using dropout, units are dropped (inclunding input)\n",
    "    # At the end of the process, weights are divided by 1/dropout_rate to \n",
    "    # compensate the average loss of signal resulting from dropping units\n",
    "    self.in_training = tf.placeholder_with_default(False, shape = (), \n",
    "                                                   name = 'in_training')\n",
    "\n",
    "    # cria layers\n",
    "    with tf.name_scope('layers'):\n",
    "        # dropout in input layer\n",
    "        layer_in = self.X if dropout_rate == 0.0 \\\n",
    "            else tf.layers.dropout(self.X, dropout_rate, self.in_training)\n",
    "        for layer in self.layers:\n",
    "            # dropout in other layers\n",
    "            layer_out = layer.output(layer_in) if dropout_rate == 0.0 \\\n",
    "                else tf.layers.dropout(layer.output(layer_in), \n",
    "                                       dropout_rate, self.in_training)\n",
    "            layer_in = layer_out\n",
    "\n",
    "    # loss function\n",
    "    with tf.name_scope('loss'):\n",
    "        # layers are sent to loss function so that the regularizer can be applied\n",
    "        self.lossf = LossFunction(loss).get(self.y, layer_out, self.layers)\n",
    "\n",
    "    # optimizer\n",
    "    with tf.name_scope('train'):\n",
    "        self.train_op = Optimizer(optimizer).get(self.lossf)\n",
    "\n",
    "    # evalution metrics\n",
    "    with tf.name_scope('eval'):\n",
    "        correct = tf.nn.in_top_k(layer_out, self.y, 1)\n",
    "        self.acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    self.init_op = tf.global_variables_initializer()\n",
    "    self.saver = tf.train.Saver()\n",
    "    \n",
    "def fit(self, train_data, n_epochs, batch_size):\n",
    "    \"\"\"Executa treino da rede neural\"\"\"\n",
    "    num_batches = train_data.num_examples // batch_size\n",
    "    with tf.Session() as s:\n",
    "        s.run(self.init_op)\n",
    "        for e in range(n_epochs):\n",
    "            tloss = 0.\n",
    "            for i in range(num_batches):\n",
    "                X_b, y_b = train_data.next_batch(batch_size)\n",
    "                _, loss_e = s.run([self.train_op, self.lossf], \n",
    "                                  feed_dict = {self.X: X_b, self.y: y_b,\n",
    "                                              self.in_training: True})\n",
    "                tloss += loss_e\n",
    "            acc_train = s.run(self.acc, \n",
    "                              feed_dict = {self.X: X_b, self.y: y_b})\n",
    "            print '%2d loss: %.8f acc: %.2f' % (e, tloss/num_batches, acc_train)\n",
    "        self.saver.save(s, '/tmp/model.ckpt')\n",
    "    \n",
    "FeedforwardNeuralNet.compile = compile\n",
    "FeedforwardNeuralNet.fit = fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = FeedforwardNeuralNet(input_dim = 28*28)\n",
    "\n",
    "model.add(units=300, activation = Activation('relu'), name = 'h1')\n",
    "model.add(units=100, activation = Activation('relu'), name = 'h2')\n",
    "model.add(units=10, name = 'out')\n",
    "\n",
    "model.compile(optimizer = 'exp_decay_momentum', dropout_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 loss: 1.55401182 acc: 1.00\n",
      " 1 loss: 0.74529432 acc: 0.98\n",
      " 2 loss: 0.66609687 acc: 0.92\n",
      " 3 loss: 0.61211880 acc: 0.92\n",
      " 4 loss: 0.56979004 acc: 0.94\n",
      " 5 loss: 0.53536397 acc: 0.94\n",
      " 6 loss: 0.50652765 acc: 0.88\n",
      " 7 loss: 0.47880428 acc: 0.98\n",
      " 8 loss: 0.46112720 acc: 0.98\n",
      " 9 loss: 0.44155752 acc: 0.94\n"
     ]
    }
   ],
   "source": [
    "model.fit(mnist.train, n_epochs = 10, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.94620001"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A taxa de dropout é normalmente mantida em 0.5. Você pode aumentar esse valor se quer tentar reduzir mais o overfitting. Ou diminui-lo, se quer evitar underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dicas gerais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes da SNNs, as escolhas seriam:\n",
    "\n",
    "* _Inicialização_: Xavier ou He\n",
    "* _Função de ativação_: ELU\n",
    "* _Normalização_: Batch Normalization\n",
    "* _Regularização_: Dropout\n",
    "* _Otimizador_: Adam\n",
    "* _Escala de evolução da taxa de aprendizado_: nenhum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com SNNs, as escolhas são:\n",
    "\n",
    "* _Inicialização_: pequeno valor próximo ao mínimo da normalização mantida pela SNN\n",
    "* _Função de ativação_: SELU\n",
    "* _Normalização_: não necessária\n",
    "* _Regularização_: Dropout para SNNs (ver paper sobre SNNs)\n",
    "* _Otimizador_: Adam\n",
    "* _Escala de evolução da taxa de aprendizado_: nenhum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas à frente, vamos melhorar mais este resultado usando uma rede convolutiva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Deep_learning    \n",
    "http://sebastianruder.com/optimizing-gradient-descent/index.html#batchgradientdescent  \n",
    "http://yann.lecun.com/exdb/mnist/  \n",
    "https://www.quora.com/Artificial-Neural-Networks-What-is-the-difference-between-activation-functions  \n",
    "https://www.tensorflow.org/versions/r0.9/tutorials/mnist/pros/index.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
