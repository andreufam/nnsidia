{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FNNs com Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A biblioteca Keras fornece uma API padrão de alto nível para acessar o tensorflow. A seguir vamos ver como implementar algumas das técnicas anteriores usando Keras. Note que a API implementada na aula anterior foi inspirada na do Keras, modo sequencial, que é descrita a seguir, usando exemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marco/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, \\\n",
    "    Convolution2D, Flatten, MaxPooling2D, Reshape, InputLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como antes, vamos usar a coleção MNIST. Desta vez, vamos ter tantos neurônios na camada de saída quanto as classes em MNIST (10). Assim, vamos usar codificação _one-hot_ na classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('data/MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente, vamos criar uma arquitetura larga. Ela tem 250 neurônios em uma única camada escondida. Ela será treinada em 5 épocas com mini-batches de tamanho 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define vars\n",
    "input_num_units = 28*28\n",
    "hidden_num_units = 250\n",
    "output_num_units = 10\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=hidden_num_units, \n",
    "       input_dim=input_num_units, activation='relu'))\n",
    "model.add(Dense(units=output_num_units, \n",
    "       input_dim=hidden_num_units, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos treiná-la usando como função de perda a entropia cruzada, com algoritmo de otimização Adam (gradiente descendente com momento) e avaliá-la em termos de acurácia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "55000/55000 [==============================] - 1s - loss: 0.0464 - acc: 0.9860 - val_loss: 0.0721 - val_acc: 0.9770\n",
      "Epoch 2/5\n",
      "55000/55000 [==============================] - 1s - loss: 0.0338 - acc: 0.9902 - val_loss: 0.0686 - val_acc: 0.9774\n",
      "Epoch 3/5\n",
      "55000/55000 [==============================] - 1s - loss: 0.0265 - acc: 0.9928 - val_loss: 0.0650 - val_acc: 0.9816\n",
      "Epoch 4/5\n",
      "55000/55000 [==============================] - 1s - loss: 0.0200 - acc: 0.9948 - val_loss: 0.0635 - val_acc: 0.9780\n",
      "Epoch 5/5\n",
      "55000/55000 [==============================] - 1s - loss: 0.0160 - acc: 0.9961 - val_loss: 0.0664 - val_acc: 0.9796\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "trained_model_500 = model.fit(mnist.train.images, mnist.train.labels, \n",
    "                              epochs=epochs, \n",
    "                              batch_size=batch_size, \n",
    "                              validation_data=(mnist.validation.images, \n",
    "                                               mnist.validation.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma dúvida que surge é se uma rede profunda, com o mesmo número de nerônios ocultos, porém através de 5 camadas, é capaz de algo melhor. É o que vamos ver na arquitetura seguinte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define vars\n",
    "input_num_units = 784\n",
    "hidden1_num_units = 50\n",
    "hidden2_num_units = 50\n",
    "hidden3_num_units = 50\n",
    "hidden4_num_units = 50\n",
    "hidden5_num_units = 50\n",
    "output_num_units = 10\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "model = Sequential([\n",
    " Dense(units=hidden1_num_units, \n",
    "       input_dim=input_num_units, activation='relu'),\n",
    " Dense(units=hidden2_num_units, \n",
    "       input_dim=hidden1_num_units, activation='relu'),\n",
    " Dense(units=hidden3_num_units, \n",
    "       input_dim=hidden2_num_units, activation='relu'),\n",
    " Dense(units=hidden4_num_units, \n",
    "       input_dim=hidden3_num_units, activation='relu'),\n",
    " Dense(units=hidden5_num_units, \n",
    "       input_dim=hidden4_num_units, activation='relu'),\n",
    " Dense(units=output_num_units, \n",
    "       input_dim=hidden5_num_units, activation='softmax'),\n",
    " ])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "55000/55000 [==============================] - 2s - loss: 0.4306 - acc: 0.8651 - val_loss: 0.1721 - val_acc: 0.9498\n",
      "Epoch 2/5\n",
      "55000/55000 [==============================] - 2s - loss: 0.1613 - acc: 0.9513 - val_loss: 0.1354 - val_acc: 0.9612\n",
      "Epoch 3/5\n",
      "55000/55000 [==============================] - 2s - loss: 0.1204 - acc: 0.9633 - val_loss: 0.1265 - val_acc: 0.9636\n",
      "Epoch 4/5\n",
      "55000/55000 [==============================] - 2s - loss: 0.0956 - acc: 0.9709 - val_loss: 0.0989 - val_acc: 0.9704\n",
      "Epoch 5/5\n",
      "55000/55000 [==============================] - 2s - loss: 0.0801 - acc: 0.9749 - val_loss: 0.0974 - val_acc: 0.9714\n"
     ]
    }
   ],
   "source": [
    "trained_model_5d = model.fit(mnist.train.images, mnist.train.labels, \n",
    "                              epochs=epochs, \n",
    "                              batch_size=batch_size, \n",
    "                              validation_data=(mnist.validation.images, \n",
    "                                               mnist.validation.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma forma de melhorar o resultado talvez seja usar Dropout. Vamos ver como se comporta, usando uma taxa de droput de 20%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define vars\n",
    "input_num_units = 784\n",
    "hidden1_num_units = 50\n",
    "hidden2_num_units = 50\n",
    "hidden3_num_units = 50\n",
    "hidden4_num_units = 50\n",
    "hidden5_num_units = 50\n",
    "output_num_units = 10\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "dropout_ratio = 0.2\n",
    "\n",
    "model = Sequential([\n",
    " Dense(units=hidden1_num_units, \n",
    "       input_dim=input_num_units, activation='relu'),\n",
    " Dropout(dropout_ratio),\n",
    " Dense(units=hidden2_num_units, \n",
    "       input_dim=hidden1_num_units, activation='relu'),\n",
    " Dropout(dropout_ratio),\n",
    " Dense(units=hidden3_num_units, \n",
    "       input_dim=hidden2_num_units, activation='relu'),\n",
    " Dropout(dropout_ratio),\n",
    " Dense(units=hidden4_num_units, \n",
    "       input_dim=hidden3_num_units, activation='relu'),\n",
    " Dropout(dropout_ratio),\n",
    " Dense(units=hidden5_num_units, \n",
    "       input_dim=hidden4_num_units, activation='relu'),\n",
    " Dropout(dropout_ratio),\n",
    "\n",
    "Dense(units=output_num_units, \n",
    "      input_dim=hidden5_num_units, activation='softmax'),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "55000/55000 [==============================] - 3s - loss: 0.9269 - acc: 0.6910 - val_loss: 0.2732 - val_acc: 0.9236\n",
      "Epoch 2/5\n",
      "55000/55000 [==============================] - 2s - loss: 0.4047 - acc: 0.8894 - val_loss: 0.2096 - val_acc: 0.9452\n",
      "Epoch 3/5\n",
      "55000/55000 [==============================] - 2s - loss: 0.3231 - acc: 0.9154 - val_loss: 0.1707 - val_acc: 0.9530\n",
      "Epoch 4/5\n",
      "55000/55000 [==============================] - 2s - loss: 0.2811 - acc: 0.9268 - val_loss: 0.1548 - val_acc: 0.9598\n",
      "Epoch 5/5\n",
      "55000/55000 [==============================] - 2s - loss: 0.2561 - acc: 0.9331 - val_loss: 0.1465 - val_acc: 0.9632\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "trained_model_5d_drop = model.fit(mnist.train.images, mnist.train.labels, \n",
    "                              epochs=epochs, \n",
    "                              batch_size=batch_size, \n",
    "                              validation_data=(mnist.validation.images, \n",
    "                                               mnist.validation.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O desempenho piorou. Uma razão possível talvez seja o fato de que não estamos treinando o suficiente. Assim, vamos aumentar o número de épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define vars\n",
    "input_num_units = 784\n",
    "hidden1_num_units = 50\n",
    "hidden2_num_units = 50\n",
    "hidden3_num_units = 50\n",
    "hidden4_num_units = 50\n",
    "hidden5_num_units = 50\n",
    "output_num_units = 10\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "dropout_ratio = 0.2\n",
    "\n",
    "model = Sequential([\n",
    " Dense(units=hidden1_num_units, \n",
    "       input_dim=input_num_units, activation='relu'),\n",
    " Dropout(dropout_ratio),\n",
    " Dense(units=hidden2_num_units, \n",
    "       input_dim=hidden1_num_units, activation='relu'),\n",
    " Dropout(dropout_ratio),\n",
    " Dense(units=hidden3_num_units, \n",
    "       input_dim=hidden2_num_units, activation='relu'),\n",
    " Dropout(dropout_ratio),\n",
    " Dense(units=hidden4_num_units, \n",
    "       input_dim=hidden3_num_units, activation='relu'),\n",
    " Dropout(dropout_ratio),\n",
    " Dense(units=hidden5_num_units, \n",
    "       input_dim=hidden4_num_units, activation='relu'),\n",
    " Dropout(dropout_ratio),\n",
    "\n",
    "Dense(units=output_num_units, \n",
    "      input_dim=hidden5_num_units, activation='softmax'),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n",
      "55000/55000 [==============================] - 3s - loss: 0.9274 - acc: 0.6877 - val_loss: 0.2709 - val_acc: 0.9262\n",
      "Epoch 2/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.3926 - acc: 0.8932 - val_loss: 0.2016 - val_acc: 0.9404\n",
      "Epoch 3/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.3175 - acc: 0.9164 - val_loss: 0.1765 - val_acc: 0.9520\n",
      "Epoch 4/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.2756 - acc: 0.9272 - val_loss: 0.1560 - val_acc: 0.9574\n",
      "Epoch 5/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.2502 - acc: 0.9345 - val_loss: 0.1518 - val_acc: 0.9576\n",
      "Epoch 6/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.2381 - acc: 0.9371 - val_loss: 0.1516 - val_acc: 0.9614\n",
      "Epoch 7/50\n",
      "55000/55000 [==============================] - 1s - loss: 0.2193 - acc: 0.9421 - val_loss: 0.1491 - val_acc: 0.9626\n",
      "Epoch 8/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.2081 - acc: 0.9454 - val_loss: 0.1320 - val_acc: 0.9624\n",
      "Epoch 9/50\n",
      "55000/55000 [==============================] - 1s - loss: 0.1987 - acc: 0.9477 - val_loss: 0.1295 - val_acc: 0.9648\n",
      "Epoch 10/50\n",
      "55000/55000 [==============================] - 1s - loss: 0.1947 - acc: 0.9492 - val_loss: 0.1343 - val_acc: 0.9642\n",
      "Epoch 11/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1871 - acc: 0.9501 - val_loss: 0.1252 - val_acc: 0.9660\n",
      "Epoch 12/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1785 - acc: 0.9521 - val_loss: 0.1249 - val_acc: 0.9660\n",
      "Epoch 13/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1754 - acc: 0.9536 - val_loss: 0.1343 - val_acc: 0.9628\n",
      "Epoch 14/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1693 - acc: 0.9541 - val_loss: 0.1233 - val_acc: 0.9664\n",
      "Epoch 15/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1661 - acc: 0.9550 - val_loss: 0.1230 - val_acc: 0.9672\n",
      "Epoch 16/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1648 - acc: 0.9554 - val_loss: 0.1212 - val_acc: 0.9696\n",
      "Epoch 17/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1576 - acc: 0.9567 - val_loss: 0.1185 - val_acc: 0.9690\n",
      "Epoch 18/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1561 - acc: 0.9579 - val_loss: 0.1270 - val_acc: 0.9670\n",
      "Epoch 19/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1525 - acc: 0.9595 - val_loss: 0.1252 - val_acc: 0.9678\n",
      "Epoch 20/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1481 - acc: 0.9600 - val_loss: 0.1218 - val_acc: 0.9694\n",
      "Epoch 21/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1415 - acc: 0.9608 - val_loss: 0.1251 - val_acc: 0.9666\n",
      "Epoch 22/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1437 - acc: 0.9614 - val_loss: 0.1266 - val_acc: 0.9682\n",
      "Epoch 23/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1385 - acc: 0.9622 - val_loss: 0.1181 - val_acc: 0.9686\n",
      "Epoch 24/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1365 - acc: 0.9628 - val_loss: 0.1245 - val_acc: 0.9712\n",
      "Epoch 25/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1336 - acc: 0.9631 - val_loss: 0.1245 - val_acc: 0.9676\n",
      "Epoch 26/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1348 - acc: 0.9635 - val_loss: 0.1194 - val_acc: 0.9698\n",
      "Epoch 27/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1323 - acc: 0.9639 - val_loss: 0.1229 - val_acc: 0.9686\n",
      "Epoch 28/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1307 - acc: 0.9636 - val_loss: 0.1300 - val_acc: 0.9676\n",
      "Epoch 29/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1274 - acc: 0.9657 - val_loss: 0.1181 - val_acc: 0.9716\n",
      "Epoch 30/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1268 - acc: 0.9654 - val_loss: 0.1205 - val_acc: 0.9708\n",
      "Epoch 31/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1247 - acc: 0.9660 - val_loss: 0.1150 - val_acc: 0.9714\n",
      "Epoch 32/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1227 - acc: 0.9658 - val_loss: 0.1220 - val_acc: 0.9702\n",
      "Epoch 33/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1241 - acc: 0.9657 - val_loss: 0.1288 - val_acc: 0.9674\n",
      "Epoch 34/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1219 - acc: 0.9667 - val_loss: 0.1178 - val_acc: 0.9692\n",
      "Epoch 35/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1230 - acc: 0.9661 - val_loss: 0.1324 - val_acc: 0.9690\n",
      "Epoch 36/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1230 - acc: 0.9658 - val_loss: 0.1221 - val_acc: 0.9694\n",
      "Epoch 37/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1183 - acc: 0.9672 - val_loss: 0.1253 - val_acc: 0.9696\n",
      "Epoch 38/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1149 - acc: 0.9680 - val_loss: 0.1278 - val_acc: 0.9682\n",
      "Epoch 39/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1160 - acc: 0.9672 - val_loss: 0.1158 - val_acc: 0.9718\n",
      "Epoch 40/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1143 - acc: 0.9684 - val_loss: 0.1180 - val_acc: 0.9702\n",
      "Epoch 41/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1139 - acc: 0.9688 - val_loss: 0.1141 - val_acc: 0.9722\n",
      "Epoch 42/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1130 - acc: 0.9687 - val_loss: 0.1145 - val_acc: 0.9700\n",
      "Epoch 43/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1129 - acc: 0.9687 - val_loss: 0.1190 - val_acc: 0.9720\n",
      "Epoch 44/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1119 - acc: 0.9684 - val_loss: 0.1131 - val_acc: 0.9712\n",
      "Epoch 45/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1120 - acc: 0.9685 - val_loss: 0.1090 - val_acc: 0.9712\n",
      "Epoch 46/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1121 - acc: 0.9691 - val_loss: 0.1115 - val_acc: 0.9724\n",
      "Epoch 47/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1130 - acc: 0.9683 - val_loss: 0.1178 - val_acc: 0.9696\n",
      "Epoch 48/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1105 - acc: 0.9686 - val_loss: 0.1271 - val_acc: 0.9700\n",
      "Epoch 49/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1076 - acc: 0.9700 - val_loss: 0.1146 - val_acc: 0.9728\n",
      "Epoch 50/50\n",
      "55000/55000 [==============================] - 2s - loss: 0.1081 - acc: 0.9695 - val_loss: 0.1166 - val_acc: 0.9718\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "trained_model_5d_drop_mais_epocas = model.fit(mnist.train.images, \n",
    "                                              mnist.train.labels, \n",
    "                              epochs=epochs, \n",
    "                              batch_size=batch_size, \n",
    "                              validation_data=(mnist.validation.images, \n",
    "                                               mnist.validation.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E nada muito extraordinário. O próximo passo agora é testar uma rede larga e profunda (e lenta :(). Com ela, temos a chance de realmente nos aproximarmos do desempenho dos melhores métodos, baseados em convoluções. Para deixar as coisas menos complexas, vou deixar 500 neurônios na 1a camada e apenas 250 nas demais. Vou testar com 25 épocas... ah, pode esperar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define vars\n",
    "input_num_units = 784\n",
    "hidden1_num_units = 500\n",
    "hidden2_num_units = 250\n",
    "hidden3_num_units = 250\n",
    "hidden4_num_units = 250\n",
    "hidden5_num_units = 250\n",
    "output_num_units = 10\n",
    "\n",
    "epochs = 25\n",
    "batch_size = 128\n",
    "\n",
    "dropout_ratio = 0.2\n",
    "\n",
    "model = Sequential([\n",
    " Dense(units=hidden1_num_units, \n",
    "       input_dim=input_num_units, activation='relu'),\n",
    " Dropout(dropout_ratio),\n",
    " Dense(units=hidden2_num_units, \n",
    "       input_dim=hidden1_num_units, activation='relu'),\n",
    " Dropout(dropout_ratio),\n",
    " Dense(units=hidden3_num_units, \n",
    "       input_dim=hidden2_num_units, activation='relu'),\n",
    " Dropout(dropout_ratio),\n",
    " Dense(units=hidden4_num_units, \n",
    "       input_dim=hidden3_num_units, activation='relu'),\n",
    " Dropout(dropout_ratio),\n",
    " Dense(units=hidden5_num_units, \n",
    "       input_dim=hidden4_num_units, activation='relu'),\n",
    " Dropout(dropout_ratio),\n",
    "\n",
    "Dense(units=output_num_units, \n",
    "      input_dim=hidden5_num_units, activation='softmax'),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "55000/55000 [==============================] - 3s - loss: 0.3620 - acc: 0.8880 - val_loss: 0.1195 - val_acc: 0.9660\n",
      "Epoch 2/25\n",
      "55000/55000 [==============================] - 2s - loss: 0.1434 - acc: 0.9588 - val_loss: 0.0996 - val_acc: 0.9702\n",
      "Epoch 3/25\n",
      "55000/55000 [==============================] - 2s - loss: 0.1100 - acc: 0.9683 - val_loss: 0.0865 - val_acc: 0.9748\n",
      "Epoch 4/25\n",
      "55000/55000 [==============================] - 2s - loss: 0.0867 - acc: 0.9752 - val_loss: 0.0840 - val_acc: 0.9782\n",
      "Epoch 5/25\n",
      "55000/55000 [==============================] - 2s - loss: 0.0765 - acc: 0.9776 - val_loss: 0.0767 - val_acc: 0.9802\n",
      "Epoch 6/25\n",
      "55000/55000 [==============================] - 2s - loss: 0.0620 - acc: 0.9819 - val_loss: 0.0737 - val_acc: 0.9800\n",
      "Epoch 7/25\n",
      "55000/55000 [==============================] - 2s - loss: 0.0600 - acc: 0.9823 - val_loss: 0.0666 - val_acc: 0.9826\n",
      "Epoch 8/25\n",
      "55000/55000 [==============================] - 2s - loss: 0.0490 - acc: 0.9853 - val_loss: 0.0780 - val_acc: 0.9832\n",
      "Epoch 9/25\n",
      "55000/55000 [==============================] - 2s - loss: 0.0476 - acc: 0.9863 - val_loss: 0.0785 - val_acc: 0.9814\n",
      "Epoch 10/25\n",
      "55000/55000 [==============================] - 2s - loss: 0.0477 - acc: 0.9861 - val_loss: 0.0707 - val_acc: 0.9832\n",
      "Epoch 11/25\n",
      "55000/55000 [==============================] - 2s - loss: 0.0419 - acc: 0.9876 - val_loss: 0.0796 - val_acc: 0.9826\n",
      "Epoch 12/25\n",
      "55000/55000 [==============================] - 2s - loss: 0.0413 - acc: 0.9879 - val_loss: 0.0714 - val_acc: 0.9840\n",
      "Epoch 13/25\n",
      "55000/55000 [==============================] - 2s - loss: 0.0352 - acc: 0.9896 - val_loss: 0.0829 - val_acc: 0.9812\n",
      "Epoch 14/25\n",
      "55000/55000 [==============================] - 2s - loss: 0.0354 - acc: 0.9900 - val_loss: 0.0687 - val_acc: 0.9826\n",
      "Epoch 15/25\n",
      "55000/55000 [==============================] - 2s - loss: 0.0311 - acc: 0.9909 - val_loss: 0.0919 - val_acc: 0.9784\n",
      "Epoch 16/25\n",
      "55000/55000 [==============================] - 2s - loss: 0.0326 - acc: 0.9910 - val_loss: 0.0727 - val_acc: 0.9830\n",
      "Epoch 17/25\n",
      "55000/55000 [==============================] - 2s - loss: 0.0306 - acc: 0.9911 - val_loss: 0.0773 - val_acc: 0.9828\n",
      "Epoch 18/25\n",
      "55000/55000 [==============================] - 2s - loss: 0.0284 - acc: 0.9918 - val_loss: 0.0700 - val_acc: 0.9826\n",
      "Epoch 19/25\n",
      "55000/55000 [==============================] - 2s - loss: 0.0266 - acc: 0.9927 - val_loss: 0.0688 - val_acc: 0.9838\n",
      "Epoch 20/25\n",
      "55000/55000 [==============================] - 2s - loss: 0.0270 - acc: 0.9920 - val_loss: 0.0717 - val_acc: 0.9828\n",
      "Epoch 21/25\n",
      "55000/55000 [==============================] - 3s - loss: 0.0266 - acc: 0.9925 - val_loss: 0.0810 - val_acc: 0.9818\n",
      "Epoch 22/25\n",
      "55000/55000 [==============================] - 2s - loss: 0.0263 - acc: 0.9929 - val_loss: 0.0803 - val_acc: 0.9816\n",
      "Epoch 23/25\n",
      "55000/55000 [==============================] - 3s - loss: 0.0236 - acc: 0.9931 - val_loss: 0.0775 - val_acc: 0.9848\n",
      "Epoch 24/25\n",
      "55000/55000 [==============================] - 2s - loss: 0.0250 - acc: 0.9933 - val_loss: 0.0795 - val_acc: 0.9830\n",
      "Epoch 25/25\n",
      "55000/55000 [==============================] - 2s - loss: 0.0206 - acc: 0.9943 - val_loss: 0.0812 - val_acc: 0.9832\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "trained_model_5d_larga_profunda = model.fit(mnist.train.images, \n",
    "                                              mnist.train.labels, \n",
    "                              epochs=epochs, \n",
    "                              batch_size=batch_size, \n",
    "                              validation_data=(mnist.validation.images, \n",
    "                                               mnist.validation.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo já conseguiu um resultado bem superior ao anterior. Contudo, fica claro agora que já há algum overfitting, devido à complexidade do modelo. Vamos ver qual o efeito agora de aplicarmos _batch normalization_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define vars\n",
    "input_num_units = 784\n",
    "hidden1_num_units = 500\n",
    "hidden2_num_units = 250\n",
    "hidden3_num_units = 250\n",
    "hidden4_num_units = 250\n",
    "hidden5_num_units = 250\n",
    "output_num_units = 10\n",
    "\n",
    "epochs = 25\n",
    "batch_size = 128\n",
    "\n",
    "dropout_ratio = 0.2\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "model = Sequential([\n",
    " Dense(units=hidden1_num_units, input_dim=input_num_units),\n",
    " BatchNormalization(), Activation('relu'),\n",
    " Dropout(dropout_ratio),\n",
    "    \n",
    " Dense(units=hidden2_num_units, input_dim=hidden1_num_units),\n",
    " BatchNormalization(), Activation('relu'),\n",
    " Dropout(dropout_ratio),\n",
    "    \n",
    " Dense(units=hidden3_num_units, input_dim=hidden2_num_units),\n",
    " BatchNormalization(), Activation('relu'),\n",
    " Dropout(dropout_ratio),\n",
    "    \n",
    " Dense(units=hidden4_num_units, input_dim=hidden3_num_units),\n",
    " BatchNormalization(), Activation('relu'),\n",
    " Dropout(dropout_ratio),\n",
    "    \n",
    " Dense(units=hidden5_num_units, input_dim=hidden4_num_units),\n",
    " BatchNormalization(), Activation('relu'),\n",
    " Dropout(dropout_ratio),\n",
    "    \n",
    " Dense(units=output_num_units, input_dim=hidden5_num_units),\n",
    " BatchNormalization(), Activation('softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "55000/55000 [==============================] - 7s - loss: 0.5520 - acc: 0.8895 - val_loss: 0.2106 - val_acc: 0.9598\n",
      "Epoch 2/25\n",
      "55000/55000 [==============================] - 5s - loss: 0.2484 - acc: 0.9532 - val_loss: 0.1225 - val_acc: 0.9734\n",
      "Epoch 3/25\n",
      "55000/55000 [==============================] - 5s - loss: 0.1769 - acc: 0.9640 - val_loss: 0.0976 - val_acc: 0.9768\n",
      "Epoch 4/25\n",
      "55000/55000 [==============================] - 5s - loss: 0.1373 - acc: 0.9696 - val_loss: 0.0820 - val_acc: 0.9794\n",
      "Epoch 5/25\n",
      "55000/55000 [==============================] - 4s - loss: 0.1124 - acc: 0.9740 - val_loss: 0.0765 - val_acc: 0.9814\n",
      "Epoch 6/25\n",
      "55000/55000 [==============================] - 5s - loss: 0.0980 - acc: 0.9766 - val_loss: 0.0691 - val_acc: 0.9808\n",
      "Epoch 7/25\n",
      "55000/55000 [==============================] - 5s - loss: 0.0855 - acc: 0.9789 - val_loss: 0.0675 - val_acc: 0.9818\n",
      "Epoch 8/25\n",
      "55000/55000 [==============================] - 5s - loss: 0.0747 - acc: 0.9812 - val_loss: 0.0653 - val_acc: 0.9832\n",
      "Epoch 9/25\n",
      "55000/55000 [==============================] - 5s - loss: 0.0693 - acc: 0.9826 - val_loss: 0.0629 - val_acc: 0.9848\n",
      "Epoch 10/25\n",
      "55000/55000 [==============================] - 5s - loss: 0.0609 - acc: 0.9839 - val_loss: 0.0640 - val_acc: 0.9818\n",
      "Epoch 11/25\n",
      "55000/55000 [==============================] - 6s - loss: 0.0564 - acc: 0.9850 - val_loss: 0.0631 - val_acc: 0.9832\n",
      "Epoch 12/25\n",
      "55000/55000 [==============================] - 5s - loss: 0.0479 - acc: 0.9871 - val_loss: 0.0638 - val_acc: 0.9832\n",
      "Epoch 13/25\n",
      "55000/55000 [==============================] - 5s - loss: 0.0477 - acc: 0.9873 - val_loss: 0.0676 - val_acc: 0.9826\n",
      "Epoch 14/25\n",
      "55000/55000 [==============================] - 5s - loss: 0.0450 - acc: 0.9883 - val_loss: 0.0590 - val_acc: 0.9834\n",
      "Epoch 15/25\n",
      "55000/55000 [==============================] - 5s - loss: 0.0408 - acc: 0.9889 - val_loss: 0.0662 - val_acc: 0.9816\n",
      "Epoch 16/25\n",
      "55000/55000 [==============================] - 4s - loss: 0.0356 - acc: 0.9907 - val_loss: 0.0577 - val_acc: 0.9848\n",
      "Epoch 17/25\n",
      "55000/55000 [==============================] - 5s - loss: 0.0363 - acc: 0.9897 - val_loss: 0.0556 - val_acc: 0.9856\n",
      "Epoch 18/25\n",
      "55000/55000 [==============================] - 5s - loss: 0.0324 - acc: 0.9911 - val_loss: 0.0586 - val_acc: 0.9840\n",
      "Epoch 19/25\n",
      "55000/55000 [==============================] - 4s - loss: 0.0326 - acc: 0.9908 - val_loss: 0.0583 - val_acc: 0.9860\n",
      "Epoch 20/25\n",
      "55000/55000 [==============================] - 4s - loss: 0.0304 - acc: 0.9914 - val_loss: 0.0562 - val_acc: 0.9860\n",
      "Epoch 21/25\n",
      "55000/55000 [==============================] - 5s - loss: 0.0267 - acc: 0.9923 - val_loss: 0.0540 - val_acc: 0.9876\n",
      "Epoch 22/25\n",
      "55000/55000 [==============================] - 5s - loss: 0.0270 - acc: 0.9922 - val_loss: 0.0622 - val_acc: 0.9848\n",
      "Epoch 23/25\n",
      "55000/55000 [==============================] - 5s - loss: 0.0249 - acc: 0.9930 - val_loss: 0.0622 - val_acc: 0.9846\n",
      "Epoch 24/25\n",
      "55000/55000 [==============================] - 5s - loss: 0.0238 - acc: 0.9933 - val_loss: 0.0657 - val_acc: 0.9830\n",
      "Epoch 25/25\n",
      "55000/55000 [==============================] - 6s - loss: 0.0222 - acc: 0.9936 - val_loss: 0.0621 - val_acc: 0.9850\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "trained_model_5d_larga_profunda = model.fit(mnist.train.images, \n",
    "                                              mnist.train.labels, \n",
    "                              epochs=epochs, \n",
    "                              batch_size=batch_size, \n",
    "                              validation_data=(mnist.validation.images, \n",
    "                                               mnist.validation.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas à frente, vamos melhorar mais este resultado usando uma rede convolutiva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Funcional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A API sequencial do Keras permite a criação de modelos camada a camada, o que é uma boa representação para muitos modelos. Contudo, modelos mais complexos podem ter múltiplas entradas e saídas e várias sequências de camadas em paralelo. Isso não é simples de representar usando uma abstração sequencial. \n",
    "\n",
    "Uma abstração alternativa seria tratar a rede neural pelo o que ela é de fato: uma complexa combinação de funções. Esta é a ideia por trás da API funcional, que vamos ver agora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uma FNN com Keras Funcional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso primeiro modelo nesta aula foi:\n",
    "\n",
    "```python\n",
    "# define vars\n",
    "input_num_units = 28*28\n",
    "hidden_num_units = 250\n",
    "output_num_units = 10\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=hidden_num_units, \n",
    "       input_dim=input_num_units, activation='relu'))\n",
    "model.add(Dense(units=output_num_units, \n",
    "       input_dim=hidden_num_units, activation='softmax'))\n",
    "       \n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "trained_model_500 = model.fit(mnist.train.images, mnist.train.labels, \n",
    "                              epochs=epochs, \n",
    "                              batch_size=batch_size, \n",
    "                              validation_data=(mnist.validation.images, \n",
    "                                               mnist.validation.labels))\n",
    "```\n",
    "\n",
    "Usando a notação funcional, ele seria re-escrito como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define vars\n",
    "input_num_units = 28*28\n",
    "hidden_num_units = 250\n",
    "output_num_units = 10\n",
    "epochs = 5\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A única parte do modelo que é diferente é realmente esta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "visible = Input(shape=(input_num_units,))\n",
    "hidden = Dense(hidden_num_units, activation = 'relu')(visible)\n",
    "output = Dense(output_num_units, activation = 'softmax')(hidden)\n",
    "model = Model(inputs = visible, outputs = output)\n",
    "\n",
    "# antes:\n",
    "# model = Sequential()\n",
    "# model.add(Dense(units=hidden_num_units, \n",
    "#       input_dim=input_num_units, activation='relu'))\n",
    "# model.add(Dense(units=output_num_units, \n",
    "#       input_dim=hidden_num_units, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou seja, cada camada é tratada como uma função parametrizável. A saída de uma função é entrada para a próxima. A entrada é definida de forma explícita e o modelo é a composição das entradas e suas saídas (neste caso, apenas uma entrada e uma saída). Alternativamente, poderíamos definir este modelo assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visible = Input(shape=(input_num_units,))\n",
    "hidden = Dense(hidden_num_units)(visible)\n",
    "hidden = Activation('relu')(hidden)\n",
    "output = Dense(output_num_units, activation = 'softmax')(hidden)\n",
    "model = Model(inputs = visible, outputs = output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta segunda formulação, em que a função de ativação é separada da camada oculta, é útil para executar operações antes da função de ativação como, por exemplo, BatchNormalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "55000/55000 [==============================] - 1s - loss: 0.3247 - acc: 0.9097 - val_loss: 0.1600 - val_acc: 0.9566\n",
      "Epoch 2/5\n",
      "55000/55000 [==============================] - 1s - loss: 0.1404 - acc: 0.9596 - val_loss: 0.1105 - val_acc: 0.9680\n",
      "Epoch 3/5\n",
      "55000/55000 [==============================] - 1s - loss: 0.0964 - acc: 0.9718 - val_loss: 0.0926 - val_acc: 0.9726\n",
      "Epoch 4/5\n",
      "55000/55000 [==============================] - 1s - loss: 0.0722 - acc: 0.9792 - val_loss: 0.0813 - val_acc: 0.9760\n",
      "Epoch 5/5\n",
      "55000/55000 [==============================] - 1s - loss: 0.0545 - acc: 0.9840 - val_loss: 0.0808 - val_acc: 0.9762\n"
     ]
    }
   ],
   "source": [
    "trained_model_500 = model.fit(mnist.train.images, mnist.train.labels, \n",
    "                              epochs=epochs, \n",
    "                              batch_size=batch_size, \n",
    "                              validation_data=(mnist.validation.images, \n",
    "                                               mnist.validation.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercício__: Usando a API funcional do Keras, crie uma FNN com 10 neurônios de entrada, 3 camadas ocultas com 10, 20 e 10 neurônios, e uma camada de saída com 1 neurônio. Use _relu_ como função de ativação em cada camada oculta e _sigmoid_ para a camada de saída."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\">\n",
    "<a href=\"#modelkf1\" class=\"btn btn-default\" data-toggle=\"collapse\">Solução #1</a>\n",
    "</div>\n",
    "<div id=\"modelkf1\" class=\"collapse\">\n",
    "```\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "inp = Input(shape=(10,))\n",
    "h1  = Dense(10, activation = 'relu')(inp)\n",
    "h2  = Dense(20, activation = 'relu')(h1)\n",
    "h3  = Dense(10, activation = 'relu')(h2)\n",
    "out = Dense( 1, activation = 'sigmoid')(h3)\n",
    "\n",
    "model = Model(inputs = inp, outputs = out)\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinando duas FNNs, um modelo de duas entradas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine que queremos saber se um usuário de uma operadora de TV à cabo pretende ou não desistir da sua assinatura. Suponha que temos duas redes neurais que usam diferentes conjuntos de evidências para prever a desistência. A primeira tem como entrada a quantidade de tempo gasto pelo usuário em dez categorias de programas. A segunda tem como entrada 8 diferentes informações demográficas do usuário, como sexo, idade, faixa de renda, plano assinado, etc. Queremos usar as representações dos usuários aprendidas por estas duas redes para criar um classificador final que tem a seguinte arquitetura: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/FnnMerge.png\" alt=\"FNN combinada\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo temos uma representação da rede em Keras, que demonstra a flexibidade da API funcional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_6 (InputLayer)             (None, 10)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_7 (InputLayer)             (None, 10)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_43 (Dense)                 (None, 10)            110         input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_46 (Dense)                 (None, 10)            110         input_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_44 (Dense)                 (None, 20)            220         dense_43[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_47 (Dense)                 (None, 20)            220         dense_46[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_45 (Dense)                 (None, 10)            210         dense_44[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_48 (Dense)                 (None, 10)            210         dense_47[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 20)            0           dense_45[0][0]                   \n",
      "                                                                   dense_48[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_49 (Dense)                 (None, 10)            210         concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_50 (Dense)                 (None, 10)            110         dense_49[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_51 (Dense)                 (None, 1)             11          dense_50[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 1,411\n",
      "Trainable params: 1,411\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Multiple Inputs\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense \n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "# first input model\n",
    "in1  = Input(shape=(10,))\n",
    "h11  = Dense(10, activation = 'relu')(in1)\n",
    "h12  = Dense(15, activation = 'relu')(h11)\n",
    "out1 = Dense(10, activation = 'relu')(h12)\n",
    "\n",
    "# second input model\n",
    "in2  = Input(shape=(8,))\n",
    "h21  = Dense(8, activation = 'relu')(in2)\n",
    "out2 = Dense(8, activation = 'relu')(h21)\n",
    "\n",
    "# merge input models\n",
    "merge = concatenate([out1, out2])\n",
    "\n",
    "# interpretation model\n",
    "hm1 = Dense(10, activation='relu')(merge)\n",
    "hm2 = Dense(10, activation='relu')(hm1)\n",
    "output = Dense(1, activation='sigmoid')(hm2)\n",
    "model = Model(inputs=[in1, in2], outputs=output)\n",
    "\n",
    "# summarize layers\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O treino e previsão para este modelo devem incruir dados para ambas as entradas, o que resultaria em algo assim:\n",
    "\n",
    "```python\n",
    "model.fit([treino_programacao, treino_demograficos], labels, epochs=10)\n",
    "preds = model.predict([teste_programacao, teste_demograficos])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercício__: Usando a API funcional do Keras, crie uma FNN que combina três outras FNNs. Cada uma das FNNs combinadas tem 10 neurônios de entrada e 2 camadas ocultas com 20 e 10 neurônios. As representações de saída de cada FNN são adicionadas (`keras.layers.merge.add`) umas às outras para formar a representação de entrada da rede de interpretação. Esta rede, por sua vez, é formada por três camadas ocultas de 10 neurônios e uma camada de saída com 5 neurônios. Use _relu_ como função de ativação em cada camada oculta e _sigmoid_ para a camada de saída. Quantos são os parâmetros treináveis desta rede?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\">\n",
    "<a href=\"#modelkf2\" class=\"btn btn-default\" data-toggle=\"collapse\">Solução #1</a>\n",
    "</div>\n",
    "<div id=\"modelkf2\" class=\"collapse\">\n",
    "```\n",
    "# Multiple Inputs\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense \n",
    "from keras.layers.merge import add\n",
    "\n",
    "in1  = Input(shape=(20,))\n",
    "h11  = Dense(20, activation = 'relu')(in1)\n",
    "out1 = Dense(10, activation = 'relu')(h11)\n",
    "\n",
    "in2  = Input(shape=(20,))\n",
    "h21  = Dense(20, activation = 'relu')(in2)\n",
    "out2 = Dense(10, activation = 'relu')(h21)\n",
    "\n",
    "in3  = Input(shape=(20,))\n",
    "h31  = Dense(20, activation = 'relu')(in3)\n",
    "out3 = Dense(10, activation = 'relu')(h31)\n",
    "\n",
    "# merge input models\n",
    "merge = add([out1, out2, out3])\n",
    "\n",
    "# interpretation model\n",
    "hm1 = Dense(10, activation='relu')(merge)\n",
    "hm2 = Dense(10, activation='relu')(hm1)\n",
    "hm3 = Dense(10, activation='relu')(hm2)\n",
    "output = Dense(5, activation='sigmoid')(hm3)\n",
    "model = Model(inputs=[in1, in2, in3], outputs=output)\n",
    "\n",
    "# summarize layers\n",
    "print(model.summary())\n",
    "\n",
    "# na minha conta, 2275 parâmetros.```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nas próximas aulas, vamos ver mais exemplos de arquiteturas complexas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta aula inclui material de <a href = \"https://linkedin.com/in/luisotsm\">Luis Otavio Silveira Martins</a>, <a href = \"https://linkedin.com/in/erich-natsubori-sato\"> Erich Natsubori Sato </a></h4>. Material adicional de Imanol Schlag (https://ischlag.github.io/2016/06/04/how-to-use-tensorboard/) e Faizan Shaikh (https://www.analyticsvidhya.com/blog/2016/10/tutorial-optimizing-neural-networks-using-keras-with-image-recognition-case-study/). A API funcional é baseada no material de Jason Brownlee, disponível em https://machinelearningmastery.com/keras-functional-api-deep-learning/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
