{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressores & Aprendizagem de Máquina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML como um problema de otimização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Estimador linear_, considerando $n-1$ atributos: \n",
    "    \n",
    "    $\\hat{y}(\\textbf{w}) = \\hat{y}(\\textbf{w}, \\textbf{x}) = w_0 x_0 + w_1 x_1 + \\ldots + w_{n-1} x_{n-1} + b$\n",
    "    \n",
    "    $\\hat{y}(\\textbf{w}) = w_0 x_0 + w_1 x_1 + \\ldots + w_{n-1} x_{n-1} + w_{n} x_{n}$ (bias incorporado em $\\textbf{x}$, $b = w_n, x_n = 1$)\n",
    "    \n",
    "    $\\hat{y}(\\textbf{w}) = \\textbf{x} \\dot~\\textbf{w}$\n",
    "\n",
    "    Se considerarmos todas as $m$ instâncias $\\textbf{x}$, temos $\\hat{y}(\\textbf{w}) = X \\dot~\\textbf{w}$ (supondo $X^{(m \\times n)}$ e $\\textbf{w}^{(n \\times 1)}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Eventualmente estimador linear poderia incorporar um transformador, uma _função de ativação_ $\\alpha$: \n",
    "    \n",
    "    $\\hat{y}(\\textbf{w}) = \\alpha(X \\dot~\\textbf{w})$\n",
    "    \n",
    "    Supondo $\\alpha(x) = x$, função identidade, temos o estimador clássico usado na regressão linear, ou seja,     $\\hat{y}(\\textbf{w}) = X \\dot~\\textbf{w}$. Funções de ativação podem ser usadas para transformar a saída para um certo intervalo (-1 a 1, 0 a 1, etc) e também modificarem a natureza simples de um estimador polinomial de grau 1, como o usado aqui. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Função de perda_ (_loss_), avalia o quão bom é o seu estimador $\\hat{y}(\\textbf{w})$ em relação aos valores reais $\\textbf{y}$. Por exemplo, ao adotarmos a média das diferenças dos quadrados (MSE -- mean squared error), temos:\n",
    "\n",
    "    $\\ell(\\hat{y}(\\textbf{w}), \\textbf{y}) = \\frac{1}{m} \\sum_{i}^{m}{(\\hat{y}_i(\\textbf{w}) - y_i)^2}$\n",
    "    \n",
    "    $\\ell(\\hat{y}(\\textbf{w}), \\textbf{y}) = mean ((\\hat{y}(\\textbf{w}) - \\textbf{y})^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Função de perda com _regularizador_ $R$:\n",
    "\n",
    "    $L(\\hat{y}(\\textbf{w}), \\textbf{y}) = \\ell(\\hat{y}(\\textbf{w}), \\textbf{y}) + \\lambda R(\\textbf{w})$\n",
    "    \n",
    "    O regularizador é uma função de penalização que tem por objetivo eliminar certos conjuntos de pesos, privilegiando outros. Ao fazer isso, o modelo se restringe a um menor espaço de pesos, melhorando sua generalização. Exemplos de regularizadores são o L1 ($R(\\mathbf{w}) \\approx \\sum_{\\forall i}{\\frac{w_i}{|w_i|}}$) e o L2 ($R(\\mathbf{w}) \\approx \\sum_{\\forall i}{{\\frac{w_i}{|w_i|}}^2}$). Note que enquanto L1 prefere pesos não nulos, o L2 não gosta de _outliers_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Objetivo de ML é determinar $\\textbf{w}$ que minimiza $L$, ou seja, determinar $\\textbf{w}$ tal que $\\frac{\\partial}{\\partial \\textbf{w}} L(\\hat{y}(\\textbf{w}), \\textbf{y}) = 0$. No nosso caso, supondo $\\lambda = 0$:\n",
    "\n",
    "    $\n",
    "    \\begin{align}\n",
    "        \\frac{\\partial}{\\partial \\textbf{w}} L(\\hat{y}(\\textbf{w}), \\textbf{y}) &= \\frac{\\partial}{\\partial \\textbf{w}} \\frac{1}{m} \\sum_{i}^{m}{(\\hat{y}_i(\\textbf{w}) - y_i)^2} = 0 \\\\\n",
    "        &= \\frac{1}{m} \\sum_{i}^{m}{\\frac{\\partial}{\\partial \\textbf{w}} (\\hat{y}_i(\\textbf{w}) - y_i)^2} \\\\\n",
    "        &= \\frac{1}{m} \\sum_{i}^{m}{\\frac{\\partial}{\\partial \\textbf{w}} (\\textbf{x}_i \\dot~\\textbf{w} - y_i)^2} \\\\\n",
    "        &= \\frac{1}{m} \\sum_{i}^{m}{\\frac{\\partial}{\\partial \\textbf{w}} ({\\textbf{x}_i}^2 \\dot~\\textbf{w}^2 - 2 \\textbf{x}_i \\dot~\\textbf{w} \\dot~y_i + {y_i}^2})  \\\\\n",
    "        &= \\frac{1}{m} \\sum_{i}^{m}{(2 {\\textbf{x}_i}^2 \\dot~\\textbf{w} - 2 \\textbf{x}_i \\dot~y_i)}  \\\\\n",
    "        &= \\frac{2}{m} \\sum_{i}^{m}{({\\textbf{x}_i}^2 \\dot~\\textbf{w} - \\textbf{x}_i \\dot~y_i)}  \\\\\n",
    "        &= \\frac{2}{m} (\\sum_{i}^{m}{{\\textbf{x}_i}^2 \\dot~\\textbf{w}} - \\sum_{i}^{m} {\\textbf{x}_i \\dot~y_i})  \\\\\n",
    "        &= \\frac{2}{m} (X^T X\\dot~\\textbf{w} - X^T \\textbf{y}) \\text{ supondo } X^{(m \\times n)}, \\textbf{y}^{(m \\times 1)} \\text{ e } \\textbf{w}^{(n \\times 1)}\\\\ \n",
    "        &= \\frac{2}{m} X^T (X\\dot~\\textbf{w} - \\textbf{y})  \n",
    "    \\end{align}    \n",
    "    $\n",
    "\n",
    "    Esta função corresponde ao gradiente da função de perda $L$ e pode ser usado para obter $\\textbf{w}$ tanto diretamente quanto iterativamente. Como a solução direta é limitada a valores pequenos de $n$ e $m$, vamos implementar a solução iterativa, o _Gradiente Descendente_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente Descendente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GradientDescendente($X^{m \\times n}$, $\\textbf{y}^{m \\times 1}$, taxa de aprendizado $\\eta$, número de épocas $N$):\n",
    "    - inicie $\\textbf{w}^{n \\times 1}$ com valores aleatorios entre -1 e 1\n",
    "    - para $N$ épocas:\n",
    "        - $\\hat{y}(\\textbf{w}) = X \\dot~\\textbf{w}$\n",
    "        - $\\ell(\\hat{y}(\\textbf{w}), \\textbf{y}) = mean ((\\hat{y}(\\textbf{w}) - \\textbf{y})^2)$\n",
    "        - $\\nabla\\ell = \\frac{2}{m} X^T (X\\dot~\\textbf{w} - ~\\textbf{y})$\n",
    "        - $\\textbf{w}' = \\textbf{w} - \\eta \\nabla\\ell$\n",
    "        - $\\textbf{w} = \\textbf{w}'$\n",
    "    - retorne $\\textbf{w}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GD em Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcocristo/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gd(X, y, n_epochs = 100, lrating = 0.1):    \n",
    "    m, n = X.shape # m instancias e n colunas (including bias)\n",
    "    Y = y.reshape(-1,1)\n",
    "    W = np.random.uniform(-1, 1, (n, 1))\n",
    "    for e in range(n_epochs):\n",
    "        Yhat = np.matmul(X, W)\n",
    "        error = Yhat - Y\n",
    "        loss = np.mean(error ** 2)\n",
    "        gloss = (2./m) * np.matmul(X.T, error)\n",
    "        Wn = W - lrating * gloss\n",
    "        W = Wn\n",
    "        if e%10 == 0:\n",
    "            print(loss)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício**: Compare os otimizadores, usando 40 épocas: \n",
    "* GradientDescentOptimizer(learning_rate = 0.1)\n",
    "* AdadeltaOptimizer(learning_rate = 1.0)\n",
    "* RMSPropOptimizer(learning_rate = 0.1)\n",
    "* AdamOptimizer(learning_rate = 1.0)\n",
    "\n",
    "Uma vez que tiver obtido as curvas de perda para cada método (ls_sgd, ls_adag, ls_rms e ls_adam) você pode plotá-las com o código abaixo:\n",
    "\n",
    "```python\n",
    "# gráficos\n",
    "plt.plot(ls_sgd, label = 'sgd')\n",
    "plt.plot(ls_adag, label = 'adag')\n",
    "plt.plot(ls_rms, label = 'rms')\n",
    "plt.plot(ls_adam, label = 'adam')\n",
    "plt.legend()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\">\n",
    "<a href=\"#losses\" class=\"btn btn-default\" data-toggle=\"collapse\">Solução #1</a>\n",
    "</div>\n",
    "<div id=\"losses\" class=\"collapse\">\n",
    "```\n",
    "# otimizadores\n",
    "def gdtf_opt(X, y, n_epochs = 100, lrating = 0.1, \n",
    "             otimizador = None):\n",
    "    m, n = X.shape\n",
    "    \n",
    "    X = tf.constant(X, dtype = tf.float32, name = \"X\")\n",
    "    y = tf.constant(y.reshape(-1, 1), dtype = tf.float32, name = \"y\")\n",
    "    W = tf.Variable(tf.random_uniform([n, 1], -1.0, 1.0), name = \"W\")\n",
    "    \n",
    "    Yhat = tf.matmul(X, W, name = \"predictions\")\n",
    "    loss = tf.reduce_mean(tf.square(Yhat - y), name = \"loss\")\n",
    "    \n",
    "    training = otimizador.minimize(loss)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    lossvalues = []\n",
    "    with tf.Session() as s:\n",
    "        s.run(init)\n",
    "        for e in range(n_epochs):\n",
    "            W_e, loss_e = s.run([training, loss])\n",
    "            lossvalues += [loss_e]\n",
    "    return W_e, lossvalues\n",
    "\n",
    "# execução\n",
    "W, ls_sgd = gdtf_opt(X, y, n_epochs=40, \n",
    "             otimizador = tf.train.GradientDescentOptimizer(\n",
    "                 learning_rate = 0.1))\n",
    "W, ls_adag = gdtf_opt(X, y, n_epochs=40, \n",
    "             otimizador = tf.train.AdagradOptimizer(learning_rate = 1))\n",
    "W, ls_rms = gdtf_opt(X, y, n_epochs=40, \n",
    "             otimizador = tf.train.RMSPropOptimizer(learning_rate = 0.1))\n",
    "W, ls_adam = gdtf_opt(X, y, n_epochs=40, \n",
    "             otimizador = tf.train.AdamOptimizer(learning_rate = 1))\n",
    "\n",
    "```\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
