{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.bigdatauniversity.com\"><img src = \"https://ibm.box.com/shared/static/jvcqp2iy2jlx2b32rmzdt0tx8lvxgzkp.png\" width = 300, align = \"center\"></a>\n",
    "\n",
    "<h1 align=center><font size = 5>RECURRENT NETWORKS IN DEEP LEARNING</font></h1>\n",
    "\n",
    "\n",
    "## Applying Recurrent Neural Networks for Language Modelling\n",
    "Hello and welcome to this part. In this notebook, we will go over the topic of what Language Modelling is and create a Recurrent Neural Network model based on the Long Short-Term Memory unit to train and be benchmarked by the Penn Treebank. By the end of this notebook, you should be able to understand how TensorFlow builds and executes a RNN model for Language Modelling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Objective\n",
    "By now, you should have an understanding of how Recurrent Networks work -- a specialized model to process sequential data by keeping track of the \"state\" or context. In this notebook, we go over a TensorFlow code snippet for creating a model focused on **Language Modelling** -- a very relevant task that is the cornerstone of many different linguistic problems such as **Speech Recognition, Machine Translation and Image Captioning**. For this, we will be using the Penn Treebank, which is an often-used dataset for benchmarking Language Modelling models.\n",
    "\n",
    "## What exactly is Language Modelling?\n",
    "Language Modelling, to put it simply, **is the task of assigning probabilities to sequences of words**. This means that, given a context of one or a few words in the language the model was trained on, the model should have a knowledge of what are the most probable words or sequence of words for the sentence. Language Modelling is one of the tasks under Natural Language Processing, and one of the most important.\n",
    "\n",
    "<img src=https://ibm.box.com/shared/static/1d1i5gub6wljby2vani2vzxp0xsph702.png width=\"768\"/>\n",
    "<center>*Example of a sentence being predicted*</center>\n",
    "\n",
    "In this example, one can see the predictions for the next word of a sentence, given the context \"This is an\". As you can see, this boils down to a sequential data analysis task -- you are given a word or a sequence of words (the input data), and, given the context (the state), you need to find out what is the next word (the prediction). This kind of analysis is very important for language-related tasks such as **Speech Recognition, Machine Translation, Image Captioning, Text Correction** and many other very relevant problems. \n",
    "\n",
    "<img src=https://ibm.box.com/shared/static/az39idf9ipfdpc5ugifpgxnydelhyf3i.png width=\"1080\"/>\n",
    "<center>*The above example schematized as an RNN in execution*</center>\n",
    "\n",
    "As the above image shows, Recurrent Network models fit this problem like a glove. Alongside LSTM and its capacity to maintain the model's state for over one thousand time steps, we have all the tools we need to undertake this problem. The goal for this notebook is to create a model that can reach **low levels of perplexity** on our desired dataset.\n",
    "\n",
    "For Language Modelling problems, **perplexity** is the way to gauge efficiency. Perplexity is simply a measure of how well a probabilistic model is able to predict its sample. A higher-level way to explain this would be saying that **low perplexity means a higher degree of trust in the predictions the model makes**. Therefore, the lower perplexity is, the better.\n",
    "\n",
    "## The Penn Treebank dataset\n",
    "Historically, datasets big enough for Natural Language Processing are hard to come by. This is in part due to the necessity of the sentences to be broken down and tagged with a certain degree of correctness -- or else the models trained on it won't be able to be correct at all. This means that we need a **large amount of data, annotated by or at least corrected by humans**. This is, of course, not an easy task at all.\n",
    "\n",
    "The Penn Treebank, or PTB for short, is a dataset maintained by the University of Pennsylvania. It is *huge* -- there are over **four million and eight hundred thousand** annotated words in it, all corrected by humans. It is composed of many different sources, from abstracts of Department of Energy papers to texts from the Library of America. Since it is verifiably correct and of such a huge size, the Penn Treebank has been used time and time again as a benchmark dataset for Language Modelling.\n",
    "\n",
    "The dataset is divided in different kinds of annotations, such as Piece-of-Speech, Syntactic and Semantic skeletons. For this example, we will simply use a sample of clean, non-annotated words (with the exception of one tag -- `<unk>`, which is used for rare words such as uncommon proper nouns) for our model. This means that we just want to predict what the next words would be, not what they mean in context or their classes on a given sentence. \n",
    "<br/>\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "<center>the percentage of lung cancer deaths among the workers at the west `<unk>` mass. paper factory appears to be the highest for any asbestos workers studied in western industrialized countries he said \n",
    " the plant which is owned by `<unk>` & `<unk>` co. was under contract with `<unk>` to make the cigarette filters \n",
    " the finding probably will support those who argue that the u.s. should regulate the class of asbestos including `<unk>` more `<unk>` than the common kind of asbestos `<unk>` found in most schools and other buildings dr. `<unk>` said</center>\n",
    "</div>\n",
    " <center>*Example of text from the dataset we are going to use, `ptb.train`*</center>\n",
    " <br/>\n",
    "\n",
    "<h2>Word Embeddings</h2><br/>\n",
    "\n",
    "For better processing, in this example model we will make use of **word embeddings**, which are **a way of representing sentence structures or words as n-dimensional vectors (where n is a reasonably high number, such as 200 or 500) of real numbers**. Basically, we will assign each word a randomly-initialized vector and input those into the network to be processed. After a number of iterations, these vectors are expected to assume values that help the network to correctly predict what it needs to -- in our case, the probable next word in the sentence. This is shown to be very effective in Natural Language Processing tasks, and is a commonplace practice.\n",
    "<br/><br/>\n",
    "<font size = 4><strong>\n",
    "$$Vec(\"Example\") = [0.02, 0.00, 0.00, 0.92, 0.30,...]$$\n",
    "</font></strong>\n",
    "<br/>\n",
    "Word Embedding tends to group up similarly used words *reasonably* together in the vectorial space. For example, if we use T-SNE (a dimensional reduction visualization algorithm) to flatten the dimensions of our vectors into a 2-dimensional space and use the words these vectors represent as their labels, we might see something like this:\n",
    "\n",
    "<img src=https://ibm.box.com/shared/static/bqhc5dg879gcoabzhxra1w8rkg3od1cu.png width=\"800\"/>\n",
    "<center>*T-SNE Mockup with clusters marked for easier visualization*</center>\n",
    "\n",
    "As you can see, words that are frequently used together, in place of each other, or in the same places as them tend to be grouped together -- being closer together the higher these correlations are. For example, \"None\" is pretty semantically close to \"Zero\", while a phrase that uses \"Italy\" can probably also fit \"Germany\" in it, with little damage to the sentence structure. A vectorial \"closeness\" for similar words like this is a great indicator of a well-built model.\n",
    "\n",
    "---\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the LSTM Recurrent Network model in TensorFlow\n",
    "Now that we know exactly what we are doing, we can start building our model using TensorFlow. The very first thing we need to do is download and extract the `simple-examples` dataset, which can be done by executing the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-01-09 17:52:18--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
      "Resolvendo www.fit.vutbr.cz (www.fit.vutbr.cz)... 147.229.9.23, 2001:67c:1220:809::93e5:917\n",
      "Conectando-se a www.fit.vutbr.cz (www.fit.vutbr.cz)|147.229.9.23|:80... conectado.\n",
      "A requisição HTTP foi enviada, aguardando resposta... 200 OK\n",
      "Tamanho: 34869662 (33M) [application/x-gtar]\n",
      "Salvando em: “simple-examples.tgz”\n",
      "\n",
      "simple-examples.tgz 100%[===================>]  33,25M   162KB/s    in 3m 42s  \n",
      "\n",
      "2017-01-09 17:56:03 (153 KB/s) - “simple-examples.tgz” salvo [34869662/34869662]\n",
      "\n",
      "tar: /resources/data: Não é possível executar a ação: open: Arquivo ou diretório não encontrado\n",
      "tar: Error is not recoverable: exiting now\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz \n",
    "!tar xzf simple-examples.tgz -C /resources/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to import the necessary modules for our code. We need **`numpy` and `tensorflow`**, obviously. Additionally, we can import directly the **`tensorflow.models.rnn.rnn`** model, which includes the function for building RNNs, and **`tensorflow.models.rnn.ptb.reader`** which is the helper module for getting the input data from the dataset we just downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.models.rnn.ptb import reader\n",
    "#from tensorflow.models.rnn import rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, for the sake of making it easy to play around with the model's hyperparameters, we can declare them beforehand. Feel free to change these -- you will see a difference in performance each time you change those!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initial weight scale\n",
    "init_scale = 0.1\n",
    "#Initial learning rate\n",
    "learning_rate = 1.0\n",
    "#Maximum permissible norm for the gradient (For gradient clipping -- another measure against Exploding Gradients)\n",
    "max_grad_norm = 5\n",
    "#The number of layers in our model\n",
    "num_layers = 2\n",
    "#The total number of recurrence steps, also known as the number of layers when our RNN is \"unfolded\"\n",
    "num_steps = 20\n",
    "#The number of processing units (neurons) in the hidden layers\n",
    "hidden_size = 200\n",
    "#The maximum number of epochs trained with the initial learning rate\n",
    "max_epoch = 4\n",
    "#The total number of epochs in training\n",
    "max_max_epoch = 13\n",
    "#The probability for keeping data in the Dropout Layer (This is an optimization, but is outside our scope for this notebook!)\n",
    "#At 1, we ignore the Dropout Layer wrapping.\n",
    "keep_prob = 1\n",
    "#The decay for the learning rate\n",
    "decay = 0.5\n",
    "#The size for each batch of data\n",
    "batch_size = 20\n",
    "#The size of our vocabulary\n",
    "vocab_size = 10000\n",
    "#Training flag to separate training from testing\n",
    "is_training = 1\n",
    "#Data directory for our dataset\n",
    "data_dir = \"/home/marco/Downloads/resources/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then create the Class that represents our model. This class needs a few things:\n",
    "- We have to create the model in accordance with our defined hyperparameters\n",
    "- We have to create the placeholders for our input data and expected outputs (the real data)\n",
    "- We have to create the LSTM cell structure and connect them with our RNN structure\n",
    "- We have to create the word embeddings and point them to the input data\n",
    "- We have to create the input structure for our RNN\n",
    "- We have to instanciate our RNN model and retrieve the variable in which we should expect our outputs to appear\n",
    "- We need to create a logistic structure to return the probability of our words\n",
    "- We need to create the loss and cost functions for our optimizer to work, and then create the optimizer\n",
    "- And finally, we need to create a training operation that can be run to actually train our model\n",
    "\n",
    "There is a lot to be done and a ton of information to process at the same time, so go over this code slowly. It may seem complex at first, but if you try to ally what you just learned about language modelling to the code you see, you should be able to understand it.\n",
    "\n",
    "This code is adapted from the [PTBModel](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py) example bundled with the TensorFlow source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "\n",
    "    def __init__(self, is_training):\n",
    "        ######################################\n",
    "        # Setting parameters for ease of use #\n",
    "        ######################################\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        ###############################################################################\n",
    "        # Creating placeholders for our input data and expected outputs (target data) #\n",
    "        ###############################################################################\n",
    "        self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        self._targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "\n",
    "        ##########################################################################\n",
    "        # Creating the LSTM cell structure and connect it with the RNN structure #\n",
    "        ##########################################################################\n",
    "        # Create the LSTM unit. Size is the same as the size of our hidden layer, and no bias is added to the Forget Gate\n",
    "        # This creates only the structure for the LSTM and has to be associated with a RNN unit still\n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(size, forget_bias=0.0)\n",
    "        \n",
    "        # Unless you changed keep_prob, this won't actually execute -- this is a dropout wrapper for our LSTM unit\n",
    "        # This is an optimization of the LSTM output, but is not needed at all\n",
    "        if is_training and keep_prob < 1:\n",
    "            lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\n",
    "        \n",
    "        # By taking in the LSTM cells as parameters, the MultiRNNCell function junctions the LSTM units to the RNN units\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * num_layers)\n",
    "\n",
    "        # Define the initial state, i.e., the model state for the very first data point\n",
    "        self._initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        ####################################################################\n",
    "        # Creating the word embeddings and pointing them to the input data #\n",
    "        ####################################################################\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            # Create the embeddings for our input data\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, size])\n",
    "            # Define where to get the data for our embeddings from\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self._input_data)\n",
    "\n",
    "        # Unless you changed keep_prob, this won't actually execute -- this is a dropout addition for our inputs\n",
    "        # This is an optimization of the input processing and is not needed at all\n",
    "        if is_training and keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, keep_prob)\n",
    "\n",
    "        ############################################\n",
    "        # Creating the input structure for our RNN #\n",
    "        ############################################\n",
    "        # The input structure is fed from the embeddings, which are filled in by the input data\n",
    "        inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(1, num_steps, inputs)]\n",
    "        \n",
    "        ####################################################################################################\n",
    "        # Instanciating our RNN model and retrieving the structure for returning the outputs and the state #\n",
    "        ####################################################################################################\n",
    "        outputs, state = rnn.rnn(cell, inputs, initial_state=self._initial_state)\n",
    "\n",
    "        #########################################################################\n",
    "        # Creating a logistic unit to return the probability of the output word #\n",
    "        #########################################################################\n",
    "        output = tf.reshape(tf.concat(1, outputs), [-1, size])\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [size, vocab_size])\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "\n",
    "        #########################################################################\n",
    "        # Defining the loss and cost functions for the model's learning to work #\n",
    "        #########################################################################\n",
    "        loss = tf.nn.seq2seq.sequence_loss_by_example([logits], [tf.reshape(self._targets, [-1])],\n",
    "                                                      [tf.ones([batch_size * num_steps])])\n",
    "        self._cost = cost = tf.reduce_sum(loss) / batch_size\n",
    "\n",
    "        # Store the final state\n",
    "        self._final_state = state\n",
    "\n",
    "        #Everything after this point is relevant only for training\n",
    "        if not is_training:\n",
    "            return\n",
    "\n",
    "        #################################################\n",
    "        # Creating the Training Operation for our Model #\n",
    "        #################################################\n",
    "        # Create a variable for the learning rate\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        # Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
    "        tvars = tf.trainable_variables()\n",
    "        # Define the gradient clipping threshold\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), max_grad_norm)\n",
    "        # Create the gradient descent optimizer with our learning rate\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        # Create the training TensorFlow Operation through our optimizer\n",
    "        self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    # Helper functions for our LSTM RNN class\n",
    "\n",
    "    # Assign the learning rate for this model\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(tf.assign(self.lr, lr_value))\n",
    "\n",
    "    # Returns the input data for this model at a point in time\n",
    "    @property\n",
    "    def input_data(self):\n",
    "        return self._input_data\n",
    "\n",
    "    # Returns the targets for this model at a point in time\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self._targets\n",
    "\n",
    "    # Returns the initial state for this model\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    # Returns the defined Cost\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    # Returns the final state for this model\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "    # Returns the current learning rate for this model\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    # Returns the training operation defined for this model\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, the actual structure of our Recurrent Neural Network with Long Short-Term Memory is finished. What remains for us to do is to actually create the methods to run through time -- that is, the `run_epoch` method to be run at each epoch and a `main` script which ties all of this together.\n",
    "\n",
    "What our `run_epoch` method should do is take our input data and feed it to the relevant operations. This will return at the very least the current result for the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################################################################################################\n",
    "# run_epoch takes as parameters the current session, the model instance, the data to be fed, and the operation to be run #\n",
    "##########################################################################################################################\n",
    "def run_epoch(session, m, data, eval_op, verbose=False):\n",
    "\n",
    "    #Define the epoch size based on the length of the data, batch size and the number of steps\n",
    "    epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    state = m.initial_state.eval()\n",
    "    \n",
    "    #For each step and data point\n",
    "    for step, (x, y) in enumerate(reader.ptb_iterator(data, m.batch_size,\n",
    "                                                    m.num_steps)):\n",
    " \n",
    "        #Evaluate and return cost, state by running cost, final_state and the function passed as parameter\n",
    "        cost, state, _ = session.run([m.cost, m.final_state, eval_op],\n",
    "                                     {m.input_data: x,\n",
    "                                      m.targets: y,\n",
    "                                      m.initial_state: state})\n",
    "        \n",
    "        #Add returned cost to costs (which keeps track of the total costs for this epoch)\n",
    "        costs += cost\n",
    "        \n",
    "        #Add number of steps to iteration counter\n",
    "        iters += m.num_steps\n",
    "\n",
    "        if verbose and step % (epoch_size // 10) == 10:\n",
    "            print(\"%.3f perplexity: %.3f speed: %.0f wps\" % (step * 1.0 / epoch_size, np.exp(costs / iters),\n",
    "              iters * m.batch_size / (time.time() - start_time)))\n",
    "\n",
    "    # Returns the Perplexity rating for us to keep track of how the model is evolving\n",
    "    return np.exp(costs / iters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create the `main` method to tie everything together. The code here reads the data from the directory, using the `reader` helper module, and then trains and evaluates the model on both a testing and a validating subset of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "~/Downloads/resources/data/simple-examples/data/ptb.train.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-05f8baa6f857>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Reads the data and separates it into training data, validation data and testing data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mraw_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mptb_raw_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Initializes the Execution Graph and the Session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/marco/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/tensorflow/models/rnn/ptb/reader.pyc\u001b[0m in \u001b[0;36mptb_raw_data\u001b[1;34m(data_path)\u001b[0m\n\u001b[0;32m     71\u001b[0m   \u001b[0mtest_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ptb.test.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m   \u001b[0mword_to_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_build_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m   \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_file_to_word_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_to_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m   \u001b[0mvalid_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_file_to_word_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_to_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/marco/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/tensorflow/models/rnn/ptb/reader.pyc\u001b[0m in \u001b[0;36m_build_vocab\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_build_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m   \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_read_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m   \u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/marco/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/tensorflow/models/rnn/ptb/reader.pyc\u001b[0m in \u001b[0;36m_read_words\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_read_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"<eos>\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/marco/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.pyc\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    104\u001b[0m       \u001b[1;34m'n'\u001b[0m \u001b[0mbytes\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mor\u001b[0m \u001b[0mwhole\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m \u001b[0mrequested\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/marco/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.pyc\u001b[0m in \u001b[0;36m_preread_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     71\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         self._read_buf = pywrap_tensorflow.CreateBufferedInputStream(\n\u001b[1;32m---> 73\u001b[1;33m             compat.as_bytes(self.__name), 1024 * 512, status)\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_prewrite_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/marco/Canopy/appdata/canopy-1.7.3.3335.rh5-x86_64/lib/python2.7/contextlib.pyc\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/marco/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.pyc\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    467\u001b[0m           \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    470\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_DeleteStatus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: ~/Downloads/resources/data/simple-examples/data/ptb.train.txt"
     ]
    }
   ],
   "source": [
    "# Reads the data and separates it into training data, validation data and testing data\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, _ = raw_data\n",
    "\n",
    "#Initializes the Execution Graph and the Session\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "    initializer = tf.random_uniform_initializer(-init_scale,\n",
    "                                            init_scale)\n",
    "    \n",
    "    # Instantiates the model for training\n",
    "    with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "        m = PTBModel(is_training=True)\n",
    "        \n",
    "    # Reuses the trained parameters for the validation and testing models\n",
    "    # They are different instances but use the same variables for weights and biases, they just don't change when data is input\n",
    "    with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n",
    "        mvalid = PTBModel(is_training=False)\n",
    "        mtest = PTBModel(is_training=False)\n",
    "\n",
    "    #Initialize all variables\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    for i in range(max_max_epoch):\n",
    "        # Define the decay for this epoch\n",
    "        lr_decay = decay ** max(i - max_epoch, 0.0)\n",
    "        \n",
    "        # Set the decayed learning rate as the learning rate for this epoch\n",
    "        m.assign_lr(session, learning_rate * lr_decay)\n",
    "\n",
    "        print(\"Epoch %d : Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "        \n",
    "        # Run the loop for this epoch in the training model\n",
    "        train_perplexity = run_epoch(session, m, train_data, m.train_op,\n",
    "                                   verbose=True)\n",
    "        print(\"Epoch %d : Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "        \n",
    "        # Run the loop for this epoch in the validation model\n",
    "        valid_perplexity = run_epoch(session, mvalid, valid_data, tf.no_op())\n",
    "        print(\"Epoch %d : Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "    \n",
    "    # Run the loop in the testing model to see how effective was our training\n",
    "    test_perplexity = run_epoch(session, mtest, test_data, tf.no_op())\n",
    "    \n",
    "    print(\"Test Perplexity: %.3f\" % test_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model's perplexity rating drops very quickly after a few iterations. As was elaborated before, **lower Perplexity means that the model is more certain about its prediction**. As such, we can be sure that this model is performing well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of the **Applying Recurrent Neural Networks to Text Processing** notebook. Hopefully you now have a better understanding of Recurrent Neural Networks and how to implement one utilizing TensorFlow. Thank you for reading this notebook, and good luck on your studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by <a href=\"https://br.linkedin.com/in/walter-gomes-de-amorim-junior-624726121\">Walter Gomes de Amorim Junior</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
