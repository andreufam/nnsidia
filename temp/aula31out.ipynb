{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcocristo/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tradicionalmente, aplicações relacionadas com linguagem natural tratam palavras como eventos discretos. Assim, duas palavras, como _cão_ e _gato_, são representados por identificadores arbitrários que não incorporam qualquer informação sobre as relações existentes entre esses conceitos como, por exemplo, que eles são animais mamíferos, com quatro patas, domesticáveis, etc. Outro problema deste tipo de codificação é que ela induz esparsidade, o que torna complexo o treinamento de um modelo estatístico.\n",
    "\n",
    "Uma alternativa à representação esparsa arbitrária é a representação com vetores densos. Para obter tais vetores, podemos usar a Hipótese de Distribuição que estabelece que _palavras que aparecem no mesmo contexto, compartilham a mesma semântica_. Há dois tipos gerais de técnicas que se baseiam nessa hipótese: as baseadas em contagem, como a _Indexação Semântica Latente_, e as baseadas em previsão, como os _modelos de linguagem neuro-probabilísticos_. Nesta aula, nos concentramos nas últimas técnicas, que tentam prever diretamente uma palavra alvo apartir da suas vizinhas, aprendendo para isso pequenos vetores densos, que são parâmetros dos modelos. Estes vetores são chamados _embeddings_ de palavras (ou vetores de palavras, na falta de uma melhor tradução para o Português). \n",
    "\n",
    "Entre os vários métodos propostos nesta linha, um de grande destaque é o Word2vec (Mikolov et al., http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf), onde redes neurais rasas e simples são usadas para aprender os embeddings. Com o uso de redes rasas, é possível escalar o modelo para lidar com trilhões de ocorrências de palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelos de linguagem neuro-probabilísticos em geral são treinados de acordo com o princípio da máxima verossimelhança. Em particular, o modelo maximiza a probabilidade da próxima palavra, dada as palavras de contexto. Um modelo muito simples, baseado nesta ideia, é ilustrado a seguir.\n",
    "\n",
    "Nele, há uma única camada escondida de $N$ neurônios. A entrada e a saída são conjuntos de $V$ neurônios, onde $V$ é o tamanho do vocabulário. Os neurônios de entrada representam uma palavra usando codificação _hot vector_. Nos neurônios de saída, a probabilidade de cada palavra é obtida via _softmax_. No exemplo a seguir, dada a frase _the cat_, a palavra _the_ representa a entrada (o contexto) que será usada para prever a palavra-alvo, _cat_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/emb0.png\" alt=\"Exemplo de RNN\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que como a entrada é 1-hot-vector, se a saída da camada oculta for linear, a saída dessa camada é meramente uma cópia dos pesos da entrada (${\\bf h}_w = {\\bf x}_w {\\bf W}_{I} = {\\bf W}_{Iw}$). Ou seja, o código usado para representar cada palavra (${\\bf h}_w$) é o peso da entrada corresponde a esta palavra (${\\bf W}_{Iw}$). E este será aprendido durante o treino. \n",
    "\n",
    "A estimativa da palavra alvo $w_t$ dado contexto é dada pela softmax, ou seja:\n",
    "\n",
    "$$P(w_t|{\\bf h}_w) = softmax(score(w_t, {\\bf h}_w)) = \\frac{e^{score(w_t, {\\bf h}_w)}}{\\sum_{w' \\in \\mathcal{V}}{e^{score(w', {\\bf h}_w)}}}$$\n",
    "\n",
    "onde $\\mathcal{V}$ é o vocabulário e _score_ é normalmente o produto interno entre os vetores. Para calcular o erro, basta subtrair o vetor da palavra alvo do vetor de estimativa softmax. Com base no erro, os pesos são atualizados, de forma que a própria representação das palavras (${\\bf h}_w$) é moficada para garantir que _palavras que normalmente ocorrem no mesmo contexto sejam capazes de prever umas às outras!_\n",
    "\n",
    "Observe que o contexto, em geral, é formado por mais de uma palavra (e o alvo não precisa ser necessariamente a próxima). Para isso, basta representar cada palavra na entrada. Abaixo, por exemplo, temos uma arquitetura em que o contexto é formado por _m = 1_ palavras que ocorrem antes e depois da palavra alvo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cbow.png\" alt=\"Exemplo de RNN\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste caso, dada a frase _the black cat_, o contexto é formado pelas palavras _the_ e _cat_ e o alvo é _black_. Note que uma camada de projeção aparece no desenho para indicar que as várias entradas discretas serão combinadas para formar uma entrada única: elas podem ser combinadas pela média, soma, multiplicação ou mesmo concatenação. \n",
    "\n",
    "Este modelo simples é chamado CBOW (_continuous bag-of-words_). Um modelo alternativo é o _Skip-gram_, que é basicamente a versão invertida do CBOW: ou seja, a entrada corresponde à palavra alvo e o alvo corresponde ao contexto. Embora a diferença entre os modelos pareça arbitrária, o skip-gram tem algumas vantagens em termos de amostragem estatística que o tornam uma solução melhor para o caso de grande número de ocorrência de palavras.\n",
    "\n",
    "A seguir, vamos estudar esses modelos na prática."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtendo dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos inicialmente obter palavras de uma amostra de 100Mb a Wikipedia em inglês de 2006."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_words_data():\n",
    "    with zipfile.ZipFile('data/wiki2006_100m.zip') as f:\n",
    "        data = f.read(f.namelist()[0])\n",
    "    return data.decode(\"ascii\").split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = fetch_words_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(words[:25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17005207"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando um dicionário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary_size = 50000\n",
    "\n",
    "vocabulary = [(\"UNK\", None)] + Counter(words).most_common(vocabulary_size-1)\n",
    "vocabulary = np.array([w for w, _ in vocabulary])\n",
    "dictionary = {w: code for code, w in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5183, 5239)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary['tiger'], dictionary['anarchism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.array([dictionary.get(word, 0) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarchism originated as a term of\n",
      "[5239 3084   12    6  195    2]\n",
      "[u'anarchism', u'originated', u'as', u'a', u'term', u'of']\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(words[:6]))\n",
    "print(data[:6])\n",
    "print([vocabulary[id] for id in data[:6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo que vamos usar é o _skip-gram_. Neste modelo, supondo um contexto de duas palavras, para a sentença _anarchism originated as a term of_, teríamos os seguintes pares de contexto e alvo:\n",
    "\n",
    "- ([anarchism, as], originated), ([originated, a], as),  ([as, term], a), ([a, of], term), ...\n",
    "\n",
    "Como no modelo skip-gram nós usamos o alvo para prever o contexto, temos:\n",
    "\n",
    "- (originated, anarchism), (originated, as), (as, originated), (as, a), (a, as), (a, term), (term, a), (term, of) ...\n",
    "\n",
    "A seguir vamos definir uma função (generate_batch) que obtem exemplos para treinar um modelo _skip gram_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# num_skips = How many times to reuse an input to generate a label\n",
    "# skip_window = How many words to consider left and right.\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [ skip_window ]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n",
      "(8, 1)\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "batch, labels = generate_batch(8, 2, 1)\n",
    "print(batch.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3084, 3084,   12,   12,    6,    6,  195,  195], dtype=int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 originated -> anarchism\n",
      "1 originated -> as\n",
      "2         as -> a\n",
      "3         as -> originated\n",
      "4          a -> term\n",
      "5          a -> as\n",
      "6       term -> a\n",
      "7       term -> of\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(batch)):\n",
    "    print('%d %10s -> %s' % (i, \n",
    "                             vocabulary[batch[i]], \n",
    "                             vocabulary[labels[:,0][i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample.\n",
    "\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "# Input data.\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que maximizar o $\\log P(w_t|{\\bf h}_w)$ corresponde a maximizar a função de custo:\n",
    "\n",
    "$$J = \\log \\frac{e^{score(w_t, {\\bf h}_w)}}{\\sum_{w' \\in \\mathcal{V}}{e^{score(w', {\\bf h}_w)}}} = score(w_t, {\\bf h}_w) - \\log{\\sum_{w' \\in \\mathcal{V}}{e^{score(w', {\\bf h}_w)}}}$$\n",
    "\n",
    "Enquanto $J$ corresponde a uma estimativa adequada de um ponto de vista probabilístico, ela é muito cara para ser processada para cada instância de treino, uma vez que o somatório é marginalizado sobre todas as palavras do vocabulário ($w' \\in \\mathcal{V}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, em lugar de calcular um modelo probabilístico completo, nosso objetivo no treino será o de uma regressão logística binária. Em particular, para nosso modelo skip-gram, queremos discriminar quem é o melhor previsor para as palavras de contexto ${\\bf h}$: o alvo $w_t$ ou $k$ palavras tomadas aleatoriamente (ruído ${\\bf \\tilde{w}}$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formalmente, nosso objetivo passa a ser maximizar:\n",
    "\n",
    "$$J_{NEG} = \\log Q_{\\theta}(D=1 | w_t, {\\bf h}) + k \\mathop{\\mathbb{E}}_{{\\bf \\tilde{w}} \\approx P_{noise}}[\\log ⁡Q_{\\theta} (D=0 | {\\bf \\tilde{w}}, {\\bf h})]$$\n",
    "\n",
    "onde $\\log Q_{\\theta}(D=1 | w_t, {\\bf h})$ é a probabilidade (obtida com um regressor logístico binário) de $w_t$ ser vista no contexto ${\\bf h}$ para um conjunto de dados $D$, considerando um conjunto de embeddings $\\theta$. O segundo componente da equação corresponde à probabilidade de que $k$ palavras contrastivas ${\\bf \\tilde{w}}$ (tomadas aleatorimente de $\\mathcal{V}$) _não_ sejam vistas no contexto ${\\bf h}$. Por exemplo, digamos que temos a frase _the black cat_, com contexto dado por (_the_, _cat_) e alvo dado por _cat_. Em $J_{NEG}$ pretende-se maximizar a probabilidade de _black_ ser vista no contexto (_the_, _cat_) ao mesmo tempo que se minimiza a probabilidade de que $k$ palavras tomadas aleatoriamente, como (_then_, _observe_, _those_) para $k=3$, sejam vistas no mesmo contexto. A figura abaixo ilustra essa estratégia em um modelo skip-gram:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/skip-gram-neg-sampling.png\" alt=\"Relações em embeddings\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em suma, o objetivo é máximizado quando o modelo associa probabilidades altas para palavras que fazem sentido no contexto (_black_) e baixa para palavras ruído (_then_, _observe_, _those_). Esta estratégia é chamada _Negative Sampling_ e há duas boas razões para usá-la: (1) as atualizações que ela propões aproximam as atualizações da softmax no limite; e (2) o custo por instância de treino do estimador cai de $O(\\mathcal{V})$ para $O(k)$, $k << \\mathcal{V}$. O tensorflow oferece um estimador baseado nesta ideia, o _noise-contrastive estimation (NCE) loss_ -- `tf.nn.nce_loss()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "embedding_size = 150\n",
    "\n",
    "# Look up embeddings for inputs.\n",
    "init_embeds = tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)\n",
    "embeddings = tf.Variable(init_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_inputs = tf.placeholder(tf.int32, shape=[None])\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct the variables for the NCE loss\n",
    "nce_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                        stddev=1.0 / np.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "# Compute the average NCE loss for the batch.\n",
    "# tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "# time we evaluate the loss.\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.nce_loss(nce_weights, nce_biases, train_labels, embed,\n",
    "                   num_sampled, vocabulary_size))\n",
    "\n",
    "# Construct the Adam optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "# Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), axis=1, keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "# Add variable initializer.\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_with_labels(low_dim_embs, labels):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "    plt.figure(figsize=(18, 18))  #in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i,:]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                     xy=(x, y),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "plot_only = 500\n",
    "low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only,:])\n",
    "labels = [vocabulary[i] for i in range(plot_only)]\n",
    "plot_with_labels(low_dim_embs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como treinamos pouco nossos embeddings, muitas das suas propriedades não estão ainda observáveis. Contudo, ao analisar um conjunto de embeddings bem treinado, é fácil perceber que os embeddings capturam informação semântica geral e bastante útil. De fato, o uso de embeddings melhora o desempenho de várias atividades envolvendo linguagem natural, tais como agrupamento, classificação, reconhecimento de tipos de discurso, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando _embeddings_ Pré-treinados (com Gensim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especializações comumente observadas nos eixos dos embeddings incluem relacionamentos semânticos como o de gênero (feminino v. masculino), conjugações verbais, relações geográficas, etc. Algumas destas são demonstradas nas figuras abaixo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/linear-relationships.png\" alt=\"Relações em embeddings\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar tais especializações em conjuntos pré-treinados de embeddings. Há vários destes conjuntos disponíveis:\n",
    "\n",
    "* Inglês: https://github.com/alexandres/lexvec#pre-trained-vectors\n",
    "* Inlgês (Glove): https://nlp.stanford.edu/projects/glove/\n",
    "* Português: http://www.nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc\n",
    "\n",
    "Nos exemplos a seguir, usamos um conjunto de embeddings para língua portuguesa, obtidos do NILC, treinados com skip-gram. Dos 2.6 Gbytes de dados da coleção original, separamos cerca de 24 mil embeddings, usando uma lista de palavras da língua portuguesa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_embeddings(fname):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(fname,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        if len(row) > 2:\n",
    "            vocab.append(row[0])\n",
    "            embd.append(row[1:])\n",
    "    print('done: %d %d-D embeddings loaded!' % (len(vocab), len(embd[0])))\n",
    "    file.close()\n",
    "    return vocab, embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = 'data/subset_skip_s300.txt'\n",
    "vocabulary, embd = load_embeddings(fname)\n",
    "vocab_size = len(vocabulary)\n",
    "embedding_dim = len(embd[0])\n",
    "final_embeddings = np.asarray(embd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O material desta aula se baseou no tutorial de Word2vec do tensorflow (https://www.tensorflow.org/tutorials/word2vec) e no livro de Aurélien Géron, \"Hands-on Machine Learning with Skitlearn and Tensorflow\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
