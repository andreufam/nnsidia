{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLearning: Aprendizagem por reforço"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Básico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ideia**: Qual o valor/utilidade de uma ação em dado estado?\n",
    "\n",
    "-- _qlearning_ é um algoritmo para aprender recompensas de _longo_ prazo!\n",
    "\n",
    "Para simplificar, suponha que um agente precisa escolher uma ação _a_ em um certo estado _s_. A ação deve mudar o estado para _s'_ e, eventualmente, ter uma implicação vantajosa ou desvantajosa para o agente. A vantagem ou desvantagem pode ser vista pelo agente como uma recompensa (ou uma punição). A utilidade da ação (_Q_) está então relacionada à probabilidade dela resultar em um prêmio para o agente.\n",
    "\n",
    "Em termos práticos, o agente precisa aprender uma tabela (_Tabela Q_), que associa o estado à utilidade esperada (Q) de uma ação. No futuro, ele pode escolher entre uma ação de acordo com a tabela ou explorar (agir aleatoriamente). Cada nova ação atualiza a tabela, de acordo com a Equação de Bellman:\n",
    "\n",
    "$$Q(s, a) = r + \\gamma(\\max Q(s',a'))$$\n",
    "\n",
    "ou seja, a utilidade esperada $Q(s,a)$ para a ação atual $a$ no estado $s$ corresponde à recompensa imediata $r$ mais o máximo valor esperado para o estado futuro $s'$ que será atingido (esse máximo valor será alcançado no futuro através da ação $a'$). A utilidade da tabela é descontada por um fator $\\gamma$, que pode ser pensado como uma certa taxa de \"importância\" do futuro possível em relação ao prêmio já obtido.\n",
    "\n",
    "Note que _r_ pode ser zero! ou seja, não há _supervisão_ garantida para toda a ação do agente. Assim, ele precisa aprender compromissos de longo prazo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Fronzenlake_ e gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos aprender as ideias básicas de aprendizagem por reforço ao criar um agente que aprenda a jogar _FrozenLake_.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "_FrozenLake_ é um jogo feito em um tabuleiro de 4x4 onde o agente tem que sair do ponto inicial S para a meta G, evitando buracos H. Ou seja, ele deve tentar andar apenas pelo superfície congelada de uma lago (posições indicadas por F). Aleatoriamente, o vento pode empurrar o agente para um buraco sem que ele possa fazer nada a respeito. Ou seja, _aprendizado perfeito é impossível_! \n",
    "\n",
    "Note que, dada uma configuração do jogo (o exemplo abaixo), apenas há 16 estados possíveis (cada bloco em que o agente pode estar) para o qual o agente pode fazer até 4 ações possíveis (ir ao Norte, Sul, Leste ou Oeste).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "| S | F | F | F |\n",
    "| F | H | F | H |\n",
    "| F | F | F | H |\n",
    "| H | F | F | G |\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para escrever um agente para este jogo usando a estratégia descrita, vamos usar o pacote _gym_, fornecido pela OpenAI (https://gym.openai.com/). Este pacote permite que escrevamos uma política de aprendizado para uma grande variedade de jogos, como por exemplo, os do console Atari. O pacote fornece uma abstração para o jogo que facilita a obtenção do seu estado atual, execução de uma ação e retorno de um _feedback_, na forma de uma recompensa ou punição. Entre os jogos suportados, temos _frozenlake_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: FrozenLake-v0\n",
      "[2017-02-28 12:08:38,733] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "# lê ambiente para jogo\n",
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A estrutura básica de um agente em um ambiente fornecido pelo gym consiste em:\n",
    "* iniciar o ambiente\n",
    "* para cada passo do jogo até ele ser concluído\n",
    "    * atualize a animação do jogo, se for o caso\n",
    "    * decida por uma ação\n",
    "    * execute a ação recebendo feedback e informação sobre novo estado\n",
    "* finalizar o ambiente\n",
    "\n",
    "Abaixo, temos um pequeno agente exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: FrozenLake-v0\n",
      "[2017-02-28 13:29:11,927] Making new env: FrozenLake-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "# obtenha uma observação do ambiente (estado)\n",
    "observation = env.reset()\n",
    "# excute ateh, no maximo, 10 passos do jogo\n",
    "for i in range(10):\n",
    "    # anime o jogo\n",
    "    env.render()\n",
    "    # execute uma ação aleatória\n",
    "    action = env.action_space.sample()  \n",
    "    # obtenha novo estado, recompensa, indicação de fim de jogo e\n",
    "    # informacao adicional do ambiente, usada principalmente para debug\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        env.render()\n",
    "        print 'reward:', reward\n",
    "        env.reset()\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para maiores detalhes sobre gym, consulte https://gym.openai.com/docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementando qLeaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora precisamos codificar a estratégia explicada no contexto do gradiente ascendente. Considerando que vamos sair da utilidade $Q(s, a)$ para a nova utilidade $r + \\gamma(\\max Q(s',a'))$, a derivada pode ser aproximada pela diferença imediata:\n",
    "\n",
    "$$Q(s,a)' \\approx (r + \\gamma(\\max Q(s',a'))) - Q(s, a)$$\n",
    "\n",
    "Assim, o novo $Q(s, a)$ pode ser atualizado como:\n",
    "\n",
    "$$Q(s, a) = Q(s, a) + \\lambda Q(s, a)'$$\n",
    "\n",
    "onde $\\lambda$ é a taxa de aprendizado. Segue, abaixo, o código para todo o processo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 0.0\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "episode 500 0.0\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "episode 1000 1.0\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "  (Right)\n",
      "episode 1500 0.0\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "episode 2000 0.0\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "episode 2500 0.0\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "episode 3000 0.0\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "episode 3500 1.0\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "  (Right)\n",
      "episode 4000 0.0\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "episode 4500 1.0\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "  (Right)\n"
     ]
    }
   ],
   "source": [
    "# inicia tabela Q com zeros\n",
    "# action = left, down, right, up\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# parametros \n",
    "lr = .85 # tx de aprendizado\n",
    "y = .99 # tx de importancia\n",
    "num_episodes = 5000\n",
    "\n",
    "# lista com premios e passos por episodio\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    # reseta ambiente e pega nova obs\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    # algortimo de aprendizado com tabela Q\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        # escolhe uma acao (de forma gulosa e com ruido descrescente \n",
    "        # ao longo do aprendizado) da tabela Q\n",
    "        a = np.argmax(Q[s,:] + \n",
    "                      np.random.randn(1, env.action_space.n) * (1./(i+1)))\n",
    "        # obtem novo estado, premio do ambiente e indicação se terminou\n",
    "        s1, r, done, _ = env.step(a)\n",
    "        # atualiza tab Q com novo conhecimento\n",
    "        Q[s,a] = Q[s,a] + lr * (r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if done == True:\n",
    "            break\n",
    "    if i % 500 == 0: \n",
    "        print 'episode', i, r\n",
    "        env.render()\n",
    "    rList.append(rAll)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.4362\n"
     ]
    }
   ],
   "source": [
    "print \"Score over time: \" +  str(sum(rList)/num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcocristo/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10986e810>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNHKG4P+ETtQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10687d450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(rList[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como pode ser visto, o agente aprendeu uma política eficaz rapidamente. Ou seja, rapidamente, ele aprendeu a evitar buracos. Eventualmente, ele ainda cai num buraco, o que é esperado, dado o vento. Abaixo, temos a tabela de utilidades aprendida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q-Table Values\n",
      "[[  6.41865546e-03   5.67664758e-03   5.50838811e-01   1.00540401e-02]\n",
      " [  1.97135783e-04   3.21783341e-03   1.80903204e-03   2.98748796e-01]\n",
      " [  4.63811080e-03   3.18378260e-03   8.89674019e-03   2.12637704e-01]\n",
      " [  2.03620225e-03   0.00000000e+00   1.06105536e-03   2.06573820e-01]\n",
      " [  6.75201996e-01   1.80894421e-03   2.32614665e-03   2.71072992e-03]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  1.83195294e-01   1.67864777e-06   8.72315379e-05   8.59638190e-05]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  1.51490467e-03   4.05165415e-04   4.08751283e-04   7.71872793e-01]\n",
      " [  1.03190844e-03   9.33249217e-01   0.00000000e+00   4.16604760e-04]\n",
      " [  1.07266773e-02   1.24396499e-04   4.31229934e-04   1.79669184e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  1.15490583e-03   2.86458834e-03   4.92783707e-01   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   1.11213792e-01   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print \"Final Q-Table Values\"\n",
    "print Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendendo Q com Redes Neurais "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O problema com tabelas Q é que elas não escalam para qualquer problema minimamente realista. Assim, vamos usar redes neurais. Dada a entrada que corresponde a um estado, esperamos que a rede aprenda os valores Q!!!\n",
    "\n",
    "__Idéia__: codificar a entrada como um hot-vector com 16 neurônios. A saída com 4 neurônios, um para cada ação possível. A função de perda é o quadrado da diferença entre os valores Q aprendidos até o momento (já considerando o efeito da última recompensa, o que seria o \"ground truth\" mais atual) e os valores Q estimados pela rede. _Leia isso de novo... É isso mesmo :P_\n",
    "\n",
    "Note que frozenlake é um jogo muito simples, com apenas $4 * 16 = 64$ estados. A rede proposta tem $4 * 16$ pesos $+ 4$ biases = 68 parâmetros para aprender. Obviamente, ela não é menor que a própria tabela Q. Contudo, para um problema mais realista, com um número muito maior de estados, a tabela Q deve ser muito maior que a rede neural. Por exemplo, suponha um jogo de videogame com resolução 420x280 = 117600 pixels e 16 cores. Este jogo tem $16^{117600}$ estados possíveis. O espaço de estados inteiro poderia ser representado por uma rede neural de cerca de alguns milhões de parâmetros, o que é muito menos que $16^{117600}$. Logo, a rede de fato deve aprender uma versão muito comprimida do espaço de estados. Em outras palavras, ela vai aprender a generalizar os estados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: FrozenLake-v0\n",
      "[2017-02-28 12:09:18,592] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# implementacao em tensor flow\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A rede\n",
    "inputs1 = tf.placeholder(shape=[1,16], dtype=tf.float32)\n",
    "W = tf.Variable(tf.random_uniform([16,4], 0, 0.01))\n",
    "Qout = tf.matmul(inputs1, W)\n",
    "predict = tf.argmax(Qout, 1)\n",
    "\n",
    "# função de perda\n",
    "nextQ = tf.placeholder(shape=[1,4], dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = trainer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O algoritmo que vamos usar pode ser descrito assim:\n",
    "* Para cada episódio:\n",
    "    * Para cada novimento do agente e enquanto não for o fim do jogo:\n",
    "        * Use rede neural para prever utilidade _Q_ de cada ação _a_ em _s_\n",
    "        * Defina como próxima ação _a_ aquela com o maior _Q_ previsto ou uma aleatória, se preferir explorar\n",
    "        * Execute nova _a_ no ambiente, obtendo recompensa _r_ e novo estado _s'_\n",
    "        * Utilize rede neural para obter utilidades _Q'_ para estado _s'_\n",
    "        * Atualize _Q_ correspondente a _a_ com a informação de _Q'_ e a recompensa _r_ obtida\n",
    "        * Treine a rede considerando o novo valor de _Q_ como a utilidade _mais_ correta para o estado _s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of succesful episodes: 0.4545%\n"
     ]
    }
   ],
   "source": [
    "# o treino\n",
    "# inicie parametros\n",
    "#init = tf.global_variables_initializer()\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# parâmetros de aprendizado\n",
    "y = .99                # importance rate  \n",
    "e = 0.1                # exploration rate\n",
    "num_episodes = 2000\n",
    "\n",
    "# passos por episódios (mais passo, mais tempo vivo!)\n",
    "jList = [] \n",
    "rList = [] # rewards\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        # reseta ambiente e pega nova observacao\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        # Q-Network\n",
    "        while j < 99:\n",
    "            j+=1\n",
    "            # escolha uma acao de forma gulosa da rede Q\n",
    "            # a acao eh escolhida com uma certa aleatoriedade! \n",
    "            # (dada por e)\n",
    "            get_state_as_hot_vector = lambda s: np.identity(16)[s:s+1]\n",
    "            a, allQ = sess.run([predict, Qout],\n",
    "                               feed_dict = {inputs1:get_state_as_hot_vector(s)})\n",
    "            # define aleatoriamente se permanece com sugestão da rede ou...\n",
    "            if np.random.rand(1) < e:\n",
    "                # uma ação tomada aleatoriament (exploração)\n",
    "                a[0] = env.action_space.sample()\n",
    "            # obtenha novo estado e recompensa do ambiente\n",
    "            s1,r,d,_ = env.step(a[0])\n",
    "            # obtenha os valores Q passando o novo estado pra rede\n",
    "            Q1 = sess.run(Qout, feed_dict={inputs1:get_state_as_hot_vector(s1)})\n",
    "            # obtenha maxQ' e defina o valor alvo como a acao escolhida.\n",
    "            maxQ1 = np.max(Q1)\n",
    "            targetQ = allQ\n",
    "            # o pulo do gato! note que a estimativa feita é atualizada com a \n",
    "            # recompensa vinda do ambiente!!!\n",
    "            targetQ[0,a[0]] = r + y*maxQ1 \n",
    "            # treine a rede usando o alvo e o valor Q previsto\n",
    "            _, W1 = sess.run([updateModel, W],\n",
    "                             feed_dict={inputs1:get_state_as_hot_vector(s), nextQ:targetQ})\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                # reduza a chance de acoes aleatorias aa medida que o modelo eh treinado.\n",
    "                e = 1./((i/50) + 10)\n",
    "                break\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "print \"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver como a rede evoluiu analisando as listas de prêmios recebidos e passos dados por episódio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x111e26d50>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1065eb9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que uma hora a rede aprende e alcança resultados consistentes. Abaixo temos o número de passos dado em cada episódio do jogo. Observe que aos poucos, o agente permanece vivo por um tempo cada vez maior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x111eb5b50>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109882990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(jList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNs são capazes de aprender, mas não de forma tão eficiente quanto Tabelas Q. No gráfico é possível notar a instabilidade, mesmo após quase 2000 sessões de treino. Para serem viáveis em problemas realmente grandes, precisamos aprender muitas heurísticas ainda. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ação -> Recompensa: os caça níqueis de _n_ braços "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em aprendizado por reforço (AR), os problemas tem 3 características:\n",
    "\n",
    "1. diferentes ações levam a diferentes recompensas\n",
    "2. recompensas podem vir bem depois de uma ação\n",
    "3. recompensas dependem do estado do ambiente\n",
    "\n",
    "Nesta seção vamos lidar com um problema bem simples de AR, o dos caça níqueis com _n_ braços. Neste problema, apenas o item 1 acima está presente. Em geral, ele é descrito assim: \n",
    "\n",
    "Imagine que há uma máquina de caça níqueis com _n_ braços, cada qual com uma probabilidade de retorno diferente. Neste jogo, a meta é descobrir o braço com melhor retorno e maximizar o ganho do agente por sempre escolher aquele braço. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Os bandidos (braços), neste caso 4.\n",
    "# No início, bandits[3] tem o menor valor e, portanto, tem maior probabilidade \n",
    "# de dar um retorno positivo. \n",
    "bandits = [0.2, 0, -0.2, -5] # neste caso, é o bandits[3]\n",
    "num_bandits = len(bandits)\n",
    "def pullBandit(bandit_val):\n",
    "    # pegue um numero aleatorio de dist normal com media = 0\n",
    "    result = np.random.randn(1)\n",
    "    if result > bandit_val:\n",
    "        # retorne recompensa positiva.\n",
    "        return 1\n",
    "    else:\n",
    "        # retorne recompensa negativa.\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Agente__: Rede neural (cof, cof!) com um valor para cada bandido. Cada valor é uma estimativa do retorno de escolher aquele bandido. Política de aprendizado é feita pelo GD que atualiza o agente mudando o valor para a ação selecionada na direção da recompensa recebida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# agente\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# feed-forward NN. Faz as escolhas.\n",
    "weights = tf.Variable(tf.ones([num_bandits]))\n",
    "chosen_action = tf.argmax(weights, 0)\n",
    "\n",
    "# procedimento de treino. \n",
    "# Rede eh alimentada com reward e acao escolhida para calculo da perda\n",
    "# Perda eh usada para atualizar pesos.\n",
    "reward_holder = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "action_holder = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "responsible_weight = tf.slice(weights, action_holder, [1])\n",
    "loss = -(tf.log(responsible_weight) * reward_holder)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards after 49 episodes: [ -1.  -1.  -1.  22.]\n",
      "Rewards after 99 episodes: [ -1.   1.  -1.  68.]\n",
      "Rewards after 149 episodes: [   0.    0.   -1.  114.]\n",
      "Rewards after 199 episodes: [   2.    0.    1.  160.]\n",
      "Rewards after 249 episodes: [   2.   -2.    3.  200.]\n",
      "Rewards after 299 episodes: [   3.   -4.    2.  246.]\n",
      "Rewards after 349 episodes: [   4.   -5.    2.  294.]\n",
      "Rewards after 399 episodes: [   5.   -5.    2.  341.]\n",
      "Rewards after 449 episodes: [   5.   -4.    1.  385.]\n",
      "Rewards after 499 episodes: [   5.   -4.    4.  432.]\n",
      "Rewards after 549 episodes: [   4.   -4.    5.  480.]\n",
      "Rewards after 599 episodes: [   5.   -4.    6.  528.]\n",
      "Rewards after 649 episodes: [   8.   -6.    7.  570.]\n",
      "Rewards after 699 episodes: [   8.   -7.    7.  619.]\n",
      "Rewards after 749 episodes: [   8.   -7.    7.  667.]\n",
      "Rewards after 799 episodes: [   9.   -7.    6.  715.]\n",
      "Rewards after 849 episodes: [   9.   -7.    6.  763.]\n",
      "Rewards after 899 episodes: [  11.   -7.    7.  810.]\n",
      "Rewards after 949 episodes: [   9.   -8.    7.  857.]\n",
      "The agent thinks bandit 4 is the most promising....\n",
      "...and it was right!\n"
     ]
    }
   ],
   "source": [
    "total_episodes = 1000 # nro de episodios.\n",
    "total_reward = np.zeros(num_bandits) # scores dos bandidos.\n",
    "e = 0.1 # chance de ação aleatória.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        \n",
    "        # escolha ação aleatória ou da rede.\n",
    "        if np.random.rand(1) < e:\n",
    "            action = np.random.randint(num_bandits)\n",
    "        else:\n",
    "            action = sess.run(chosen_action)\n",
    "        \n",
    "        # obtenha recompensa pela escolha feita.\n",
    "        reward = pullBandit(bandits[action]) \n",
    "        \n",
    "        # atualize rede.\n",
    "        _, resp, ww = sess.run([update, responsible_weight, weights], \n",
    "                               feed_dict={reward_holder:[reward], action_holder:[action]})\n",
    "        \n",
    "        # atualize escores.\n",
    "        total_reward[action] += reward\n",
    "        if i % 50 == 0 and i > 0:\n",
    "            print \"Rewards after \" + str(i-1) + \" episodes: \" + str(total_reward)\n",
    "        i+=1\n",
    "print \"The agent thinks bandit \" + str(np.argmax(ww)+1) + \" is the most promising....\"\n",
    "if np.argmax(ww) == np.argmax(-np.array(bandits)):\n",
    "    print \"...and it was right!\"\n",
    "else:\n",
    "    print \"...and it was wrong!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que sem estados, a melhor ação em qualquer momento, é a melhor ação sempre! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estado/Ação -> Recompensa: o _n_ caça níqueis! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta seção, vamos considerar problemas de AR onde há estados, mas estes não dependem de estados prévios e ações. Vamos usar para isso o problema dos _n_ caça níqueis.\n",
    "\n",
    "Nesse problema, temos _n_ caça níqueis (estados) com _m_ braços (ações). O objetivo é aprender a melhor ação para qualquer estado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class contextual_bandit():\n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        # os bandidos. Atualmente, braços 4, 2 e 1 são os melhores.\n",
    "        self.bandits = np.array([[0.2,0,-0.0,-5], [0.1,-5,1,0.25], [-5,5,5,5]])\n",
    "        self.num_bandits = self.bandits.shape[0]\n",
    "        self.num_actions = self.bandits.shape[1]\n",
    "        \n",
    "    def getBandit(self):\n",
    "        # retorna um estado aleatório para cada episódio\n",
    "        self.state = np.random.randint(0,len(self.bandits)) \n",
    "        return self.state\n",
    "        \n",
    "    def pullArm(self, action):\n",
    "        bandit_val = self.bandits[self.state, action]\n",
    "        result = np.random.randn(1)\n",
    "        if result > bandit_val:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self, lr, s_size, a_size):\n",
    "        # NN. estado -> acao.\n",
    "        self.state_in= tf.placeholder(shape=[1],dtype=tf.int32)\n",
    "        state_in_OH = slim.one_hot_encoding(self.state_in,s_size)\n",
    "        output = slim.fully_connected(state_in_OH,\n",
    "                                      a_size,\n",
    "                                      biases_initializer=None,\n",
    "                                      activation_fn=tf.nn.sigmoid,\n",
    "                                      weights_initializer=tf.ones_initializer())\n",
    "        self.output = tf.reshape(output,[-1])\n",
    "        self.chosen_action = tf.argmax(self.output, 0)\n",
    "\n",
    "        # Treino. acao, reward -> NN -> loss -> atualizacao e pesos.\n",
    "        self.reward_holder = tf.placeholder(shape=[1],dtype=tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape=[1],dtype=tf.int32)\n",
    "        self.responsible_weight = tf.slice(self.output, self.action_holder, [1])\n",
    "        self.loss = -(tf.log(self.responsible_weight) * self.reward_holder)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "        self.update = optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward for each of the 3 bandits: [ 0.    0.25  0.  ]\n",
      "Mean reward for each of the 3 bandits: [ 24.75  31.75  38.25]\n",
      "Mean reward for each of the 3 bandits: [ 72.25  71.5   64.5 ]\n",
      "Mean reward for each of the 3 bandits: [ 109.5   108.25  102.  ]\n",
      "Mean reward for each of the 3 bandits: [ 147.5   148.75  135.  ]\n",
      "Mean reward for each of the 3 bandits: [ 188.5   188.75  170.  ]\n",
      "Mean reward for each of the 3 bandits: [ 229.25  224.    207.  ]\n",
      "Mean reward for each of the 3 bandits: [ 265.75  263.    244.  ]\n",
      "Mean reward for each of the 3 bandits: [ 307.25  300.75  279.75]\n",
      "Mean reward for each of the 3 bandits: [ 348.25  339.75  315.75]\n",
      "Mean reward for each of the 3 bandits: [ 384.75  380.    349.5 ]\n",
      "Mean reward for each of the 3 bandits: [ 420.25  422.25  385.25]\n",
      "Mean reward for each of the 3 bandits: [ 456.    465.5   420.75]\n",
      "Mean reward for each of the 3 bandits: [ 496.25  505.5   451.5 ]\n",
      "Mean reward for each of the 3 bandits: [ 534.75  541.    491.  ]\n",
      "Mean reward for each of the 3 bandits: [ 573.25  581.5   523.  ]\n",
      "Mean reward for each of the 3 bandits: [ 612.    620.75  554.5 ]\n",
      "Mean reward for each of the 3 bandits: [ 646.75  658.75  593.75]\n",
      "Mean reward for each of the 3 bandits: [ 685.75  694.5   631.5 ]\n",
      "Mean reward for each of the 3 bandits: [ 723.75  732.    663.5 ]\n",
      "The agent thinks action 4 for bandit 1 is the most promising....\n",
      "...and it was right!\n",
      "The agent thinks action 2 for bandit 2 is the most promising....\n",
      "...and it was right!\n",
      "The agent thinks action 1 for bandit 3 is the most promising....\n",
      "...and it was right!\n"
     ]
    }
   ],
   "source": [
    "# treinamento\n",
    "tf.reset_default_graph() \n",
    "\n",
    "cBandit = contextual_bandit() # bandidos\n",
    "myAgent = agent(lr=0.001, s_size=cBandit.num_bandits, a_size=cBandit.num_actions) # agente\n",
    "weights = tf.trainable_variables()[0] # pesos a serem avaliados olhando pra rede\n",
    "\n",
    "total_episodes = 10000 # episodios.\n",
    "total_reward = np.zeros([cBandit.num_bandits, cBandit.num_actions]) # scores das máquinas.\n",
    "e = 0.1 # chance de escolher acao aleatoria\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        s = cBandit.getBandit() # obter estado\n",
    "        \n",
    "        # agir aleatoriamente ou de acordo com a rede\n",
    "        if np.random.rand(1) < e:\n",
    "            action = np.random.randint(cBandit.num_actions)\n",
    "        else:\n",
    "            action = sess.run(myAgent.chosen_action, feed_dict={myAgent.state_in:[s]})\n",
    "        \n",
    "        reward = cBandit.pullArm(action) # obtem recompensa por acao\n",
    "        \n",
    "        # atualize NN\n",
    "        feed_dict = {myAgent.reward_holder:[reward],\n",
    "                     myAgent.action_holder:[action],\n",
    "                     myAgent.state_in:[s]}\n",
    "        _,ww = sess.run([myAgent.update, weights], feed_dict=feed_dict)\n",
    "        \n",
    "        # atualize escores.\n",
    "        total_reward[s,action] += reward\n",
    "        if i % 500 == 0:\n",
    "            print \"Mean reward for each of the \" + str(cBandit.num_bandits) \\\n",
    "                + \" bandits: \" + str(np.mean(total_reward,axis=1))\n",
    "        i+=1\n",
    "        \n",
    "for a in range(cBandit.num_bandits):\n",
    "    print \"The agent thinks action \" + str(np.argmax(ww[a])+1) + \" for bandit \" \\\n",
    "        + str(a+1) + \" is the most promising....\"\n",
    "    if np.argmax(ww[a]) == np.argmin(cBandit.bandits[a]):\n",
    "        print \"...and it was right!\"\n",
    "    else:\n",
    "        print \"...and it was wrong!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Estado <-> Ação) -> Recompensa: equilibrando um lápis! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste jogo, o objetivo é equilibrar um lápis, de ponta a cabeça, na palma da mão, apenas movendo a mão para esquerda ou direita. Agora nosso agente deve ser capaz de observar o mundo (o lápis) e contar com recompensas que tardam a aparecer (vários movimentos antes do lápis cair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/marco/Canopy/appdata/canopy-1.7.3.3335.rh5-x86_64/lib/python2.7/logging/__init__.py\", line 874, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file registration.py, line 90\n",
      "[2017-01-06 17:45:12,566] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que acontece se eu tentar equilibrar fazendo movimentos aleatórios? Nesta versão do jogo, do gym, para cada movimento que o agente mantém o lápis no ar, ele ganha um ponto. O objetivo é ganhar ao menos 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for this episode was: 12.0\n",
      "Reward for this episode was: 17.0\n",
      "Reward for this episode was: 15.0\n",
      "Reward for this episode was: 51.0\n",
      "Reward for this episode was: 20.0\n",
      "Reward for this episode was: 16.0\n",
      "Reward for this episode was: 15.0\n",
      "Reward for this episode was: 19.0\n",
      "Reward for this episode was: 45.0\n",
      "Reward for this episode was: 26.0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "random_episodes = 0\n",
    "reward_sum = 0\n",
    "while random_episodes < 10:\n",
    "    env.render()\n",
    "    observation, reward, done, _ = env.step(np.random.randint(0,2))\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        random_episodes += 1\n",
    "        print \"Reward for this episode was:\",reward_sum\n",
    "        reward_sum = 0\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos o desempenho não é muito alentador. Vamos tentar com uma rede neural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "H = 10 # num de neuronios na camada escondida\n",
    "batch_size = 5 # quantos episodios antes de atualizar os pesos?\n",
    "learning_rate = 1e-2 # tx de aprendizado.\n",
    "gamma = 0.99 # fator de desconto da recompensa \n",
    "\n",
    "D = 4 # dimensionalidade da entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Rede deste o ponto em que eh feita a observacao do ambiente \n",
    "# ate o fornecimento da probabiliade de escolher a acao: esq ou dir.\n",
    "observations = tf.placeholder(tf.float32, [None,D] , name=\"input_x\")\n",
    "W1 = tf.get_variable(\"W1\", shape=[D, H],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(observations,W1))\n",
    "W2 = tf.get_variable(\"W2\", shape=[H, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "score = tf.matmul(layer1,W2)\n",
    "probability = tf.nn.sigmoid(score)\n",
    "\n",
    "# parte da rede que aprende politica de acao\n",
    "tvars = tf.trainable_variables()\n",
    "input_y = tf.placeholder(tf.float32,[None,1], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32,name=\"reward_signal\")\n",
    "\n",
    "# Funcao de perda: muda pesos na direcao de oferecer acoes mais vantajosas \n",
    "# (recompensa no tempo) com maior probabilidade, e ações menos vantajosas menos provavelmente.\n",
    "loglik = tf.log(input_y*(input_y - probability) + (1 - input_y)*(input_y + probability))\n",
    "loss = -tf.reduce_mean(loglik * advantages) \n",
    "newGrads = tf.gradients(loss,tvars)\n",
    "\n",
    "# Após colecionar gradientes de muitos episódios, eles são aplicados.\n",
    "# Note que os gradientes não são aplicados após cada episódio de forma a lidar\n",
    "# com ruído no sinal de recompensa.\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate) \n",
    "W1Grad = tf.placeholder(tf.float32,name=\"batch_grad1\") \n",
    "W2Grad = tf.placeholder(tf.float32,name=\"batch_grad2\")\n",
    "batchGrad = [W1Grad,W2Grad]\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad,tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Função de Descontos**: Esta função nos permite ponderar as recompensas recebidas. No contexto do jogo de equilíbrio, o ideal é que ações que mantem o lápis no ar por muito tempo ganhem um prêmio maior e ações que contribuem para a queda do lápis tenham  recompensa negativa ou decrementada. Assim, as ações perto do fim do episódio são ponderadas para menos uma vez que são vistas como mais negativas. As ações no início, ao contrário, são vistas como mais positivas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    \"\"\" calcule desconto sobre 1D-vetor (float) das recompensas \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "total_episodes = 10000\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    rendering = False\n",
    "    sess.run(init)\n",
    "    observation = env.reset() # Obtain an initial observation of the environment\n",
    "\n",
    "    # Reset the gradient placeholder. We will collect gradients in \n",
    "    # gradBuffer until we are ready to update our policy network. \n",
    "    gradBuffer = sess.run(tvars)\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    \n",
    "    while episode_number <= total_episodes:\n",
    "        \n",
    "        # Rendering the environment slows things down, \n",
    "        # so let's only look at it once our agent is doing a good job.\n",
    "        if reward_sum/batch_size > 100 or rendering == True : \n",
    "            env.render()\n",
    "            rendering = True\n",
    "            \n",
    "        # Make sure the observation is in a shape the network can handle.\n",
    "        x = np.reshape(observation,[1,D])\n",
    "        \n",
    "        # Run the policy network and get an action to take. \n",
    "        tfprob = sess.run(probability,feed_dict={observations: x})\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "        \n",
    "        xs.append(x) # observation\n",
    "        y = 1 if action == 0 else 0 # a \"fake label\"\n",
    "        ys.append(y)\n",
    "\n",
    "        # step the environment and get new measurements\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "\n",
    "        drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "        if done: \n",
    "            episode_number += 1\n",
    "            # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            tfp = tfps\n",
    "            xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[] # reset array memory\n",
    "\n",
    "            # compute the discounted reward backwards through time\n",
    "            discounted_epr = discount_rewards(epr)\n",
    "            # size the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "            discounted_epr -= np.mean(discounted_epr)\n",
    "            discounted_epr /= np.std(discounted_epr)\n",
    "            \n",
    "            # Get the gradient for this episode, and save it in the gradBuffer\n",
    "            tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n",
    "            for ix,grad in enumerate(tGrad):\n",
    "                gradBuffer[ix] += grad\n",
    "                \n",
    "            # If we have completed enough episodes, then update the policy network with our gradients.\n",
    "            if episode_number % batch_size == 0: \n",
    "                sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W2Grad:gradBuffer[1]})\n",
    "                for ix,grad in enumerate(gradBuffer):\n",
    "                    gradBuffer[ix] = grad * 0\n",
    "                \n",
    "                # Give a summary of how well our network is doing for each batch of episodes.\n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "                print 'Average reward for episode %f.  Total average reward %f.' % (reward_sum/batch_size, running_reward/batch_size)\n",
    "                \n",
    "                if reward_sum/batch_size > 200: \n",
    "                    print \"Task solved in\",episode_number,'episodes!'\n",
    "                    break\n",
    "                    \n",
    "                reward_sum = 0\n",
    "            \n",
    "            observation = env.reset()\n",
    "        \n",
    "print episode_number,'Episodes completed.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Este curso é baseado em material disponível em https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0#.tflji3pdd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
