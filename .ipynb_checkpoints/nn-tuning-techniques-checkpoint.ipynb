{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melhorando o modelo\n",
    "\n",
    "Nesta seção vamos ver como refinar os parâmetros do osso modelo, de forma que ele atinja melhor generalização. Note que as técnicas que vamos ver aqui dão resultados mais visiveis com problemas mais complexos que nmist. Contudo, vamos focar no nmist para manter os experimentos em tempos razoáveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nosso baseline: rede convolutiva em Keras\n",
    "\n",
    "Tipicamente, o projeto de uma rede neural começa com projetos que já foram mostrados bem sucedidos em problemas similares e com parâmetros que, em geral, funcionam bem. A ideia é obter um desempenho razoável para um baseline e, então, melhorá-lo até conseguir o máximo possível.\n",
    "\n",
    "Nós vamos começar com a arquitetura abaixo, implementada com o Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49500 samples, validate on 5500 samples\n",
      "Epoch 1/12\n",
      "49500/49500 [==============================] - 2s - loss: 0.6897 - acc: 0.7783 - val_loss: 0.1545 - val_acc: 0.9585\n",
      "Epoch 2/12\n",
      "49500/49500 [==============================] - 2s - loss: 0.2747 - acc: 0.9169 - val_loss: 0.1050 - val_acc: 0.9707\n",
      "Epoch 3/12\n",
      "49500/49500 [==============================] - 2s - loss: 0.2078 - acc: 0.9381 - val_loss: 0.0865 - val_acc: 0.9762\n",
      "Epoch 4/12\n",
      "49500/49500 [==============================] - 2s - loss: 0.1700 - acc: 0.9489 - val_loss: 0.0731 - val_acc: 0.9789\n",
      "Epoch 5/12\n",
      "49500/49500 [==============================] - 2s - loss: 0.1513 - acc: 0.9543 - val_loss: 0.0675 - val_acc: 0.9798\n",
      "Epoch 6/12\n",
      "49500/49500 [==============================] - 2s - loss: 0.1345 - acc: 0.9604 - val_loss: 0.0617 - val_acc: 0.9824\n",
      "Epoch 7/12\n",
      "49500/49500 [==============================] - 2s - loss: 0.1234 - acc: 0.9628 - val_loss: 0.0599 - val_acc: 0.9835\n",
      "Epoch 8/12\n",
      "49500/49500 [==============================] - 2s - loss: 0.1124 - acc: 0.9661 - val_loss: 0.0555 - val_acc: 0.9855\n",
      "Epoch 9/12\n",
      "49500/49500 [==============================] - 1s - loss: 0.1052 - acc: 0.9679 - val_loss: 0.0535 - val_acc: 0.9847\n",
      "Epoch 10/12\n",
      "49500/49500 [==============================] - 1s - loss: 0.0985 - acc: 0.9700 - val_loss: 0.0523 - val_acc: 0.9860\n",
      "Epoch 11/12\n",
      "49500/49500 [==============================] - 1s - loss: 0.0943 - acc: 0.9711 - val_loss: 0.0514 - val_acc: 0.9860\n",
      "Epoch 12/12\n",
      "49500/49500 [==============================] - 2s - loss: 0.0904 - acc: 0.9726 - val_loss: 0.0490 - val_acc: 0.9871\n",
      " 9920/10000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.043455511234607551, 0.98499999999999999]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic class for specifying and training a neural network\n",
    "from keras.models import Model \n",
    "from keras.layers import Input, Dense, Flatten, \\\n",
    "    Convolution2D, MaxPooling2D, Dropout\n",
    "\n",
    "batch_size = 128 # 128 training examples at once per iter\n",
    "num_epochs = 12 # we iterate twelve times over the entire training set\n",
    "kernel_size = 3 # we will use 3x3 kernels throughout\n",
    "pool_size = 2 # we will use 2x2 pooling throughout\n",
    "conv_depth = 32 # use 32 kernels in both convolutional layers\n",
    "drop_prob_1 = 0.25 # dropout after pooling with probability 0.25\n",
    "drop_prob_2 = 0.5 # dropout in the FC layer with probability 0.5\n",
    "hidden_size = 128 # there will be 128 neurons in both hidden layers\n",
    "\n",
    "num_train = 55000 # there are 55000 training examples in MNIST\n",
    "num_test = 10000 # there are 10000 test examples in MNIST\n",
    "\n",
    "height, width, depth = 28, 28, 1 # MNIST images are 28x28 and greyscale\n",
    "num_classes = 10 # there are 10 classes (1 per digit)\n",
    "\n",
    "# fetch MNIST data\n",
    "X_train = mnist.train.images\n",
    "Y_train = mnist.train.labels\n",
    "X_test  = mnist.test.images\n",
    "Y_test  = mnist.test.labels\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], depth, height, width)\n",
    "X_test = X_test.reshape(X_test.shape[0], depth, height, width)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Using theano ordering -- channel dimension first\n",
    "inp = Input(shape=(depth, height, width)) \n",
    "# Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer)\n",
    "conv_1 = Convolution2D(conv_depth, kernel_size, kernel_size, \n",
    "                       border_mode='same', activation='relu')(inp)\n",
    "conv_2 = Convolution2D(conv_depth, kernel_size, kernel_size, \n",
    "                       border_mode='same', activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size), \n",
    "                      dim_ordering=\"th\")(conv_2)\n",
    "drop_1 = Dropout(drop_prob_1)(pool_1)\n",
    "flat = Flatten()(drop_1)\n",
    "# Hidden ReLU layer\n",
    "hidden = Dense(hidden_size, activation='relu')(flat) \n",
    "drop = Dropout(drop_prob_2)(hidden)\n",
    "# Output softmax layer\n",
    "out = Dense(num_classes, activation='softmax')(drop) \n",
    "\n",
    "# To define a model, just specify its input and output layers\n",
    "model = Model(input=inp, output=out) \n",
    "\n",
    "# using the cross-entropy loss function\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', # using the Adam optimiser\n",
    "              metrics=['accuracy']) # reporting the accuracy\n",
    "\n",
    "# Train the model using the training set...\n",
    "# ...holding out 10% of the data for validation\n",
    "model.fit(X_train, Y_train, \n",
    "          batch_size=batch_size, nb_epoch=num_epochs,\n",
    "          verbose=1, validation_split=0.1)\n",
    "# Evaluate the trained model on the test set!\n",
    "model.evaluate(X_test, Y_test, verbose=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, esse modelo atingiu um desempenho próximo a 99%. Nós vamos ver agora várias estratégias que, em geral, são usadas para melhorar este resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regularização $L_2$\n",
    "\n",
    "Na tentativa de reduzir o erro no treino, o método pode sofrer de **overfitting**, como ilustrado na figura a seguir. \n",
    "\n",
    "![](images/plotsin.png)\n",
    "\n",
    "Anteriormente, vimos como usar *dropout* para minimizar overfitting. Entre vários outros regularizadores, um muito usado é o $L_2$ ou _decaimento de peso_.\n",
    "\n",
    "A ideia do $L_2$ é tirar complexidade do modelo ao penalizar pesos que atingem valores muito altos. Para tanto, ele minimiza a norma $L_2$ do peso usando um hiperparâmetro $\\lambda$ que especifica a importância da minimização da norma.\n",
    "\n",
    "Para introduzir esse regularizador, nós adicionamos à função de custo o fator $\\frac{\\lambda}{2}||\\vec{w}||^2 = \\frac{\\lambda}{2}\\sum_{i=0}^{W} {w_i^2}$ (o valor $1/2$ é usado apenas para facilitar o cálculo da derivada durante as atualizações do backpropagation). A função passa a ser algo como $\\mathcal{L}(\\vec{\\hat{y}}, \\vec{y}) + \\frac{\\lambda}{2}\\sum_{i=0}^{W} {w_i^2}$ (em nosso caso, $\\mathcal{L}$ corresponde à entropia cruzada). \n",
    "\n",
    "O valor de $\\lambda$ é importante. Para valores muito baixos, o regularizador não opera; para muitos altos, o modelo ótimo irá ter todos os pesos = 0. Vamos usar $\\lambda = 0.0001$ em nosso caso; adicionamos ele ao modelo importanto l2 e setando o parâmetro `W_regularizer` na camda desejada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import l2 # L2-regularisation\n",
    "# ...\n",
    "l2_lambda = 0.0001\n",
    "# ...\n",
    "# This is how to add L2-regularisation to any Keras \n",
    "# layer with weights (e.g. Convolution2D/Dense)\n",
    "conv_1 = Convolution2D(conv_depth, kernel_size, \n",
    "                       kernel_size, border_mode='same', \n",
    "                       W_regularizer=l2(l2_lambda), activation='relu')(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialização da rede\n",
    "\n",
    "Um dos aspectos mais importantes de aprendizagem profunda, em geral, são os valores iniciais dos pesos. Às vezes, a escolha apropriada de pesos é a diferença entre desempenho magnífico e nem conseguir convergir. De fato, se iniciarmos os pesos todos com zero, não há aprendizado já que provalvelmente nenhum peso vai ficar ativo. Inicialização uniforme entre $\\pm 1$ também não é normalmente o melhor a fazer.  \n",
    "\n",
    "Entre os esquemas existentes, vamos discutir dois muito usados:\n",
    "- [*Xavier*](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf) (também chamado *Glorot*): A ideia chave aqui é facilitar a passagem do sinal pela camada tanto na ida quanto na volta, **quando a ativação for linear** (este método funciona bem também para *sigmoid* porque o intervalo em que ela é *não saturada* é quase linear). Os pesos são obtidos de uma distribuição de probabilidade (uniforme ou normal) com variância igual a: $\\mathrm{Var}(W) = \\frac{2}{n_{\\mathrm{in}} + n_{\\mathrm{out}}}$, onde $n_{\\mathrm{in}}$ e $n_{\\mathrm{out}}$ são o número de neurônios na camada anterior e seguinte, respectivamente.\n",
    "- [*He*](https://arxiv.org/pdf/1502.01852.pdf): Esta é uma adaptação do esquema anterior especificamente para a função **ReLU**. Ele compensa o fato de que esta ativação é zero para metade do espaço de entrada possível. Assim, $\\mathrm{Var}(W) = \\frac{2}{n_{\\mathrm{in}}}$ nesse caso.\n",
    "\n",
    "Para derivar a variância para inicialização de Xavier basta observar o que ocorre com a variância da saída de um neurônio linear (ignorando o bias), baseado na variância das suas entradas, assumingo que pesso e entradas _não são correlacionados_ e e ambos são _média zero_:\n",
    "\n",
    "$$\\mathrm{Var}\\left(\\sum_{i=1}^n w_i x_i\\right) = \\sum_{i=1}^{n_\\mathrm{in}} \\mathrm{Var}(w_ix_i) = \\sum_{i=1}^{n_\\mathrm{in}} \\mathrm{Var}(W)\\mathrm{Var}(X) = n_\\mathrm{in}\\mathrm{Var}(W)\\mathrm{Var}(X)$$\n",
    "\n",
    "Ou seja, para preservar a variância da entrada depois de passar pela camada, $\\mathrm{Var}(W) = \\frac{1}{n_\\mathrm{in}}$. Se aplicarmos o mesmo argumento no caso da atualização durante a backpropagation obtemos que $\\mathrm{Var}(W) = \\frac{1}{n_\\mathrm{out}}$. Como estas duas restrições não podem ser satisfeitas ao mesmo tempo, nós definimos a variância como a média das duas, ou seja, $\\mathrm{Var}(W) = \\frac{2}{n_\\mathrm{in} + n_\\mathrm{out}}$, o que funciona normalmente muito bem na prática.\n",
    "\n",
    "Estes dois esquemas funcionam bem para quase todas as redes, exceto as recorrentes, onde é melhor considerar uma inicialização [*ortogonal*](http://arxiv.org/pdf/1312.6120v3.pdf). \n",
    "\n",
    "Para incluir uma inicialização particular para uma camada em Keras, você deve especificar um parâmetro `init` para ela. Em nosso exemplo, vamos usar o esquema He (`he_uniform`) para as camadas ReLU e o Xavier uniforme (`glorot_uniform`) para a camada softmax (já que ela é apenas uma generalização da função logística para múltiplas entradas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add He initialisation to a layer\n",
    "conv_1 = Convolution2D(conv_depth, kernel_size, kernel_size, \n",
    "                       border_mode='same', \n",
    "                       init='he_uniform', W_regularizer=l2(l2_lambda), \n",
    "                       activation='relu')(inp)\n",
    "# Add Xavier initialisation to a layer\n",
    "out = Dense(num_classes, init='glorot_uniform', \n",
    "            W_regularizer=l2(l2_lambda), activation='softmax')(drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalisation\n",
    "\n",
    "De todas as técnicas descritas aqui, a mais importante é a **batch normalisation**, um método descrito por [Ioffe and Szegedy](https://arxiv.org/abs/1502.03167) para acelerar o treinamento de redes neurais. Ela é baseada num simples modo de falha que atrapalha o treinamento de redes neurais: _à medida que o sinal se propaga pela rede, mesmo se normalizado na entrada, ele pode acabar completamente enviesado em alguma camada oculta, tanto em termos de variância quanto média_ (efeito conhecido como *deriva interna da covariância*). Isto resulta em grandes discrepâncias entre as atualizações de gradientes ao longo de diferentes camadas. Como resultado, somos mais conservadores com a taxa de aprendizado e aplicamos regularizadores mais fortes, o que desacelera o treino.\n",
    "\n",
    "Batch normalisation propõe a normalização das ativações de uma camada para média zero e variância 1, através do batch de dados passando pela rede (ou seja, no treino, nós normalizamos pelos `batch_size` exemplos e, no teste, normalizamos considerando estatísticas derivadas do _treino todo_---uma vez que não conhecemos os dados de teste com antecedência). Nós calculamos a média e a variância para um batch particular de ativações $\\mathcal{B} = \\{x_1, \\dots, x_m\\}$ assim:\n",
    "\n",
    "$$\\begin{align*}\\mu_{\\mathcal{B}} &= \\frac{1}{m}\\sum_{i=1}^{m}x_i \\\\ \\sigma_\\mathcal{B}^2 &= \\frac{1}{m}\\sum_{i=1}^{m}\\left(x_i - \\mu_\\mathcal{B}\\right)^2\\end{align*}$$\n",
    "\n",
    "Nós então usamos estas estatísticas para transformar as ativações de forma que elas tenham média zero e variância um:\n",
    "\n",
    "$$\\hat{x}_i = \\frac{x_i - \\mu_\\mathcal{B}}{\\sqrt{\\sigma_\\mathcal{B}^2 + \\varepsilon}}$$\n",
    "\n",
    "onde $\\varepsilon > 0$ é um pequeno valor que nos protege de divisão por zero (no caso do desvio padrão do batch ser muito pequeno ou mesmo zero). Finalmente, para obter a ativação final $y$, precisamos estar certos que nenhuma propriedade de generalizaçao foi perdida ao executar a normalização---e desde que as operações feitas foram um deslocamento (média) e um escalonamento (desvio), nós permitimos um deslocamento e um escalonamento arbitrários dos valores normalizados para obter o valor final (isso permite a rede, por exemplo, voltar aos valores originais se ela os considerar mais úteis):\n",
    "\n",
    "$$y_i = \\gamma\\hat{x}_i + \\beta$$\n",
    "\n",
    "onde $\\beta$ e $\\gamma$ são parâmetros *treináveis* da operação de batch normalization (otimizáveis via gradiente descendente nos dados de treino). Esta generalização significa que batch normalisation pode se aplicada diretamente às _entradas_ da rede neural (dado que a presença desses parâmetros permite à rede assumir uma estatística de entrada diferente da que nós selecionamos através do processamento manual dos dados).\n",
    "\n",
    "Este método, quando aplicado às camadas de uma rede convolutiva quase sempre levam a maior velocidade do aprendizado. Eles também agem como ótimos regularizadores, nos permitindo um cuidado maior na escolha da taxa de aprendizado, na importância do $L_2$ e uso do dropout (tornando-o muitas vezes completamente desnecessário). Esta regularização ocorre como consequência do fato de que a saída de um único exemplo *não é mais determinística* (já que ela depende do batch inteiro a qual ela pertence), ajudando a rede a generalizar melhor.\n",
    "\n",
    "Note que no artigo original, os autores usam batch normalisation *antes* de aplicar a função de ativação do neurônio (nas combinações lineares computadas dos dados de entrada). Contudo, [em resultados recentes](http://arxiv.org/abs/1511.06422) observou-se que poderia ser mais benéfico (e, no mínimo, tao bom quanto) aplicá-la *depois*, o que é feito aqui.\n",
    "\n",
    "Em Keras, batch normalisation corresponde a uma camada: `BatchNormalization`, para a qual podemos fornecer alguns parâmetros. O mais importante deles é o `axis` (sobre que eixo dos dados as estatísticas deveriam ser computadas). Em particular, quando trabalhando com camadas de convolução, queremos normalizar através dos canais individuais, logo `axis = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch normalisation\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "# ...\n",
    "# apply BN to the input (N.B. need to rename here)\n",
    "inp_norm = BatchNormalization(axis=1)(inp) \n",
    "# conv_1 = Convolution2D(...)(inp_norm)\n",
    "# apply BN to the first conv layer\n",
    "conv_1 = BatchNormalization(axis=1)(conv_1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enriquecimento de dados\n",
    "\n",
    "Algumas vezes, é importante também refinar a coleção de treino, especialmente em tarefas de reconhecimento.\n",
    "\n",
    "Em geral, esperamos que o modelo permaneça invariante diante de possíveis níveis de distorção, tais como deslocamentos, rotações e redimensionamentos. Como o modelo é aprendido do treino que fornecemos para ele, pode ser útil introduzir tais distorções nos dados de treino, mesmo que _artificialmente_.\n",
    "\n",
    "Assim, antes de dar um exemplo para a rede, nós podemos tranformá-lo de forma apropriada, dando oportunidade para rede ver estes exemplos distorcidos e, assim, lidar melhor com eles se encontrados em coleções reais. A figura abaixo mostra exemplos de distorções aplicadas a dígitos na coleção MNIST:\n",
    "\n",
    "deslocado  |  deslocado | esticado | deslocado & aumentado | rotacionado & reduzido\n",
    ":-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:\n",
    "![](images/t_4_1.bmp) | ![](images/t_4_2.bmp) | ![](images/t_4_3.bmp) | ![](images/t_4_4.bmp) | ![](images/t_4_5.bmp)\n",
    "\n",
    "Keras fornece uma interface para enriquecimento de imagens:  `ImageDataGenerator`. Nós iniciamos esta classe com os tipos de transformações que queremos e então passamos os dados de treino pelo gerador, chamando o método `fit` seguido do método `flow`, que retorna um iterator infinito sobre os batches enriquecidos. Keras também oferece um método `model.fit_generator` que permite treinar diretamente o modelo usando este iterador, simplificando significativamente o código. Neste caso, como não há o parâmetro `validation_split`, é necessário separar o conjunto de validação manualmente.\n",
    "\n",
    "Em nosso exemplo, vamos aplicar deslocamentos aleatórios horizontais e verticais nos dados. `ImageDataGenerator` também suporta rotações, mudanças de tamanho, espelhamentos e esticamentos/achatamentos. Com exceção dos espelhamentos, todas estas seriam mudanças esperadas em um processo de reconhecimento de caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "# ... after model.compile(...)\n",
    "# Explicitly split the training and validation sets\n",
    "X_val = X_train[54000:]\n",
    "Y_val = Y_train[54000:]\n",
    "X_train = X_train[:54000]\n",
    "Y_train = Y_train[:54000]\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    # randomly shift images horizontally (fraction of total width)\n",
    "    width_shift_range=0.1, \n",
    "    # randomly shift images vertically (fraction of total height)\n",
    "    height_shift_range=0.1) \n",
    "datagen.fit(X_train)\n",
    "\n",
    "# fit the model on the batches generated by datagen.flow()\n",
    "# ---most parameters similar to model.fit\n",
    "model.fit_generator(datagen.flow(X_train, Y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        samples_per_epoch=X_train.shape[0],\n",
    "                        nb_epoch=num_epochs,\n",
    "                        validation_data=(X_val, Y_val),\n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles\n",
    "\n",
    "Um aspecto interessante de redes neurais é que, treinadas sob diferentes condições inciais, elas podem exibir _propriedades discriminativas_ diferentes para diferentes classes: ou seja, classificam melhor certas classes; se confundem mais com outras. No caso de MNIST, uma rede poderia ser muito boa em diferenciar o 3 e o 5; mas ao mesmo tempo ruim em diferenciar 1 e 7; outra rede, poderia ter o comportamento inverso.\n",
    "\n",
    "Estas discrepâncias podem ser exploradas por **ensembles** na própria arquitetura! Em lugar de criar um modelo, vários com diferentes condições iniciais são combinados para termos a resposta no fim. Neste exemplo, vamos fazer isso com três modelos diferentes. As diferenças entre as arquiteturas podem ser vistas na figura a seguir ([fonte: Keras](https://keras.io/visualization/)):\n",
    "\n",
    "Baseline (with BN)                     |  Ensemble\n",
    ":-------------------------:|:-------------------------:\n",
    "![](images/base.png)  |  ![](images/ens.png)\n",
    "\n",
    "O Keras fornece uma maneira fácil de combinar modelos - nós podemos colocar os modelos em um laço, extraindo deles as saídas para uma camada final que os combina usando `merge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import merge # for merging predictions in an ensemble\n",
    "# ...\n",
    "ens_models = 3 # we will train three separate models on the data\n",
    "# ...\n",
    "# Apply BN to the input (N.B. need to rename here)\n",
    "inp_norm = BatchNormalization(axis=1)(inp) \n",
    "\n",
    "outs = [] # the list of ensemble outputs\n",
    "for i in range(ens_models):\n",
    "    # conv_1 = Convolution2D(...)(inp_norm)\n",
    "    # ...\n",
    "    outs.append(Dense(num_classes, init='glorot_uniform', \n",
    "                      W_regularizer=l2(l2_lambda), \n",
    "                      activation='softmax')(drop)) # Output softmax layer\n",
    "\n",
    "# average the predictions to obtain the final output\n",
    "out = merge(outs, mode='ave') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parar antes...\n",
    "\n",
    "Nós vamos ver agora um exemplo de otimização de hiperparâmetro. Até agora usamos a coleção de validação meramente para monitorar o progresso do treino, o que é um desperdício de tempo, uma vez que não fazemos nada útil com isso. Na verdade, o conjunto de validação serve para avaliar os hiperparâmtros da rede, como a profundidade, o número de neurônios, os fatores de regularização, etc. A ideia é verficar o desempenho do modelo com diferentes parametrizações na validação e escolher a melhor combinação para avaliar no teste. Nunca esqueça que **não podemos usar o teste para melhorar o modelo. Ele serve apenas para avaliá-lo ao fim de tudo**.\n",
    "\n",
    "O uso mais simples da validação é definir o *número de épocas*, um procedimento conhecido como **early stopping**; simplesmente, pare o treino assim que o erro na validação não tiver diminuido por um certo número seguido de épocas (um pârametro chamado *paciência*). Como MNIST satura bem cedo, vamos usar uma paciência de 5 épocas para um total de 50 épocas (que provavelmente não vai ser alcançado).\n",
    "\n",
    "Keras suporta _early stopping_ via o _callback_ `EarlyStopping`. Callbacks são métodos que são chamados ao fim de cada época de treino, uma vez que você o forneça para os métodos `fit` ou `fit_generator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "# ...\n",
    "# we iterate at most fifty times over the entire training set\n",
    "num_epochs = 50 \n",
    "# ...\n",
    "# fit the model on the batches generated by datagen.flow()\n",
    "# ---most parameters similar to model.fit\n",
    "model.fit_generator(datagen.flow(X_train, Y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        samples_per_epoch=X_train.shape[0],\n",
    "                        nb_epoch=num_epochs,\n",
    "                        validation_data=(X_val, Y_val),\n",
    "                        verbose=1,\n",
    "                        # adding early stopping\n",
    "                        callbacks=[EarlyStopping(monitor='val_loss', \n",
    "                                                 patience=5)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O método completo!\n",
    "\n",
    "A seguir, temos a arquitetura com as técnica descritas aplicadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 11s - loss: 1.0846 - acc: 0.6970 - val_loss: 0.3301 - val_acc: 0.9502\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 8s - loss: 0.4567 - acc: 0.9029 - val_loss: 0.2016 - val_acc: 0.9686\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 8s - loss: 0.3343 - acc: 0.9355 - val_loss: 0.1740 - val_acc: 0.9756\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 8s - loss: 0.2808 - acc: 0.9490 - val_loss: 0.1545 - val_acc: 0.9804\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 8s - loss: 0.2522 - acc: 0.9542 - val_loss: 0.1414 - val_acc: 0.9816\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 8s - loss: 0.2285 - acc: 0.9604 - val_loss: 0.1308 - val_acc: 0.9838\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 8s - loss: 0.2082 - acc: 0.9643 - val_loss: 0.1315 - val_acc: 0.9832\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 8s - loss: 0.1959 - acc: 0.9665 - val_loss: 0.1220 - val_acc: 0.9832\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 7s - loss: 0.1827 - acc: 0.9702 - val_loss: 0.1169 - val_acc: 0.9862\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 8s - loss: 0.1780 - acc: 0.9693 - val_loss: 0.1119 - val_acc: 0.9856\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 8s - loss: 0.1666 - acc: 0.9725 - val_loss: 0.1054 - val_acc: 0.9874\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 8s - loss: 0.1634 - acc: 0.9729 - val_loss: 0.1072 - val_acc: 0.9854\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 8s - loss: 0.1595 - acc: 0.9738 - val_loss: 0.1040 - val_acc: 0.9856\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 8s - loss: 0.1527 - acc: 0.9751 - val_loss: 0.1022 - val_acc: 0.9874\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 8s - loss: 0.1522 - acc: 0.9747 - val_loss: 0.0991 - val_acc: 0.9896\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 7s - loss: 0.1484 - acc: 0.9754 - val_loss: 0.0948 - val_acc: 0.9876\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 7s - loss: 0.1454 - acc: 0.9761 - val_loss: 0.0970 - val_acc: 0.9886\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 8s - loss: 0.1423 - acc: 0.9772 - val_loss: 0.0922 - val_acc: 0.9886\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 7s - loss: 0.1416 - acc: 0.9774 - val_loss: 0.0893 - val_acc: 0.9900\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 7s - loss: 0.1390 - acc: 0.9768 - val_loss: 0.0925 - val_acc: 0.9886\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 7s - loss: 0.1368 - acc: 0.9781 - val_loss: 0.0909 - val_acc: 0.9894\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 7s - loss: 0.1357 - acc: 0.9782 - val_loss: 0.0909 - val_acc: 0.9894\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 7s - loss: 0.1359 - acc: 0.9788 - val_loss: 0.0913 - val_acc: 0.9890\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 7s - loss: 0.1309 - acc: 0.9799 - val_loss: 0.0875 - val_acc: 0.9888\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 7s - loss: 0.1306 - acc: 0.9794 - val_loss: 0.0861 - val_acc: 0.9900\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 8s - loss: 0.1319 - acc: 0.9793 - val_loss: 0.0882 - val_acc: 0.9912\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 8s - loss: 0.1302 - acc: 0.9793 - val_loss: 0.0891 - val_acc: 0.9904\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 7s - loss: 0.1294 - acc: 0.9794 - val_loss: 0.0879 - val_acc: 0.9894\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 8s - loss: 0.1251 - acc: 0.9810 - val_loss: 0.0846 - val_acc: 0.9904\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 8s - loss: 0.1261 - acc: 0.9800 - val_loss: 0.0866 - val_acc: 0.9896\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 7s - loss: 0.1272 - acc: 0.9801 - val_loss: 0.0834 - val_acc: 0.9910\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 8s - loss: 0.1244 - acc: 0.9807 - val_loss: 0.0814 - val_acc: 0.9892\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 8s - loss: 0.1234 - acc: 0.9806 - val_loss: 0.0892 - val_acc: 0.9890\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 7s - loss: 0.1237 - acc: 0.9804 - val_loss: 0.0863 - val_acc: 0.9914\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 8s - loss: 0.1221 - acc: 0.9811 - val_loss: 0.0850 - val_acc: 0.9902\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 8s - loss: 0.1216 - acc: 0.9815 - val_loss: 0.0862 - val_acc: 0.9906\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 8s - loss: 0.1225 - acc: 0.9815 - val_loss: 0.0841 - val_acc: 0.9914\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 8s - loss: 0.1204 - acc: 0.9818 - val_loss: 0.0850 - val_acc: 0.9892\n",
      " 9984/10000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.079195528954267499, 0.99070000000000003]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic class for specifying and training a neural network\n",
    "from keras.models import Model \n",
    "from keras.layers import Input, Dense, Flatten, \\\n",
    "    Convolution2D, MaxPooling2D, Dropout, merge\n",
    "# L2-regularisation\n",
    "from keras.regularizers import l2 \n",
    "# batch normalisation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "# data augmentation\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "# early stopping\n",
    "from keras.callbacks import EarlyStopping \n",
    "\n",
    "batch_size = 128 # 128 training examples per iter\n",
    "num_epochs = 50 # max of fifty iters over the training \n",
    "kernel_size = 3 # we will use 3x3 kernels throughout\n",
    "pool_size = 2 # we will use 2x2 pooling throughout\n",
    "conv_depth = 32 # 32 kernels in both convolutional layers\n",
    "drop_prob_1 = 0.25 # dropout after pooling - probability 0.25\n",
    "drop_prob_2 = 0.5 # dropout in the FC layer - probability 0.5\n",
    "hidden_size = 128 # 128 neurons in both hidden layers\n",
    "l2_lambda = 0.0001 # L2-regularisation factor\n",
    "ens_models = 3 # three-models ensamble\n",
    "\n",
    "num_train = 55000 # 55000 training examples in MNIST\n",
    "num_test = 10000 # 10000 test examples in MNIST\n",
    "\n",
    "# MNIST images are 28x28 and greyscale\n",
    "height, width, depth = 28, 28, 1 \n",
    "num_classes = 10 # 10 classes (1 per digit)\n",
    "\n",
    "# fetch data\n",
    "X_train = mnist.train.images\n",
    "Y_train = mnist.train.labels\n",
    "X_test  = mnist.test.images\n",
    "Y_test  = mnist.test.labels\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], depth, height, width)\n",
    "X_test = X_test.reshape(X_test.shape[0], depth, height, width)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Explicitly split the training and validation sets\n",
    "X_val = X_train[50000:]\n",
    "Y_val = Y_train[50000:]\n",
    "X_train = X_train[:50000]\n",
    "Y_train = Y_train[:50000]\n",
    "\n",
    "# N.B. Keras expects channel dimension first\n",
    "inp = Input(shape=(depth, height, width)) \n",
    "# Apply BN to the input (N.B. need to rename here)\n",
    "inp_norm = BatchNormalization(axis=1)(inp) \n",
    "\n",
    "outs = [] # the list of ensemble outputs\n",
    "for i in range(ens_models):\n",
    "    # Conv [32] -> Conv [32] -> Pool (with dropout on the \n",
    "    # pooling layer), applying BN in between\n",
    "    conv_1 = Convolution2D(conv_depth, kernel_size, \n",
    "                           kernel_size, border_mode='same', \n",
    "                           init='he_uniform', \n",
    "                           W_regularizer=l2(l2_lambda), \n",
    "                           activation='relu')(inp_norm)\n",
    "    conv_1 = BatchNormalization(axis=1)(conv_1)\n",
    "    conv_2 = Convolution2D(conv_depth, kernel_size, \n",
    "                           kernel_size, border_mode='same', \n",
    "                           init='he_uniform', \n",
    "                           W_regularizer=l2(l2_lambda), \n",
    "                           activation='relu')(conv_1)\n",
    "    conv_2 = BatchNormalization(axis=1)(conv_2)\n",
    "    pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size), \n",
    "                          dim_ordering=\"th\")(conv_2)\n",
    "    drop_1 = Dropout(drop_prob_1)(pool_1)\n",
    "    flat = Flatten()(drop_1)\n",
    "    # Hidden ReLU layer\n",
    "    hidden = Dense(hidden_size, init='he_uniform', \n",
    "                   W_regularizer=l2(l2_lambda), \n",
    "                   activation='relu')(flat) \n",
    "    hidden = BatchNormalization(axis=1)(hidden)\n",
    "    drop = Dropout(drop_prob_2)(hidden)\n",
    "    # Output softmax layer\n",
    "    outs.append(Dense(num_classes, init='glorot_uniform', \n",
    "                      W_regularizer=l2(l2_lambda), \n",
    "                      activation='softmax')(drop)) \n",
    "\n",
    "# average the predictions to obtain the final output\n",
    "out = merge(outs, mode='ave') \n",
    "\n",
    "# To define a model, just specify its input and output layers\n",
    "model = Model(input=inp, output=out) \n",
    "\n",
    "# using the cross-entropy loss function\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', # Adam optimiser\n",
    "              metrics=['accuracy']) # accuracy\n",
    "\n",
    "# randomly shift images horizontally (fraction of total width)\n",
    "# randomly shift images vertically (fraction of total height)\n",
    "datagen = ImageDataGenerator(\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1, dim_ordering=\"th\")  \n",
    "datagen.fit(X_train)\n",
    "\n",
    "# fit the model on the batches generated by datagen.flow()\n",
    "# ---most parameters similar to model.fit\n",
    "# adding early stopping\n",
    "model.fit_generator(datagen.flow(X_train, Y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        samples_per_epoch=X_train.shape[0],\n",
    "                        nb_epoch=num_epochs,\n",
    "                        validation_data=(X_val, Y_val),\n",
    "                        verbose=1,\n",
    "                        callbacks=[EarlyStopping(monitor='val_loss', \n",
    "                                                 patience=5)]) \n",
    "\n",
    "# Evaluate the trained model on the test set!\n",
    "model.evaluate(X_test, Y_test, verbose=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embora nosso modelo tenha conseguido melhorar nosso baseline, o ganho parece pequeno em MNIST. O benefícios, contudo, são bem mais óbvios quando aplicados a problemas mais complexos como a classificação de imagens em CIFAR-10, mas você tem que ter mais hardware e/ou paciência."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observações sobre transferência de aprendizagem\n",
    "\n",
    "É comum que muitas vezes dispomos de uma coleção pequena para treinar. Ela raramente pode ser usada com redes muito complexas, com sucesso. Neste caso, podemos usar os pesos aprendidos em uma rede complexa como ponto inicial de nosso treino. Este é um tipo especial de refinamento com redes neurais conhecido como _transferência de aprendizagem_ (TA).\n",
    "\n",
    "Entre os vários fatores importantes para TA, dois a considerar são o tamanho da base a treinar (base nova -- BN) e a similaridade com a base treinada (base original -- BO). Supondo que as duas bases foram criadas para tarefas similares (ex: classificação de imagens naturais), temos quatro cenários:\n",
    "\n",
    "* BN < BO com conteudo similar: se a BN é pequena, a tuning de parâmetros vai levar a overfitting. Como elas são similares, os atributos de alto nível da BO servem para a BN. Logo, é melhor treinar um classificador linear nos atributos (de alto nível) aprendidos pela BO.\n",
    "\n",
    "* BN ~< BO com conteúdo similar: comece com pesos da BO e depois de treinar BN, faça tuning dos parâmetros.\n",
    "\n",
    "* BN < BO com conteúdo distinto: treine um classificador linear usando atributos de baixo nível da BO.\n",
    "\n",
    "* BN ~< BO com conteúdo distinto: use os pesos da BO (partir de uma rede pre-treinada pode ser sempre útil), treine a BN e faça tuning de todos os parâmetros da arquitetura.\n",
    "\n",
    "Para maiores detalhes, vejam http://blog.revolutionanalytics.com/2016/08/deep-learning-part-2.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De material de *Petar Veličković* e *Anusua Trivedi*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
