{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressores & Aprendizagem de Máquina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML como um problema de otimização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Estimador linear_, considerando $n-1$ atributos: \n",
    "    \n",
    "    $\\hat{y}(\\textbf{w}) = \\hat{y}(\\textbf{w}, \\textbf{x}) = w_0 x_0 + w_1 x_1 + \\ldots + w_{n-1} x_{n-1} + b$\n",
    "    \n",
    "    $\\hat{y}(\\textbf{w}) = w_0 x_0 + w_1 x_1 + \\ldots + w_{n-1} x_{n-1} + w_{n} x_{n}$ (bias incorporado em $\\textbf{x}$, $b = w_n, x_n = 1$)\n",
    "    \n",
    "    $\\hat{y}(\\textbf{w}) = \\textbf{x} \\dot~\\textbf{w}$\n",
    "\n",
    "    Se considerarmos todas as $m$ instâncias $\\textbf{x}$, temos $\\hat{y}(\\textbf{w}) = X \\dot~\\textbf{w}$ (supondo $X^{(m \\times n)}$ e $\\textbf{w}^{(n \\times 1)}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Eventualmente estimador linear poderia incorporar um transformador, uma _função de ativação_ $\\alpha$: \n",
    "    \n",
    "    $\\hat{y}(\\textbf{w}) = \\alpha(X \\dot~\\textbf{w})$\n",
    "    \n",
    "    Supondo $\\alpha(x) = x$, função identidade, temos o estimador clássico usado na regressão linear, ou seja,     $\\hat{y}(\\textbf{w}) = X \\dot~\\textbf{w}$. Funções de ativação podem ser usadas para transformar a saída para um certo intervalo (-1 a 1, 0 a 1, etc) e também modificarem a natureza simples de um estimador polinomial de grau 1, como o usado aqui. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Função de perda_ (_loss_), avalia o quão bom é o seu estimador $\\hat{y}(\\textbf{w})$ em relação aos valores reais $\\textbf{y}$. Por exemplo, ao adotarmos a média das diferenças dos quadrados (MSE -- mean squared error), temos:\n",
    "\n",
    "    $\\ell(\\hat{y}(\\textbf{w}), \\textbf{y}) = \\frac{1}{m} \\sum_{i}^{m}{(\\hat{y}_i(\\textbf{w}) - y_i)^2}$\n",
    "    \n",
    "    $\\ell(\\hat{y}(\\textbf{w}), \\textbf{y}) = mean ((\\hat{y}(\\textbf{w}) - \\textbf{y})^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Função de perda com _regularizador_ $R$:\n",
    "\n",
    "    $L(\\hat{y}(\\textbf{w}), \\textbf{y}) = \\ell(\\hat{y}(\\textbf{w}), \\textbf{y}) + \\lambda R(\\textbf{w})$\n",
    "    \n",
    "    O regularizador é uma função de penalização que tem por objetivo eliminar certos conjuntos de pesos, privilegiando outros. Ao fazer isso, o modelo se restringe a um menor espaço de pesos, melhorando sua generalização. Exemplos de regularizadores são o L1 ($R(\\mathbf{w}) \\approx \\sum_{\\forall i}{|w_i|}$) e o L2 ($R(\\mathbf{w}) \\approx \\sum_{\\forall i}{|w_i|}^2$). Note que enquanto L1 prefere pesos não nulos, o L2 não gosta de _outliers_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Objetivo de ML é determinar $\\textbf{w}$ que minimiza $L$, ou seja, determinar $\\textbf{w}$ tal que $\\frac{\\partial}{\\partial \\textbf{w}} L(\\hat{y}(\\textbf{w}), \\textbf{y}) = 0$. No nosso caso, supondo $\\lambda = 0$:\n",
    "\n",
    "    $\n",
    "    \\begin{align}\n",
    "        \\frac{\\partial}{\\partial \\textbf{w}} L(\\hat{y}(\\textbf{w}), \\textbf{y}) &= \\frac{\\partial}{\\partial \\textbf{w}} \\frac{1}{m} \\sum_{i}^{m}{(\\hat{y}_i(\\textbf{w}) - y_i)^2} = 0 \\\\\n",
    "        &= \\frac{1}{m} \\sum_{i}^{m}{\\frac{\\partial}{\\partial \\textbf{w}} (\\hat{y}_i(\\textbf{w}) - y_i)^2} \\\\\n",
    "        &= \\frac{1}{m} \\sum_{i}^{m}{\\frac{\\partial}{\\partial \\textbf{w}} (\\textbf{x}_i \\dot~\\textbf{w} - y_i)^2} \\\\\n",
    "        &= \\frac{1}{m} \\sum_{i}^{m}{\\frac{\\partial}{\\partial \\textbf{w}} ({\\textbf{x}_i}^2 \\dot~\\textbf{w}^2 - 2 \\textbf{x}_i \\dot~\\textbf{w} \\dot~y_i + {y_i}^2})  \\\\\n",
    "        &= \\frac{1}{m} \\sum_{i}^{m}{(2 {\\textbf{x}_i}^2 \\dot~\\textbf{w} - 2 \\textbf{x}_i \\dot~y_i)}  \\\\\n",
    "        &= \\frac{2}{m} \\sum_{i}^{m}{({\\textbf{x}_i}^2 \\dot~\\textbf{w} - \\textbf{x}_i \\dot~y_i)}  \\\\\n",
    "        &= \\frac{2}{m} (\\sum_{i}^{m}{{\\textbf{x}_i}^2 \\dot~\\textbf{w}} - \\sum_{i}^{m} {\\textbf{x}_i \\dot~y_i})  \\\\\n",
    "        &= \\frac{2}{m} (X^T X\\dot~\\textbf{w} - X^T \\textbf{y}) \\text{ supondo } X^{(m \\times n)}, \\textbf{y}^{(m \\times 1)} \\text{ e } \\textbf{w}^{(n \\times 1)}\\\\ \n",
    "        &= \\frac{2}{m} X^T (X\\dot~\\textbf{w} - \\textbf{y})  \n",
    "    \\end{align}    \n",
    "    $\n",
    "\n",
    "    Esta função corresponde ao gradiente da função de perda $L$ e pode ser usado para obter $\\textbf{w}$ tanto diretamente quanto iterativamente. Como a solução direta é limitada a valores pequenos de $n$ e $m$, vamos implementar a solução iterativa, o _Gradiente Descendente_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente Descendente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GradientDescendente($X^{m \\times n}$, $\\textbf{y}^{m \\times 1}$, taxa de aprendizado $\\eta$, número de épocas $N$):\n",
    "    - inicie $\\textbf{w}^{n \\times 1}$ com valores aleatorios entre -1 e 1\n",
    "    - para $N$ épocas:\n",
    "        - $\\hat{y}(\\textbf{w}) = X \\dot~\\textbf{w}$\n",
    "        - $\\ell(\\hat{y}(\\textbf{w}), \\textbf{y}) = mean ((\\hat{y}(\\textbf{w}) - \\textbf{y})^2)$\n",
    "        - $\\nabla\\ell = \\frac{2}{m} X^T (X\\dot~\\textbf{w} - ~\\textbf{y})$\n",
    "        - $\\textbf{w}' = \\textbf{w} - \\eta \\nabla\\ell$\n",
    "        - $\\textbf{w} = \\textbf{w}'$\n",
    "    - retorne $\\textbf{w}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GD em Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 10)\n",
    "y = 6. * x - 1.\n",
    "y += np.random.normal(0, 1.0, len(x))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y, '.')\n",
    "plt.plot(6. * x - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xscaled = (x - np.mean(x))/np.std(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xscaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorporando bias em X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.stack([np.ones(len(y)), xscaled]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = X.shape\n",
    "m, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.uniform(-1, 1, (2,1))\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(X, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gd(X, y, n_epochs = 100, lrating = 0.1):    \n",
    "    m, n = X.shape # m instancias e n colunas (incluindo bias)\n",
    "    Y = y.reshape(-1,1)\n",
    "    W = np.random.uniform(-1, 1, (n, 1))\n",
    "    for e in range(n_epochs):\n",
    "        Yhat = np.matmul(X, W)\n",
    "        error = Yhat - Y\n",
    "        loss = np.mean(error ** 2)\n",
    "        gloss = (2./m) * np.matmul(X.T, error)\n",
    "        Wn = W - lrating * gloss\n",
    "        W = Wn\n",
    "        if e%10 == 0:\n",
    "            print(loss)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = gd(X, y, n_epochs = 200, lrating = 0.1)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y, '.')\n",
    "plt.plot(np.matmul(X,W))\n",
    "plt.xlim(-1,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traduzindo para TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gdtf(X, y, n_epochs = 100, lrating = 0.1):\n",
    "    m, n = X.shape\n",
    "    \n",
    "    X = tf.constant(X, dtype = tf.float32, name = \"X\")\n",
    "    y = tf.constant(y.reshape(-1, 1), dtype = tf.float32, name = \"y\")\n",
    "    W = tf.Variable(tf.random_uniform([n, 1], -1.0, 1.0), name = \"W\")\n",
    "    \n",
    "    Yhat = tf.matmul(X, W, name = \"predictions\")\n",
    "    error = Yhat - y\n",
    "    loss = tf.reduce_mean(tf.square(error), name = \"loss\")\n",
    "    gloss = (2./m) * tf.matmul(tf.transpose(X), error)\n",
    "    training = tf.assign(W, W - lrating * gloss)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as s:\n",
    "        s.run(init)\n",
    "        for e in range(n_epochs):\n",
    "            s.run(training)\n",
    "            if e%10==0:\n",
    "                print(loss.eval())\n",
    "        finalW = W.eval()\n",
    "    return finalW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = gdtf(X, y, n_epochs=100)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y, '.')\n",
    "plt.plot(np.matmul(X,W))\n",
    "plt.plot(6*x - 1.0, 'r--')\n",
    "plt.ylim(-10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além de saber se função está sendo minimizada, é importante ver como ela está sendo minimizada. Assim, o código normalmente coleta mais informações, mas com o cuidado de não interferir constantemente com a otimização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gdtf(X, y, n_epochs = 100, lrating = 0.1):\n",
    "    m, n = X.shape\n",
    "    \n",
    "    X = tf.constant(X, dtype = tf.float32, name = \"X\")\n",
    "    y = tf.constant(y.reshape(-1, 1), dtype = tf.float32, name = \"y\")\n",
    "    W = tf.Variable(tf.random_uniform([n, 1], -1.0, 1.0), name = \"W\")\n",
    "    \n",
    "    Yhat = tf.matmul(X, W, name = \"predictions\")\n",
    "    error = Yhat - y\n",
    "    loss = tf.reduce_mean(tf.square(error), name = \"loss\")\n",
    "    gloss = (2./m) * tf.matmul(tf.transpose(X), error)\n",
    "    training = tf.assign(W, W - lrating * gloss)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    lossvalues = []\n",
    "    with tf.Session() as s:\n",
    "        s.run(init)\n",
    "        for e in range(n_epochs):\n",
    "            s.run(training)\n",
    "            lossvalues += [loss.eval()]  # guardando os valores de perda\n",
    "            if e%10==0:                  # evitando I/O a cada rodada\n",
    "                print(lossvalues[-1])\n",
    "        finalW = W.eval()\n",
    "    return finalW, lossvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, lossvalues = gdtf(X, y, n_epochs=100)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossvalues[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que deveriamos evitar avaliar o grafo múltiplas vezes para obter valores de nós. O ideal é obter todos os valores de interesse em uma única avaliação, como feito abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gdtf(X, y, n_epochs = 100, lrating = 0.1):\n",
    "    m, n = X.shape\n",
    "    \n",
    "    X = tf.constant(X, dtype = tf.float32, name = \"X\")\n",
    "    y = tf.constant(y.reshape(-1, 1), dtype = tf.float32, name = \"y\")\n",
    "    W = tf.Variable(tf.random_uniform([n, 1], -1.0, 1.0), name = \"W\")\n",
    "    \n",
    "    Yhat = tf.matmul(X, W, name = \"predictions\")\n",
    "    error = Yhat - y\n",
    "    loss = tf.reduce_mean(tf.square(error), name = \"loss\")\n",
    "    gloss = (2./m) * tf.matmul(tf.transpose(X), error)\n",
    "    training = tf.assign(W, W - lrating * gloss)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    lossvalues = []\n",
    "    with tf.Session() as s:\n",
    "        s.run(init)\n",
    "        for e in range(n_epochs):\n",
    "            W_e, loss_e = s.run([training, loss]) # Roda o grafo uma unica vez para treino e perda\n",
    "            lossvalues += [loss_e]\n",
    "            if e%10==0:\n",
    "                print(lossvalues[-1])\n",
    "    return W_e, lossvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, lossvalues = gdtf(X, y, n_epochs=100)\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diferenciação automática"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que a função a ser diferenciada depende do estimador, da ativação, da função de perda e do regularizador. Logo, ela pode ser bastante complexa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para estas situações, o TF ajuda com um mecanismo de auto diferenciação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# autodiferenciador\n",
    "def gdtf2(X, y, n_epochs = 100, lrating = 0.1):\n",
    "    m, n = X.shape\n",
    "        \n",
    "    X = tf.constant(X, dtype = tf.float32, name = \"X\")\n",
    "    y = tf.constant(y.reshape(-1, 1), dtype = tf.float32, name = \"y\")\n",
    "    W = tf.Variable(tf.random_uniform([n, 1], -1.0, 1.0), name = \"W\")\n",
    "    \n",
    "    Yhat = tf.matmul(X, W, name = \"predictions\")\n",
    "    error = Yhat - y\n",
    "    loss = tf.reduce_mean(tf.square(error), name = \"loss\")\n",
    "    # gradients(funcao de perda, variaveis) --> [gradiente parcial em cada variavel]\n",
    "    gloss = tf.gradients(loss, [W])[0]\n",
    "    training = tf.assign(W, W - lrating * gloss)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    lossvalues = []\n",
    "    with tf.Session() as s:\n",
    "        s.run(init)\n",
    "        for e in range(n_epochs):\n",
    "            W_e, loss_e = s.run([training, loss])\n",
    "            lossvalues += [loss_e]\n",
    "            if e%10==0:\n",
    "                print(loss_e)\n",
    "    return W_e, lossvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, _ = gdtf2(X, y, n_epochs=100)\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diferentes otimizadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como lidar com mínimos locais e _saddle points_?? (pontos em que eixos representam concavidades opostas, muitas vezes não fáceis de detectar devido a platôs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/NY-UOaLbhrE\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma possibilidade para lidar com estes problemas é usar diferentes otimizadores, como os descritos a seguir:\n",
    "\n",
    "* _SGD_: gradiente descendente estocástico. Usa gradiente da função para determinar direção e distância do ponto de mínimo. Ele corresponde ao GD clássico (_batch GD_) considerando estimativas baseadas em uma única instância $x_i$; se mais instâncias forem usadas, digamos $b$ instâncias, tal que $b << m$, então o algoritmo é chamado de GD (ou SGD, dependendo dos autores) com mini lotes, cada lote com tamanho $b$ (mini-batch [stochastic] gradient descent);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Momento_: atualização dos pesos depende de útima atualização, de forma que o impulso anterior interfere no comportamento atual (mais adiante). Supondo uma bolinha descendo um morro, ela vai ficando cada vez mais rápida à medida que desce. Algumas variantes deste algorimo podem incorporar ruído Gaussiano;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Gradiente Acelerado de Nesterov_ (NAG): o momento é corrigido pelo efeito que ele vai ter. Intuitivamente, é como se o comportamento fosse baseado no impulso (passado) corrigido pelo comportamento que se vai adquirir em seguida (futuro). Supondo a bolinha descendo o morro, ela vai desacelerando à medida que se aproxima de um novo morro (isso diminui o efeito de vai-e-vem perto do mínimo);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Adagrad_: Os métodos anteriores supõem que uma mesma taxa de atualização $\\eta$ deve ser aplicada para todos os parâmetros $w_0, w_1, ..., w_n$. Adagrad é um método adaptativo que aplica um valor distinto de $\\eta$ ($\\eta_0, \\eta_1, ..., \\eta_n$) para cada parâmetro de acordo com os valores anteriores observados para esse parâmetro. A ideia é atualizar _mais_ parâmetros associados a atributos observados menos frequentemente; e _menos_, parâmetros associados a atributos mais frequentes. Estes métodos são particularmente rápidos para problemas em que há atributos esparsos como em linguagem natural. Uma vantagem desse método é não ser necessário adotar uma política manual de atualização de um $\\eta$ global. Uma desvantagem é que a taxa de aprendizado tende à zero com o tempo;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Adadelta_: extensão de Adagrad que tenta resolver o problema de sumiço de $\\eta$ baseando a adaptação em uma janela de valores do passado de tamanho fixo em lugar de todos os valores;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _RMSProp_: método proposto por Hinton, quase ao mesmo tempo que o Adadelta, para corrigir o problema do Adagrad. Embora haja pequenas diferenças em relação ao Adadelta, eles se baseiam em ideias muito similares;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Adam_, _AdaMax_ e _Nadam_: modificações de Adadelta (Adam e AdaMax) e RMSProb (Nadam) para incorporar momento aos métodos adaptativos;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em geral, estes algoritmos buscam lidar melhor com diferentes topologias que o SGD. Por exemplo, veja o caso de superfícies com simetrias (as animações abaixo são de Alec Radford): algoritmos baseados em momento exploram até encontrar saída; os adaptativos rapidamente começam a descer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/LongValleyImgur.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No caso de superfícies com _saddle points_, o SGD se mostra claramente muito lento; os de momento repetem seu comportamento exploratório; os adaptativos novamente são muito rápidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/SaddlePointImgur.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, um exemplo de uso de um otimizador de momento no TF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# otimizadores\n",
    "def gdtf3(X, y, n_epochs = 100, lrating = 0.1):\n",
    "    m, n = X.shape\n",
    "    \n",
    "    X = tf.constant(X, dtype = tf.float32, name = \"X\")\n",
    "    y = tf.constant(y.reshape(-1, 1), dtype = tf.float32, name = \"y\")\n",
    "    W = tf.Variable(tf.random_uniform([n, 1], -1.0, 1.0), name = \"W\")\n",
    "    \n",
    "    Yhat = tf.matmul(X, W, name = \"predictions\")\n",
    "    loss = tf.reduce_mean(tf.square(Yhat - y), name = \"loss\")\n",
    "    \n",
    "    #otimizador = tf.train.GradientDescentOptimizer(learning_rate = lrating)\n",
    "    otimizador = tf.train.MomentumOptimizer(learning_rate = lrating, momentum = 0.9)\n",
    "    training = otimizador.minimize(loss)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    lossvalues = []\n",
    "    with tf.Session() as s:\n",
    "        s.run(init)\n",
    "        for e in range(n_epochs):\n",
    "            W_e, loss_e = s.run([training, loss])\n",
    "            lossvalues += [loss_e]\n",
    "            if e%10==0:\n",
    "                print(loss_e)\n",
    "    return W_e, lossvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, _ = gdtf3(X, y, n_epochs=100)\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O otimizador baseado em momento, usado no programa acima, \"lembra\" a última atualização e a repete em parte -- algo como descrito na Equação abaixo:\n",
    "\n",
    "$$\\textbf{w}' = \\textbf{w} + \\mu * v - \\eta \\nabla\\ell$$\n",
    "\n",
    "onde $\\mu$ é o quanto a última atualização $v$ deve ser lembrada. Um típico valor de de $\\mu$ pode ser 0.9. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício**: Compare os otimizadores, usando 40 épocas: \n",
    "* GradientDescentOptimizer(learning_rate = 0.1)\n",
    "* AdagradOptimizer(learning_rate = 1.0)\n",
    "* RMSPropOptimizer(learning_rate = 0.1)\n",
    "* AdamOptimizer(learning_rate = 1.0)\n",
    "\n",
    "Uma vez que tiver obtido as curvas de perda para cada método (ls_sgd, ls_adag, ls_rms e ls_adam) você pode plotá-las com o código abaixo:\n",
    "\n",
    "```python\n",
    "# gráficos\n",
    "plt.plot(ls_sgd, label = 'sgd')\n",
    "plt.plot(ls_adag, label = 'adag')\n",
    "plt.plot(ls_rms, label = 'rms')\n",
    "plt.plot(ls_adam, label = 'adam')\n",
    "plt.legend()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\">\n",
    "<a href=\"#losses\" class=\"btn btn-default\" data-toggle=\"collapse\">Solução #1</a>\n",
    "</div>\n",
    "<div id=\"losses\" class=\"collapse\">\n",
    "```\n",
    "# otimizadores\n",
    "def gdtf_opt(X, y, n_epochs = 100, lrating = 0.1, \n",
    "             otimizador = None):\n",
    "    m, n = X.shape\n",
    "    \n",
    "    X = tf.constant(X, dtype = tf.float32, name = \"X\")\n",
    "    y = tf.constant(y.reshape(-1, 1), dtype = tf.float32, name = \"y\")\n",
    "    W = tf.Variable(tf.random_uniform([n, 1], -1.0, 1.0), name = \"W\")\n",
    "    \n",
    "    Yhat = tf.matmul(X, W, name = \"predictions\")\n",
    "    loss = tf.reduce_mean(tf.square(Yhat - y), name = \"loss\")\n",
    "    \n",
    "    training = otimizador.minimize(loss)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    lossvalues = []\n",
    "    with tf.Session() as s:\n",
    "        s.run(init)\n",
    "        for e in range(n_epochs):\n",
    "            W_e, loss_e = s.run([training, loss])\n",
    "            lossvalues += [loss_e]\n",
    "    return W_e, lossvalues\n",
    "\n",
    "# execução\n",
    "W, ls_sgd = gdtf_opt(X, y, n_epochs=40, \n",
    "             otimizador = tf.train.GradientDescentOptimizer(\n",
    "                 learning_rate = 0.1))\n",
    "W, ls_adag = gdtf_opt(X, y, n_epochs=40, \n",
    "             otimizador = tf.train.AdagradOptimizer(learning_rate = 1))\n",
    "W, ls_rms = gdtf_opt(X, y, n_epochs=40, \n",
    "             otimizador = tf.train.RMSPropOptimizer(learning_rate = 0.1))\n",
    "W, ls_adam = gdtf_opt(X, y, n_epochs=40, \n",
    "             otimizador = tf.train.AdamOptimizer(learning_rate = 1))\n",
    "\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processamento em lotes: os métodos estocásticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apenas relembrando placeholders..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "A = tf.placeholder(tf.float32, shape=(None,3))\n",
    "B = A + 5\n",
    "\n",
    "with tf.Session() as s:\n",
    "    b1 = s.run(B, feed_dict = {A: [[1,2,3]]})\n",
    "    \n",
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GD com batches --> gradiente descendente estocástico\n",
    "def gdtf4(Xi, yi, batch_size = 1, n_epochs = 100, lrating = 0.1):\n",
    "    def get_batch(batch_size):\n",
    "        ids = np.random.randint(Xi.shape[0], size = batch_size)\n",
    "        return Xi[ids, :], yi[ids,:]\n",
    "    \n",
    "    m, n = Xi.shape\n",
    "    yi = yi.reshape(-1,1)\n",
    "    \n",
    "    n_batches = int(np.ceil(float(m)/batch_size))\n",
    "    \n",
    "    X = tf.placeholder(dtype = tf.float32, shape=(None, n), name = \"X\")\n",
    "    y = tf.placeholder(dtype = tf.float32, shape=(None, 1), name = \"y\")\n",
    "    W = tf.Variable(tf.random_uniform([n, 1], -1.0, 1.0), name = \"W\")\n",
    "    \n",
    "    Yhat = tf.matmul(X, W, name = \"predictions\")\n",
    "    loss = tf.reduce_mean(tf.square(Yhat - y), name = \"loss\")\n",
    "    \n",
    "    #otimizador = tf.train.GradientDescentOptimizer(learning_rate = lrating)\n",
    "    otimizador = tf.train.MomentumOptimizer(learning_rate = lrating, momentum = 0.9)\n",
    "    training = otimizador.minimize(loss)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    lossvalues = []\n",
    "    mlossvalues = []\n",
    "    with tf.Session() as s:\n",
    "        s.run(init)\n",
    "        for e in range(n_epochs):\n",
    "            mloss = 0.\n",
    "            for batch_index in range(n_batches):\n",
    "                # politica completamente estocastica!\n",
    "                Xb, yb = get_batch(batch_size)\n",
    "                _, W_e, loss_e = s.run([training, W, loss], feed_dict = {X: Xb, y: yb})\n",
    "                mloss += loss_e\n",
    "                lossvalues += [loss_e]\n",
    "            mloss = float(mloss) / n_batches\n",
    "            mlossvalues += n_batches * [mloss]\n",
    "            print(mloss)\n",
    "    return W_e, lossvalues, mlossvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, lossvalues, mlossvalues = gdtf4(X, y, n_epochs=20, batch_size = 2, lrating = 0.01)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y, '.')\n",
    "plt.plot(np.matmul(X,W))\n",
    "plt.plot(6*x - 1.0, 'r--')\n",
    "plt.ylim(-10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossvalues)\n",
    "plt.plot(mlossvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que muitas estratégias de batching podem ser usadas. Determinar o conjunto de exemplos de onde serão feitas as estimativas é parte importante de ML baseada em otimização."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoints "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em longos processos de treino, é interessante salvar o estado geral dos parâmetros sendo aprendidos, de forma que seja possível retomar o processo mais tarde, partindo do último ponto de parada.\n",
    "\n",
    "É possível fazer isso em TF com checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "def gdtf5(Xi, yi, batch_size = 1, n_epochs = 1000, lrating = 0.01, \n",
    "          model_dir = '/tmp/model0tf.ckpt', save = True, restore = False):\n",
    "    def get_batch(batch_size):\n",
    "        ids = np.random.randint(Xi.shape[0], size = batch_size)\n",
    "        return Xi[ids, :], yi[ids,:]\n",
    "    \n",
    "    m, n = Xi.shape\n",
    "    yi = yi.reshape(-1,1)\n",
    "    \n",
    "    n_batches = int(np.ceil(float(m)/batch_size))\n",
    "    \n",
    "    X = tf.placeholder(dtype = tf.float32, shape=(None, n), name = \"X\")\n",
    "    y = tf.placeholder(dtype = tf.float32, shape=(None, 1), name = \"y\")\n",
    "    W = tf.Variable(tf.random_uniform([n, 1], -1.0, 1.0), name = \"W\")\n",
    "    \n",
    "    Yhat = tf.matmul(X, W, name = \"predictions\")\n",
    "    loss = tf.reduce_mean(tf.square(Yhat - y), name = \"loss\")\n",
    "    \n",
    "    otimizador = tf.train.GradientDescentOptimizer(learning_rate = lrating)\n",
    "    #otimizador = tf.train.MomentumOptimizer(learning_rate = lrating, momentum = 0.9)\n",
    "    training = otimizador.minimize(loss)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    # init saving object\n",
    "    if save: saver = tf.train.Saver({\"W\": W})\n",
    "    \n",
    "    with tf.Session() as s:\n",
    "        if restore:\n",
    "            saver.restore(s, model_dir)\n",
    "        else:\n",
    "            s.run(init)\n",
    "        for e in range(n_epochs):\n",
    "            mloss = 0.\n",
    "            for batch_index in range(n_batches):\n",
    "                Xb, yb = get_batch(batch_size)\n",
    "                _, W_e, loss_e = s.run([training, W, loss], feed_dict = {X: Xb, y: yb})\n",
    "                mloss += loss_e\n",
    "            mloss = float(mloss) / n_batches\n",
    "            \n",
    "            if e%10 == 0:\n",
    "                print(mloss)\n",
    "                if save: \n",
    "                    print('checkpoint: saving model...')\n",
    "                    save_path = saver.save(s, model_dir)\n",
    "        if save: save_path = saver.save(s, model_dir)\n",
    "    return W_e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = gdtf5(X, y, n_epochs=1, batch_size = 2)\n",
    "W\n",
    "plt.plot(y, '.')\n",
    "plt.plot(np.matmul(X,W))\n",
    "plt.plot(6*x - 1.0, 'r--')\n",
    "plt.ylim(-10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = gdtf5(X, y, n_epochs=50, batch_size = 2)\n",
    "W\n",
    "plt.plot(y, '.')\n",
    "plt.plot(np.matmul(X,W))\n",
    "plt.plot(6*x - 1.0, 'r--')\n",
    "plt.ylim(-10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transferência de aprendizado v0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = gdtf5(X, y, n_epochs=1, batch_size = 2, restore = True)\n",
    "W\n",
    "plt.plot(y, '.')\n",
    "plt.plot(np.matmul(X,W))\n",
    "plt.plot(6*x - 1.0, 'r--')\n",
    "plt.ylim(-10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registros para acompanhamento -> logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# log \n",
    "def gdtf6(Xi, yi, batch_size = 1, n_epochs = 1000, lrating = 0.01, \n",
    "          model_dir = '/tmp/model0tf.ckpt', save = True, restore = False):\n",
    "    def get_batch(batch_size):\n",
    "        ids = np.random.randint(Xi.shape[0], size = batch_size)\n",
    "        return Xi[ids, :], yi[ids,:]\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    from datetime import datetime\n",
    "    \n",
    "    now = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "    logdir = 'logs/run-' + now + '/'\n",
    "    \n",
    "    m, n = Xi.shape\n",
    "    yi = yi.reshape(-1,1)\n",
    "    \n",
    "    n_batches = int(np.ceil(float(m)/batch_size))\n",
    "    \n",
    "    X = tf.placeholder(dtype = tf.float32, shape=(None, n), name = \"X\")\n",
    "    y = tf.placeholder(dtype = tf.float32, shape=(None, 1), name = \"y\")\n",
    "    W = tf.Variable(tf.random_uniform([n, 1], -1.0, 1.0), name = \"W\")\n",
    "    \n",
    "    Yhat = tf.matmul(X, W, name = \"predictions\")\n",
    "    loss = tf.reduce_mean(tf.square(Yhat - y), name = \"loss\")\n",
    "    \n",
    "    otimizador = tf.train.GradientDescentOptimizer(learning_rate = lrating)\n",
    "    #otimizador = tf.train.MomentumOptimizer(learning_rate = lrating, momentum = 0.9)\n",
    "    training = otimizador.minimize(loss)\n",
    "    \n",
    "    # log summary\n",
    "    loss_summary = tf.summary.scalar('loss-MSE', loss)\n",
    "    logfile = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    # init saving object\n",
    "    if save: saver = tf.train.Saver({\"W\": W})\n",
    "    \n",
    "    with tf.Session() as s:\n",
    "        if restore:\n",
    "            saver.restore(s, model_dir)\n",
    "        else:\n",
    "            s.run(init)\n",
    "        for e in range(n_epochs):\n",
    "            mloss = 0.\n",
    "            for batch_index in range(n_batches):\n",
    "                Xb, yb = get_batch(batch_size)\n",
    "                _, W_e, loss_e, summary_str = s.run([training, W, loss, loss_summary], \n",
    "                                                    feed_dict = {X: Xb, y: yb})\n",
    "                mloss += loss_e\n",
    "                # *** Na vida real fazer um log a cada batch eh muito caro ***\n",
    "                logfile.add_summary(summary_str, e*n_batches + batch_index)\n",
    "            mloss = float(mloss) / n_batches\n",
    "            \n",
    "            if e%10 == 0:\n",
    "                print(mloss)\n",
    "                if save: \n",
    "                    print('checkpoint: saving model...')\n",
    "                    save_path = saver.save(s, model_dir)\n",
    "        if save: save_path = saver.save(s, model_dir)\n",
    "    logfile.close()        \n",
    "    \n",
    "    return W_e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = gdtf6(X, y, n_epochs=10, batch_size = 2)\n",
    "W\n",
    "plt.plot(y, '.')\n",
    "plt.plot(np.matmul(X,W))\n",
    "plt.ylim(-10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W = gdtf6(X, y, n_epochs=100, batch_size = 10)\n",
    "W\n",
    "plt.plot(y, '.')\n",
    "plt.plot(np.matmul(X,W))\n",
    "plt.ylim(-10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para observar so carregar servidor tensorboard e ir para o endereço localhost:6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melhorando logs com escopos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scopes\n",
    "def gdtf8(Xi, yi, batch_size = 1, n_epochs = 1000, lrating = 0.01, \n",
    "          model_dir = '/tmp/model0tf.ckpt', save = True, restore = False):\n",
    "    def get_batch(batch_size):\n",
    "        ids = np.random.randint(Xi.shape[0], size = batch_size)\n",
    "        return Xi[ids, :], yi[ids,:]\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    from datetime import datetime\n",
    "    \n",
    "    now = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "    logdir = 'logs/run-' + now + '/'\n",
    "    \n",
    "    m, n = Xi.shape\n",
    "    yi = yi.reshape(-1,1)\n",
    "    \n",
    "    n_batches = int(np.ceil(float(m)/batch_size))\n",
    "    \n",
    "    X = tf.placeholder(dtype = tf.float32, shape=(None, n), name = \"X\")\n",
    "    y = tf.placeholder(dtype = tf.float32, shape=(None, 1), name = \"y\")\n",
    "    W = tf.Variable(tf.random_uniform([n, 1], -1.0, 1.0), name = \"W\")\n",
    "    \n",
    "    Yhat = tf.matmul(X, W, name = \"predictions\")\n",
    "    \n",
    "    with tf.name_scope('loss'):\n",
    "        error = Yhat - y\n",
    "        mse = tf.reduce_mean(tf.square(error), name = \"mse\")\n",
    "    \n",
    "    otimizador = tf.train.GradientDescentOptimizer(learning_rate = lrating)\n",
    "    training = otimizador.minimize(mse)\n",
    "    \n",
    "    # log summary\n",
    "    loss_summary = tf.summary.scalar('loss-MSE', mse)\n",
    "    logfile = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    # init saving object\n",
    "    if save: saver = tf.train.Saver({\"W\": W})\n",
    "    \n",
    "    with tf.Session() as s:\n",
    "        if restore:\n",
    "            saver.restore(s, model_dir)\n",
    "        else:\n",
    "            s.run(init)\n",
    "        for e in range(n_epochs):\n",
    "            mloss = 0.\n",
    "            for batch_index in range(n_batches):\n",
    "                Xb, yb = get_batch(batch_size)\n",
    "                _, W_e, loss_e, summary_str = s.run([training, W, mse, loss_summary], \n",
    "                                                    feed_dict = {X: Xb, y: yb})\n",
    "                mloss += loss_e\n",
    "                # *** Na vida real fazer um log a cada batch eh muito caro ***\n",
    "                logfile.add_summary(summary_str, e*n_batches + batch_index)\n",
    "            mloss = float(mloss) / n_batches\n",
    "            \n",
    "            if e%10 == 0:\n",
    "                print(mloss)\n",
    "                if save: \n",
    "                    print('checkpoint: saving model...')\n",
    "                    save_path = saver.save(s, model_dir)\n",
    "        if save: save_path = saver.save(s, model_dir)\n",
    "            \n",
    "    logfile.close()        \n",
    "    return W_e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = gdtf8(X, y, n_epochs=100, batch_size = 10)\n",
    "W\n",
    "plt.plot(y, '.')\n",
    "plt.plot(np.matmul(X,W))\n",
    "plt.ylim(-10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regressor Logístico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desta vez, vamos usar um regressor logístico, ou seja, nosso estimador vai usar como função de ativação a função sigmoid. Nosso interesse nesses regressores é o fato de que eles podem ser usados para modelar neurônios:\n",
    "\n",
    "* Entradas $X$ do regressor corrspondem aos sinais recebidos de outros neurônios, via sinapses através dos dentritos;\n",
    "* Pesos $\\bf{w}$ correspondem à permeabilidade elétrica dos dentritos, ou seja, a importância da conexcão com cada neurônio particular na entrada;\n",
    "* A saída no fim dos axônios corresponde à estimativa $\\sigma(X \\bf{w})$, com $\\sigma$ indicando uma função sigmoid.\n",
    "\n",
    "Vamos observar a função sigmoid e sua derivada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "xis = tf.placeholder(tf.float32, name = 'xis')\n",
    "sigmoid = 1. / (1 + tf.exp(-xis))\n",
    "dsigf = tf.gradients(sigmoid, [xis])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = np.linspace(-7, 7, 50)\n",
    "\n",
    "with tf.Session() as s:\n",
    "    sv, dsv = s.run([sigmoid, dsigf], feed_dict = {xis: xvals})\n",
    "\n",
    "plt.plot(xvals,sv)\n",
    "plt.plot(xvals,dsv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função sigmoid converte qualquer número para o intervalo 0 a 1. Ela produz gradientes positivos na redondeza de $y$ = 0.5.\n",
    "\n",
    "Ao usar uma sigmoid, o problema de regressão pode ser visto como um problema de classificação, já que um valor entre 0 a 1 pode ser interpretado como uma probabilidade, a probabilidade da entrada corresponder a uma certa classe.\n",
    "\n",
    "Para vermos como lidamos com um problema de classificação usando regressores logísticos, vamos usar a coleção Irís. Nesta coleção, o objetivo é classificar uma flor representada por 4 atributos (largura e comprimento da pétala, largura e comprimento do sepal) em 3 classes que correspondem à espécie da flor (_setosa_, _versicolor_ e _virginica_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquie temos um exemplo de 5 flores representadas por 4 atributos (largura e comprimento da pétala, largura e comprimento do sepal):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['data'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir a classe dessas 5 instâncias: 0, que eu acho que é _setosa_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['target'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o problema ser resolvido, vamos precisar de um modelo de regressão para cada classe. Assim, para um problema de $t$ = 3 classes, precisamos ter três valores reais, um para cada regressor. Desta forma, os valores reais serão definidos como vetores de tamanho $t$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris_X, iris_y = iris['data'], iris['target']\n",
    "# obtem hot-vectors associados com target\n",
    "iris_y = pd.get_dummies(iris_y).values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como antes, vamos normalizar a entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_X = (iris_X - np.mean(iris_X, axis = 0)) / np.std(iris_X, axis = 0)\n",
    "iris_X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, podemos dividir a coleção em treino e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separa treino e teste\n",
    "trainX, testX, trainY, testY = train_test_split(iris_X, \n",
    "                                                iris_y, \n",
    "                                                test_size=0.33, \n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de escrever o código completo do regressor, lembre que temos que incorporar os biases no peso. Fazemos isso incorporando um vetor de 1's na entrada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.hstack((np.ones((iris_X.shape[0], 1)), iris_X))[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contudo, desta vez, vamos fazer isso dentro do código, como abaixo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# codigo mais simples -- mudando o gdtf3\n",
    "def gdlog_tf(X, Y, n_epochs = 100, lrating = 0.1):\n",
    "    # supoe que bias nao foi incorporado -- entao coloca\n",
    "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    \n",
    "    m, n = X.shape\n",
    "    _, t = Y.shape # number of targets\n",
    "    \n",
    "    X = tf.constant(X, dtype = tf.float32, name = \"X\")\n",
    "    Y = tf.constant(Y, dtype = tf.float32, name = \"Y\")\n",
    "    W = tf.Variable(tf.random_uniform([n, t], -1.0, 1.0), name = \"W\")\n",
    "    \n",
    "    Yhat = tf.nn.sigmoid(tf.matmul(X, W, name = \"predictions\"))\n",
    "    #loss = tf.reduce_mean(tf.square(Yhat - Y), name = \"loss\")\n",
    "    loss = tf.nn.l2_loss(Yhat - Y, name=\"squared_error_cost\")\n",
    "    \n",
    "    otimizador = tf.train.GradientDescentOptimizer(learning_rate = lrating)\n",
    "    training = otimizador.minimize(loss)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as s:\n",
    "        s.run(init)\n",
    "        for e in range(n_epochs):\n",
    "            _, W_e, loss_e = s.run([training, W, loss])\n",
    "            if e%200==0:\n",
    "                print(loss_e)\n",
    "    return W_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para testarmos, vamos inicialmente treinar o modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W = gdlog_tf(trainX, trainY, n_epochs=3000)\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E obter previsões com o mesmo, usando o conjunto de teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pred(W, testX):\n",
    "    # get test predictions\n",
    "    sigmoid = lambda x: 1. / (1. + np.exp(-x))\n",
    "    return sigmoid(np.matmul(testX, W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para avaliarmos o que foi feito, precisamos incorporar o bias na entrada e contar quantos casos foram classificados corretamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# incorporate bias to test\n",
    "Xt = np.hstack((np.ones((testX.shape[0], 1)), testX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pred, testY):\n",
    "    # calculate accuracy\n",
    "    acc = float(sum(np.equal(np.argmax(pred, 1), np.argmax(testY, 1))))/len(pred)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(get_pred(W, Xt), testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que podemos modificar um pouco a estratégia de avaliação para tratar as saídas do regressor como uma probabilidade normalizada. Desta forma, podemos ver quanta dúvida ele teve em cada flor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = get_pred(W, Xt)\n",
    "# normalize cada linha entre 0 e 1 --> softmax, ou seja, sigmoids normalizadas\n",
    "pred = np.divide(pred, np.reshape(np.sum(pred, axis = 1), (-1, 1)))\n",
    "for i in range(5):\n",
    "    print('P: %.2f %.2f %.2f  R: %d %d %d'%(pred[i, 0], pred[i, 1], pred[i, 2], \n",
    "                                     testY[i, 0], testY[i, 1], testY[i, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No exemplo acima, note que houve bastante dúvida na classificação da 3a flor, entre as classes 1 (40%) e 2 (60%). Ele corretamente escolher a classe 2. Se modificarmos nossa função de estimativa para incorporar esta normalização, temos a função softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(W, testX):\n",
    "    # get test predictions\n",
    "    sigmoid = lambda x: 1. / (1. + np.exp(-x))\n",
    "    pred = sigmoid(np.matmul(Xt, W))\n",
    "    return np.divide(pred, np.reshape(np.sum(pred, axis = 1), (-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(softmax(W, Xt), testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E acabamos de implementar nossa primeira rede neural!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
