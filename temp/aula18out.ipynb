{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs Sequence to Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nas RNN $n \\times m$ que vimos, a \"interpretação\" ocorre concomitantemente à \"leitura\". Muitas vezes, contudo, é melhor esperar primeiro por uma interpretação do todo para, só então, iniciar o processo de decodificação. \n",
    "\n",
    "Arquiteturas que usam esta estratégias são as __Seq2seq__. Elas consistem normalmente de duas RNNs, uma codificadora e uma decodificadora, que operam como ilustrado a seguir:\n",
    "\n",
    "_sequência-entrada_ -> **[codificador]** -> _representação_ -> **[decodificador]** -> _sequência-saída_\n",
    "\n",
    "Assim, a ideia geral é usar a representação interna de uma rede codificadora para capturar o significado e contexto da entrada. Esta informação é então fornecida para a decodificadora que pode, a partir de um símbolo de partida e da representação da codificadora, ir prevendo a próxima saída decodificada até o fim da sequência.  \n",
    "\n",
    "Vamos estudar esta rede com uma aplicação em um problema muito comum em países de língua inglesa: soletrar uma palavra a partir de sua pronúncia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soletrando a partir de pronúncias (Mofenas --> Grafenas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No problema que vamos abordar, queremos traduzir a pronuncia de uma palavra, dada como uma lista de fonemas, para a grafia da palavra. Este problema é mais simples que _fala para texto_ ou _tradução_ (no sentido de não precisarmos de quantidades colossais de dados para ver algo acontecer [:)]); contudo, uma dificuldade aqui é a avaliação na escrita de palavras nunca vistas antes. Isto é díficil porque (1) há muitas pronúncias com várias transcrições razoáveis além de (2) palavras homônicas com transcrições distintas (_read_, no passado e presente, por exemplo). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulando os dados..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente temos que ler o dicionário de fonemas da CMU, _The CMU pronouncing dictionary_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdic = pd.read_csv('data/cmudict-compact.csv', comment=';', \n",
    "                   header = -1, names = ['word', 'pronunciation'],\n",
    "                   keep_default_na = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133779"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>pronunciation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40150</th>\n",
       "      <td>FACEY</td>\n",
       "      <td>F EY1 S IY0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40151</th>\n",
       "      <td>FACHET</td>\n",
       "      <td>F AE1 CH AH0 T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40152</th>\n",
       "      <td>FACIAL</td>\n",
       "      <td>F EY1 SH AH0 L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40153</th>\n",
       "      <td>FACIALS</td>\n",
       "      <td>F EY1 SH AH0 L Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40154</th>\n",
       "      <td>FACIANE</td>\n",
       "      <td>F AA0 S IY0 AA1 N EY0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40155</th>\n",
       "      <td>FACIE</td>\n",
       "      <td>F EY1 S IY0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40156</th>\n",
       "      <td>FACILE</td>\n",
       "      <td>F AE1 S AH0 L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40157</th>\n",
       "      <td>FACILITATE</td>\n",
       "      <td>F AH0 S IH1 L AH0 T EY2 T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40158</th>\n",
       "      <td>FACILITATED</td>\n",
       "      <td>F AH0 S IH1 L AH0 T EY2 T IH0 D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40159</th>\n",
       "      <td>FACILITATES</td>\n",
       "      <td>F AH0 S IH1 L AH0 T EY2 T S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              word                    pronunciation\n",
       "40150        FACEY                      F EY1 S IY0\n",
       "40151       FACHET                   F AE1 CH AH0 T\n",
       "40152       FACIAL                   F EY1 SH AH0 L\n",
       "40153      FACIALS                 F EY1 SH AH0 L Z\n",
       "40154      FACIANE            F AA0 S IY0 AA1 N EY0\n",
       "40155        FACIE                      F EY1 S IY0\n",
       "40156       FACILE                    F AE1 S AH0 L\n",
       "40157   FACILITATE        F AH0 S IH1 L AH0 T EY2 T\n",
       "40158  FACILITATED  F AH0 S IH1 L AH0 T EY2 T IH0 D\n",
       "40159  FACILITATES      F AH0 S IH1 L AH0 T EY2 T S"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdic[40150:40160]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para manter o problema em um tamanho razoaável, vamos usar apenas uma fração do dicionário de fonemas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_samples = 50000  # Number of samples to train on.\n",
    "pdic = pdic.sample(n = num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>pronunciation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62174</th>\n",
       "      <td>JUBILEE(1)</td>\n",
       "      <td>JH UW2 B AH0 L IY1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80316</th>\n",
       "      <td>MORGUN</td>\n",
       "      <td>M AO1 R G AH0 N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38198</th>\n",
       "      <td>EPISTEMIC</td>\n",
       "      <td>EH2 P IH0 S T EH1 M IH0 K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62628</th>\n",
       "      <td>KAJUAHAR</td>\n",
       "      <td>K AH0 JH UW1 AH0 HH AA0 R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64535</th>\n",
       "      <td>KIRSCHNER</td>\n",
       "      <td>K ER1 SH N ER0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word              pronunciation\n",
       "62174  JUBILEE(1)         JH UW2 B AH0 L IY1\n",
       "80316      MORGUN            M AO1 R G AH0 N\n",
       "38198   EPISTEMIC  EH2 P IH0 S T EH1 M IH0 K\n",
       "62628    KAJUAHAR  K AH0 JH UW1 AH0 HH AA0 R\n",
       "64535   KIRSCHNER             K ER1 SH N ER0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdic.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em nosso problema, a entrada serão as sequências de fonemas e a saída, as palavras. O script abaixo extrai todas as entradas (listas de fonemas) e alvos (palavras), bem como os conjuntos de símbolos observados nas entradas e alvos (note que os alvos são sempre precedidos de um símbolo que indica início de sequência ['\\t'] e terminados em um que indica fim de sequência ['\\n']). Ele Ttambém filtra as palavras muito curtas, muito longas ou com símbolos especiais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_input(inp):    \n",
    "    return ((len(inp) < 5 or      # filter long words \n",
    "             len(inp) > 15) or\n",
    "            # filter words with not alphabetical chars\n",
    "            any((not s.isalpha() for s in inp)))\n",
    "\n",
    "def vectorize(pdic):\n",
    "    # Vectorize the data.\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    input_symbols = set()\n",
    "    target_symbols = set()\n",
    "    for idx, cols in pdic.iterrows():\n",
    "        target = cols['word']\n",
    "        if filter_input(target):\n",
    "            continue\n",
    "        # We use \"tab\" as the \"start sequence\" character\n",
    "        # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "        target_text = '\\t' + target + '\\n' # sequence of letters\n",
    "        target_texts.append(target_text) \n",
    "        input_text = cols['pronunciation'].split() # sequence of phonemes\n",
    "        input_texts.append(input_text)\n",
    "        for symbol in input_text:\n",
    "            if symbol not in input_symbols:\n",
    "                input_symbols.add(symbol)\n",
    "        for symbol in target_text:\n",
    "            if symbol not in target_symbols:\n",
    "                target_symbols.add(symbol)\n",
    "    input_symbols = sorted(list(input_symbols))\n",
    "    target_symbols = sorted(list(target_symbols))\n",
    "    return input_texts, target_texts, input_symbols, target_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_texts, target_texts, input_symbols, target_symbols = vectorize(pdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40336"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['M', 'AO1', 'R', 'G', 'AH0', 'N'], '\\tMORGUN\\n')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts[0], target_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69, 28)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_symbols), len(target_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_encoder_tokens = len(input_symbols)\n",
    "num_decoder_tokens = len(target_symbols)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 17)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_encoder_seq_length, max_decoder_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, vamos criar os mapas que vão fornecer os mapeamentos de cada símbolo para o seu índice correspondente. Com isso, iniciamos os vetores de embeddings que serão usados para representar cada um dos símbolos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(s, i) for i, s in enumerate(input_symbols)])\n",
    "target_token_index = dict(\n",
    "    [(s, i) for i, s in enumerate(target_symbols)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index['AE0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_token_index['A']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nossa arquitetura seq2seq será assim:\n",
    "\n",
    "<img src=\"images/rnn_s2s0.png\" alt=\"Exemplo de RNN\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codificação _one-hot-vector_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hot_vectors(input_texts, target_texts):\n",
    "    encoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "        dtype='float32')\n",
    "    decoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "        dtype='float32')\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "        dtype='float32')\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        for t, sym in enumerate(input_text):\n",
    "            encoder_input_data[i, t, input_token_index[sym]] = 1.\n",
    "        for t, sym in enumerate(target_text):\n",
    "            # decoder_target_data is ahead of decoder_target_data by one timestep\n",
    "            decoder_input_data[i, t, target_token_index[sym]] = 1.\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, target_token_index[sym]] = 1.\n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_mat_as_map(m):\n",
    "    if m.shape[1] == num_encoder_tokens:\n",
    "        symbs = [s.rjust(3, ' ') for s in input_symbols]\n",
    "        print('   ' + ''.join([s[-3] for s in symbs]))\n",
    "        print('   ' + ''.join([s[-2] for s in symbs]))\n",
    "        print('   ' + ''.join([s[-1] for s in symbs]))\n",
    "    else:\n",
    "        print('   tnABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "    for i in range(m.shape[0]):\n",
    "        print('%2d ' % i, end='')\n",
    "        for j in range(m.shape[1]):\n",
    "            print('%s' % '.' if m[i,j]==0 else '*', end='')\n",
    "        print('\\n', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(encoder_input_data, \n",
    " decoder_input_data, \n",
    " decoder_target_data) = get_hot_vectors(input_texts, target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\tEPISTEMIC\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tnABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
      " 0 ......*.....................\n",
      " 1 .................*..........\n",
      " 2 ..........*.................\n",
      " 3 ....................*.......\n",
      " 4 .....................*......\n",
      " 5 ......*.....................\n",
      " 6 ..............*.............\n",
      " 7 ..........*.................\n",
      " 8 ....*.......................\n",
      " 9 .*..........................\n",
      "10 ............................\n",
      "11 ............................\n",
      "12 ............................\n",
      "13 ............................\n",
      "14 ............................\n",
      "15 ............................\n",
      "16 ............................\n"
     ]
    }
   ],
   "source": [
    "print_mat_as_map(decoder_target_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AAAAAAAAAAAAAAAAAA    EEEEEEEEE   IIIIII      OOOOOO      UUUUUU     \n",
      "   AAAEEEHHHOOOWWWYYY C DHHHRRRYYY  HHHHYYYJ    NWWWYYY   S THHHWWW    Z\n",
      "   012012012012012012BHDH012012012FGH012012HKLMNG012012PRSHTH012012VWYZH\n",
      " 0 ........................*............................................\n",
      " 1 ....................................................*................\n",
      " 2 ..................................*..................................\n",
      " 3 ......................................................*..............\n",
      " 4 ........................................................*............\n",
      " 5 .......................*.............................................\n",
      " 6 ...........................................*.........................\n",
      " 7 ..................................*..................................\n",
      " 8 .........................................*...........................\n",
      " 9 .....................................................................\n",
      "10 .....................................................................\n",
      "11 .....................................................................\n",
      "12 .....................................................................\n",
      "13 .....................................................................\n",
      "14 .....................................................................\n",
      "15 .....................................................................\n"
     ]
    }
   ],
   "source": [
    "print_mat_as_map(encoder_input_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementando em tensorflow, usando Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_dim = 256 # 256 LSTM cells\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state = True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_state = True, \n",
    "                    return_sequences = True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                    initial_state = encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation = 'softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/rnn_s2s0.png\" alt=\"Exemplo de RNN\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32268 samples, validate on 8068 samples\n",
      "Epoch 1/20\n",
      "32268/32268 [==============================] - 34s - loss: 1.1981 - val_loss: 1.0355\n",
      "Epoch 2/20\n",
      "32268/32268 [==============================] - 30s - loss: 0.9039 - val_loss: 0.8830\n",
      "Epoch 3/20\n",
      "32268/32268 [==============================] - 31s - loss: 0.7389 - val_loss: 0.7349\n",
      "Epoch 4/20\n",
      "32268/32268 [==============================] - 31s - loss: 0.5871 - val_loss: 0.6253\n",
      "Epoch 5/20\n",
      "32268/32268 [==============================] - 32s - loss: 0.4796 - val_loss: 0.4914\n",
      "Epoch 6/20\n",
      "32268/32268 [==============================] - 31s - loss: 0.4066 - val_loss: 0.5041\n",
      "Epoch 7/20\n",
      "32268/32268 [==============================] - 31s - loss: 0.3559 - val_loss: 0.4121s\n",
      "Epoch 8/20\n",
      "32268/32268 [==============================] - 31s - loss: 0.3171 - val_loss: 0.3898\n",
      "Epoch 9/20\n",
      "32268/32268 [==============================] - 31s - loss: 0.2855 - val_loss: 0.3482\n",
      "Epoch 10/20\n",
      "32268/32268 [==============================] - 31s - loss: 0.2601 - val_loss: 0.3171\n",
      "Epoch 11/20\n",
      "32268/32268 [==============================] - 31s - loss: 0.2388 - val_loss: 0.3022\n",
      "Epoch 12/20\n",
      "32268/32268 [==============================] - 32s - loss: 0.2201 - val_loss: 0.2894\n",
      "Epoch 13/20\n",
      "32268/32268 [==============================] - 30s - loss: 0.2029 - val_loss: 0.2933\n",
      "Epoch 14/20\n",
      "32268/32268 [==============================] - 32s - loss: 0.1886 - val_loss: 0.2782\n",
      "Epoch 15/20\n",
      "32268/32268 [==============================] - 32s - loss: 0.1748 - val_loss: 0.2812\n",
      "Epoch 16/20\n",
      "32268/32268 [==============================] - 31s - loss: 0.1621 - val_loss: 0.2742\n",
      "Epoch 17/20\n",
      "32268/32268 [==============================] - 31s - loss: 0.1497 - val_loss: 0.2745\n",
      "Epoch 18/20\n",
      "32268/32268 [==============================] - 31s - loss: 0.1394 - val_loss: 0.2806\n",
      "Epoch 19/20\n",
      "32268/32268 [==============================] - 31s - loss: 0.1288 - val_loss: 0.2888\n",
      "Epoch 20/20\n",
      "32268/32268 [==============================] - 31s - loss: 0.1192 - val_loss: 0.2897\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 20 # coloquem 6\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "         batch_size = batch_size, epochs = epochs, validation_split=0.2)\n",
    "model.save('/tmp/seq2seq.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferência"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para fazer a inferência, iremos usar a seguinte estratégia:\n",
    "\n",
    "1. Obtenha o estado do codificador para a sequência de entrada.\n",
    "2. Inicie com uma sequência alvo de tamanho 1 (apenas o símbolo de início de sequência).\n",
    "3. Dê o estado do codificador e a sequência criada até agora para o decodificador produzir uma distribuição de probabilidade para o próximo símbolo.\n",
    "4. Amostre o próximo símbolo usando a distribuição (no exemplo a sequir, é apenas usado argmax).\n",
    "5. Concatene o símbolo amostrado para a sequêcia alvo\n",
    "6. Repita desde 1 até encontrar o símbolo de fim de sequência ou alcançar o tamanho máximo de representação da saída.\n",
    "\n",
    "Note que esta estratégia poderia ter sido usada para treinar a rede também."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos testar o nosso modelo de inferência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    start = 0 if decoded_sentence[0] != ' ' else 1\n",
    "    return decoded_sentence[start:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Guess         Correct = Phonemes\n",
      "         MORGAN          MORGUN   M AO1 R G AH0 N\n",
      "      EPISTEMIC       EPISTEMIC + EH2 P IH0 S T EH1 M IH0 K\n",
      "      CAJUAHARA        KAJUAHAR   K AH0 JH UW1 AH0 HH AA0 R\n",
      "      KIRSCHNER       KIRSCHNER + K ER1 SH N ER0\n",
      "          LAVOE          LAVEAU   L AH0 V OW1\n",
      "          ASPEN           ASPEN + AE1 S P AH0 N\n",
      "     PERSECUTER      PERSECUTOR   P ER1 S AH0 K Y UW2 T ER0\n",
      "       LEANHART        LIENHART   L IY1 N HH AA2 R T\n",
      "       FETHERLY       FEATHERLY   F EH1 DH ER0 L IY0\n",
      "          NEELL           NEILL   N IY1 L\n",
      "        LINHARS        LINHARES   L IH1 N HH ER0 Z\n",
      "      ROMESBURG       ROMESBURG + R OW1 M Z B ER0 G\n",
      "       SPECIALS        SPECIALS + S P EH1 SH AH0 L Z\n",
      "          CLARA           CLARA + K L AE1 R AH0\n",
      "         RUDENS        RUDENESS   R UW1 D N AH0 S\n",
      "          EASON           EASON + IY1 Z AH0 N\n",
      "     BRUTILIZED      BRUTALIZED   B R UW1 T AH0 L AY2 Z D\n",
      "          PIERS           PYRES   P AY1 ER0 Z\n",
      "        REBENMA          REBMAN   R EH1 B M AH0 N\n",
      "          COREL          CHORAL   K AO1 R AH0 L\n",
      "       GRIFFETH        GRIFFITH   G R IH1 F AH0 TH\n",
      "         ISHMAL         ISHMAIL   IH1 SH M EY0 L\n",
      "        LEMELER          LEMLER   L EH1 M L ER0\n",
      "       BLUMBERG        BLUMBERG + B L AH1 M B ER0 G\n",
      "      CHAUNGICH       CHONGQING   CH AO1 NG K IH1 NG\n",
      "   ENCROTIMANTS   ENCROACHMENTS   IH0 N K R OW1 CH M AH0 N T S\n",
      "         BOCKUS          BOCKUS + B AA1 K AH0 S\n",
      "        COWALIK         KOWALIK   K AW0 AA1 L IH0 K\n",
      "        PAPPERT         PAPPERT + P AE1 P ER0 T\n",
      "  ACCOMMODATIVE   ACCOMMODATIVE + AH0 K AA1 M AH0 D EY2 T IH0 V\n",
      "       REINSATL       REINSTALL   R IY2 IH0 N S T AA1 L\n",
      "        HUSEMAN         HUSEMAN + HH UW1 S M AH0 N\n",
      "        BODZYAK         BODZIAK   B AO1 D Z IY0 AE0 K\n",
      "    LEGISLACTER     LEGISLATURE   L EH1 JH AH0 S L EY2 CH ER0\n",
      "        CORENTI        CORRENTI   K ER0 EH1 N T IY0\n",
      "       SPEGENER         SPIGNER   S P AY1 G N ER0\n",
      "         SCORTZ          SKIRTS   S K ER1 T S\n",
      "         HAUPPY           HOPPY   HH AO1 P IY0\n",
      "          JUNDA           JUNDA + JH AH1 N D AH0\n",
      "          CARRY           KERRI   K EH1 R IY0\n",
      "         GOUGED           GOUDE   G AW1 D\n",
      "          LINDE           LINDH   L IH1 N D\n",
      "        BRONSEN         BRONZEN   B R AA1 N Z AH0 N\n",
      "          CHANG           CHANG + CH AE1 NG\n",
      "        CHIJADA         CHIYODA   CH IH0 Y OW1 D AH0\n",
      "         RONALD          RONALD + R AA1 N AH0 L D\n",
      "         STRING          STRING + S T R IH1 NG\n",
      "      BESEETHED       BESEECHED   B IY0 S IY1 CH T\n",
      "     STEININGER      STEININGER + S T AY1 N IH0 NG ER0\n",
      "      STAINLESS       STAINLESS + S T EY1 N L AH0 S\n"
     ]
    }
   ],
   "source": [
    "print('%15s %15s = %s' % ('Guess', 'Correct', 'Phonemes'))\n",
    "for seq_index in range(50):\n",
    "    # Take one sequence (part of the training test)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    inputs = ' '.join(input_texts[seq_index])\n",
    "    correct = pdic[pdic['pronunciation']==inputs]['word'].iloc[0]\n",
    "    ok = '+' if decoded_sentence[:-1] == correct else ' '\n",
    "    print('%15s %15s %s %s'%(decoded_sentence[:-1], correct, ok, inputs))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
